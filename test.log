
/cm/local/apps/slurm/var/spool/job42892688/slurm_script --steps 2

____________Step 2: getScore use plda and lr start @ Mon Jan 24 13:45:53 SGT 2022____________

############################
lre17_train_30s
lre17_train_30s
lre17_eval_3s
true
############################
#LOG::Steps 01 Check and generate config Done
[Auto find] Your vectortype is xvector

#LOG Steps02::Determine Your vectortype is xvector Done
[Notice] It will set the default config lre17_train_30s[lre17_train_30s lre17_train_30s lre17_eval_3s] for lda, submean and whiten, if used.
#LOG Steps03::Configuration information Done
steps4 start
Vectordir:pretrained-model
lre17_eval_3s
lre17_train_30s
#LOG Steps04::Check x-vector or I-vector generation error Done
##Step5 start
lre17_train_30s[lre17_train_30s lre17_train_30s lre17_eval_3s]
#LOG Steps05:: Done
#LOG Steps06::Build connection between score process and enrollset Done
#LOG Steps07::False to speed up and true to clear files with error   Done
Get raw trials...
[Note] You should check it by yourself to avoid spks-lable-discrepancy in two set.
test spks are [ ara-acm ara-apc ara-ary ara-arz eng-gbr eng-usg por-brz qsl-pol qsl-rus spa-car spa-eur spa-lac zho-cmn zho-nan ]
register spks are [ ara-acm ara-apc ara-ary ara-arz eng-gbr eng-usg por-brz qsl-pol qsl-rus spa-car spa-eur spa-lac zho-cmn zho-nan ]
Write to pretrained-model/lre17_eval_3s/trials.raw done.
Remove invalid trials...
0 utt's trials have been removed and the invalid utts is in pretrained-model/lre17_eval_3s/trials.invalid.utt.list
Write to pretrained-model/lre17_eval_3s/trials done.
Make lre17_eval_3s trials done.
#LOG Steps08::Get Trials Done
#LOG Steps09:: Done
[ lr ]
ivector-normalize-length scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.26335, standard deviation was 0.352726
ivector-compute-lda --dim=100 --total-covariance-factor=0.1 ark:pretrained-model/lre17_train_30s/xvector_norm.ark ark:data/lre17_train_30s/utt2spk pretrained-model/lre17_train_30s/transform_100.mat 
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:288) Read 492731 utterances, 0 with errors.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:294) Computing within-class covariance.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:299) 2-norm of iVector mean is 15.9953
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:136) Stats have 14 speakers, 492731 utterances. 
WARNING (ivector-compute-lda[5.5.883~1-3eea3]:ComputeNormalizingTransform():ivector-compute-lda.cc:98) Floored 129 eigenvalues of covariance to 1.98841e-05
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:171) Singular values of between-class covariance after projecting with interpolated [total/within] covariance with a weight of 0.1 on the total covariance, are:  [ 2.90588 2.40519 2.15112 1.57986 1.19346 1.04706 1.00489 0.626699 0.492955 0.430551 0.339654 0.262772 0.222067 3.39918e-10 1.50561e-11 1.41766e-11 1.3787e-11 1.29261e-11 1.23991e-11 1.20515e-11 1.17956e-11 1.15073e-11 1.10855e-11 1.08423e-11 1.06726e-11 1.043e-11 1.01446e-11 9.95466e-12 9.83686e-12 9.57723e-12 9.48959e-12 9.26464e-12 9.12586e-12 8.87946e-12 8.80499e-12 8.68645e-12 8.51801e-12 8.23287e-12 8.02002e-12 7.93907e-12 7.75004e-12 7.65597e-12 7.50592e-12 7.38407e-12 7.29823e-12 7.26841e-12 7.11195e-12 7.03757e-12 6.84603e-12 6.69938e-12 6.68551e-12 6.52209e-12 6.43416e-12 6.36334e-12 6.18289e-12 6.10256e-12 6.02968e-12 6.02539e-12 5.94713e-12 5.87727e-12 5.75655e-12 5.65056e-12 5.59136e-12 5.47039e-12 5.42325e-12 5.29381e-12 5.1476e-12 5.13813e-12 4.97784e-12 4.89734e-12 4.82425e-12 4.79652e-12 4.68154e-12 4.63167e-12 4.53169e-12 4.4588e-12 4.41556e-12 4.32868e-12 4.29275e-12 4.19057e-12 4.11548e-12 4.02008e-12 3.97968e-12 3.93841e-12 3.84898e-12 3.72921e-12 3.66285e-12 3.59908e-12 3.56063e-12 3.48e-12 3.37638e-12 3.33863e-12 3.26507e-12 3.18899e-12 3.10062e-12 3.06316e-12 3.00453e-12 2.97976e-12 2.94594e-12 2.83621e-12 2.77813e-12 2.72591e-12 2.67029e-12 2.61699e-12 2.5666e-12 2.49457e-12 2.47714e-12 2.4372e-12 2.37685e-12 2.33161e-12 2.30659e-12 2.24263e-12 2.21768e-12 2.1415e-12 2.1314e-12 1.9936e-12 1.98225e-12 1.93959e-12 1.89675e-12 1.84232e-12 1.8375e-12 1.75251e-12 1.6588e-12 1.61799e-12 1.58681e-12 1.56003e-12 1.52131e-12 1.50476e-12 1.46223e-12 1.43422e-12 1.40598e-12 1.37978e-12 1.32589e-12 1.3049e-12 1.24837e-12 1.20679e-12 1.20302e-12 1.16225e-12 1.15229e-12 1.11321e-12 1.08741e-12 1.07572e-12 1.03118e-12 9.95066e-13 9.6071e-13 9.44611e-13 9.38644e-13 9.01184e-13 8.89295e-13 8.61739e-13 8.18741e-13 7.93326e-13 7.64134e-13 7.58101e-13 7.31556e-13 6.96536e-13 6.84815e-13 6.76163e-13 6.65161e-13 6.44676e-13 6.0626e-13 5.88007e-13 5.65442e-13 5.43494e-13 5.38411e-13 5.12856e-13 4.99757e-13 4.87929e-13 4.61777e-13 4.41923e-13 4.31617e-13 4.22456e-13 4.08345e-13 3.9768e-13 3.77143e-13 3.70577e-13 3.58922e-13 3.45225e-13 3.31515e-13 3.10234e-13 2.98156e-13 2.9378e-13 2.80455e-13 2.74832e-13 2.62717e-13 2.545e-13 2.48787e-13 2.43613e-13 2.34035e-13 2.07882e-13 2.05728e-13 1.95553e-13 1.86808e-13 1.84096e-13 1.77587e-13 1.71563e-13 1.62316e-13 1.54437e-13 1.50053e-13 1.3997e-13 1.35626e-13 1.26337e-13 1.23558e-13 1.1968e-13 1.12383e-13 1.02613e-13 9.82605e-14 9.51325e-14 8.87849e-14 8.67413e-14 8.27222e-14 7.68338e-14 7.34747e-14 7.28316e-14 6.4683e-14 6.32607e-14 5.92533e-14 5.8259e-14 5.41779e-14 5.26459e-14 5.00974e-14 4.78823e-14 4.38248e-14 4.2568e-14 3.98685e-14 3.85568e-14 3.42532e-14 3.32714e-14 3.08674e-14 2.871e-14 2.64161e-14 2.59336e-14 2.47206e-14 2.27356e-14 2.05709e-14 2.03413e-14 1.90146e-14 1.80811e-14 1.6708e-14 1.44957e-14 1.36066e-14 1.29644e-14 1.24864e-14 1.06508e-14 1.00594e-14 9.57377e-15 9.0738e-15 7.92316e-15 7.28889e-15 6.41913e-15 6.12398e-15 5.39913e-15 4.97701e-15 3.98205e-15 3.49914e-15 3.1913e-15 2.56876e-15 2.0396e-15 1.4198e-15 1.02199e-15 6.15529e-16 3.15916e-16 -1.96798e-16 -3.17994e-16 -5.90426e-16 -7.74202e-16 -1.11434e-15 -1.5514e-15 -2.05356e-15 -3.02328e-15 -3.13438e-15 -3.45145e-15 -4.22404e-15 -4.74058e-15 -5.23778e-15 -6.017e-15 -6.94501e-15 -7.65427e-15 -7.84041e-15 -8.74914e-15 -9.13164e-15 -9.87051e-15 -1.17554e-14 -1.19892e-14 -1.34698e-14 -1.51032e-14 -1.55803e-14 -1.759e-14 -1.88156e-14 -1.99412e-14 -2.13197e-14 -2.15938e-14 -2.30491e-14 -2.3581e-14 -2.49873e-14 -2.6116e-14 -2.86055e-14 -3.22324e-14 -3.3155e-14 -3.35879e-14 -3.68105e-14 -3.87799e-14 -4.09096e-14 -4.43543e-14 -4.86832e-14 -5.08205e-14 -5.21878e-14 -5.67198e-14 -5.96175e-14 -6.54364e-14 -6.73705e-14 -7.05449e-14 -7.60507e-14 -7.97494e-14 -8.15429e-14 -8.55435e-14 -8.77699e-14 -9.61526e-14 -9.99249e-14 -1.02573e-13 -1.04567e-13 -1.13184e-13 -1.16864e-13 -1.27924e-13 -1.33393e-13 -1.36094e-13 -1.42171e-13 -1.52787e-13 -1.54833e-13 -1.64929e-13 -1.699e-13 -1.79654e-13 -1.83442e-13 -1.97142e-13 -2.11987e-13 -2.2212e-13 -2.24497e-13 -2.29748e-13 -2.32274e-13 -2.42866e-13 -2.52672e-13 -2.72472e-13 -2.80419e-13 -2.89329e-13 -3.01063e-13 -3.12078e-13 -3.31314e-13 -3.44217e-13 -3.53446e-13 -3.59518e-13 -3.73339e-13 -3.91661e-13 -3.99794e-13 -4.22261e-13 -4.36173e-13 -4.581e-13 -4.6864e-13 -5.03118e-13 -5.14217e-13 -5.20797e-13 -5.56986e-13 -5.64542e-13 -5.82459e-13 -6.05782e-13 -6.25377e-13 -6.30576e-13 -6.55691e-13 -6.80004e-13 -6.83333e-13 -7.21023e-13 -7.25088e-13 -7.50613e-13 -7.70865e-13 -8.13039e-13 -8.25481e-13 -8.44359e-13 -8.4773e-13 -9.02988e-13 -9.20939e-13 -9.4504e-13 -9.96507e-13 -1.01588e-12 -1.03602e-12 -1.07917e-12 -1.09231e-12 -1.11465e-12 -1.1499e-12 -1.16233e-12 -1.19555e-12 -1.23182e-12 -1.26457e-12 -1.31607e-12 -1.33455e-12 -1.34232e-12 -1.35921e-12 -1.44e-12 -1.45164e-12 -1.48693e-12 -1.52027e-12 -1.55585e-12 -1.60597e-12 -1.61634e-12 -1.65232e-12 -1.69198e-12 -1.7438e-12 -1.77951e-12 -1.8239e-12 -1.85721e-12 -1.89554e-12 -1.95747e-12 -2.02753e-12 -2.06826e-12 -2.13031e-12 -2.13196e-12 -2.15219e-12 -2.27799e-12 -2.31803e-12 -2.34879e-12 -2.40175e-12 -2.47899e-12 -2.545e-12 -2.59225e-12 -2.60742e-12 -2.62943e-12 -2.66125e-12 -2.76783e-12 -2.81567e-12 -2.90772e-12 -2.96702e-12 -3.01653e-12 -3.15335e-12 -3.23855e-12 -3.24751e-12 -3.3031e-12 -3.33305e-12 -3.40871e-12 -3.45337e-12 -3.50447e-12 -3.61073e-12 -3.64742e-12 -3.72637e-12 -3.77371e-12 -3.84323e-12 -3.90057e-12 -3.97848e-12 -4.07818e-12 -4.12454e-12 -4.15046e-12 -4.23121e-12 -4.29324e-12 -4.37574e-12 -4.44523e-12 -4.54149e-12 -4.65499e-12 -4.76751e-12 -4.80435e-12 -4.88517e-12 -4.90975e-12 -5.01624e-12 -5.16949e-12 -5.22383e-12 -5.35253e-12 -5.36621e-12 -5.60203e-12 -5.66449e-12 -5.75557e-12 -5.87071e-12 -5.91888e-12 -5.98385e-12 -6.03217e-12 -6.2059e-12 -6.35759e-12 -6.49219e-12 -6.5615e-12 -6.6576e-12 -6.72985e-12 -6.80709e-12 -6.98579e-12 -7.06008e-12 -7.25475e-12 -7.41201e-12 -7.61434e-12 -7.68743e-12 -7.79264e-12 -7.90597e-12 -7.99185e-12 -8.1357e-12 -8.2733e-12 -8.52011e-12 -8.68321e-12 -8.72805e-12 -8.78545e-12 -9.14293e-12 -9.36166e-12 -9.41481e-12 -9.55606e-12 -9.68282e-12 -1.0036e-11 -1.01177e-11 -1.02118e-11 -1.02935e-11 -1.06209e-11 -1.0927e-11 -1.13614e-11 -1.14727e-11 -1.16499e-11 -1.21045e-11 -1.23256e-11 -1.28173e-11 -1.37283e-11 -1.45226e-11 -1.54851e-11 ]

LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:318) Wrote LDA transform to pretrained-model/lre17_train_30s/transform_100.mat
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 492731 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 1.53327 (should be close to zero), length divided by sqrt(dim) was 1.2004 (should probably be close to one)
ivector-mean ark:pretrained-model/lre17_train_30s/xvector_lda100.ark pretrained-model/lre17_train_30s/xvector_lda100.global.vec 
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_train_30s/xvector_lda100.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 492731 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.12033, standard deviation was 0.431066
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_eval_3s/xvector.scp ark:pretrained-model/lre17_eval_3s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 7310 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 2.19973 (should be close to zero), length divided by sqrt(dim) was 1.58456 (should probably be close to one)
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_eval_3s/xvector_lda100.ark ark:pretrained-model/lre17_eval_3s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 7310 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_eval_3s/xvector_lda100_submean.ark ark:pretrained-model/lre17_eval_3s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 7310 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.42317, standard deviation was 0.756873
logistic-regression-train --max-steps=20 --mix-up=0 ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark ark:data/lre17_train_30s/utt2label pretrained-model/lre17_train_30s/lr/lr.raw.model 
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance ara-acm-ar-20031217-063700-0-b-40228-40378
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance ara-apc-arb-lev-20040721-231950-a-03109-03260
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance ara-apc-fla-0585-b-22127-22303
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance eng-usg-en-4902-b-50689-50953
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance spa-car-20050929-220820-154-fsp-a-65613-65764
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance spa-lac-20051213-185913-737-fsp-b-35030-35185
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance zho-cmn-TM-003970-b-52704-52901
LOG (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:90) Retrieved 492731 vectors with 7 missing. There were 14 class labels.
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -2.6281
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -1.09791
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.756456
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.666002
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.619344
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.608191
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.59609
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.593833
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.589134
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.585039
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.580517
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.58006
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.577648
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.577063
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.576048
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.575238
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.574579
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.574178
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.574065
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.573982
LOG (logistic-regression-train[5.5.883~1-3eea3]:Train():logistic-regression.cc:58) Finished training parameters without mixture components.
logistic-regression-copy pretrained-model/lre17_train_30s/lr/lr.raw.model pretrained-model/lre17_train_30s/lr/lr.scale.model 
LOG (logistic-regression-copy[5.5.883~1-3eea3]:main():logistic-regression-copy.cc:71) Wrote model to pretrained-model/lre17_train_30s/lr/lr.scale.model
logistic-regression-eval --apply-log=true --max-steps=20 --mix-up=0 pretrained-model/lre17_train_30s/lr/lr.scale.model ark:pretrained-model/lre17_train_30s/lr/trials.label ark:pretrained-model/lre17_eval_3s/xvector_lda100_submean_norm.ark - 
LOG (logistic-regression-eval[5.5.883~1-3eea3]:ComputeScores():logistic-regression-eval.cc:111) Calculated scores for 102340 vectors with 0 missing. 
compute-eer - 
LOG (compute-eer[5.5.671~1-e5a5a]:main():compute-eer.cc:136) Equal error rate is 16.197%, at threshold -3.11413
EER% 16.2

[ lre17_eval_3s ]
16.2	pretrained-model/lre17_eval_3s/score/lr_lre17_train_30s_lre17_eval_3s_lda100_submean_norm.eer


############################
lre17_train_30s
lre17_train_30s
lre17_eval_3s
true
############################
#LOG::Steps 01 Check and generate config Done
[Auto find] Your vectortype is xvector

#LOG Steps02::Determine Your vectortype is xvector Done
[Notice] It will set the default config lre17_train_30s[lre17_train_30s lre17_train_30s lre17_eval_3s] for lda, submean and whiten, if used.
#LOG Steps03::Configuration information Done
steps4 start
Vectordir:pretrained-model
lre17_eval_3s
lre17_train_30s
#LOG Steps04::Check x-vector or I-vector generation error Done
##Step5 start
lre17_train_30s[lre17_train_30s lre17_train_30s lre17_eval_3s]
#LOG Steps05:: Done
#LOG Steps06::Build connection between score process and enrollset Done
#LOG Steps07::False to speed up and true to clear files with error   Done
#LOG Steps08::Get Trials Done
#LOG Steps09:: Done
[ lr ]
ivector-normalize-length scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.26335, standard deviation was 0.352726
ivector-compute-lda --dim=100 --total-covariance-factor=0.1 ark:pretrained-model/lre17_train_30s/xvector_norm.ark ark:data/lre17_train_30s/utt2spk pretrained-model/lre17_train_30s/transform_100.mat 
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:288) Read 492731 utterances, 0 with errors.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:294) Computing within-class covariance.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:299) 2-norm of iVector mean is 15.9953
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:136) Stats have 14 speakers, 492731 utterances. 
WARNING (ivector-compute-lda[5.5.883~1-3eea3]:ComputeNormalizingTransform():ivector-compute-lda.cc:98) Floored 129 eigenvalues of covariance to 1.98841e-05
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:171) Singular values of between-class covariance after projecting with interpolated [total/within] covariance with a weight of 0.1 on the total covariance, are:  [ 2.90588 2.40519 2.15112 1.57986 1.19346 1.04706 1.00489 0.626699 0.492955 0.430551 0.339654 0.262772 0.222067 3.39918e-10 1.50561e-11 1.41766e-11 1.3787e-11 1.29261e-11 1.23991e-11 1.20515e-11 1.17956e-11 1.15073e-11 1.10855e-11 1.08423e-11 1.06726e-11 1.043e-11 1.01446e-11 9.95466e-12 9.83686e-12 9.57723e-12 9.48959e-12 9.26464e-12 9.12586e-12 8.87946e-12 8.80499e-12 8.68645e-12 8.51801e-12 8.23287e-12 8.02002e-12 7.93907e-12 7.75004e-12 7.65597e-12 7.50592e-12 7.38407e-12 7.29823e-12 7.26841e-12 7.11195e-12 7.03757e-12 6.84603e-12 6.69938e-12 6.68551e-12 6.52209e-12 6.43416e-12 6.36334e-12 6.18289e-12 6.10256e-12 6.02968e-12 6.02539e-12 5.94713e-12 5.87727e-12 5.75655e-12 5.65056e-12 5.59136e-12 5.47039e-12 5.42325e-12 5.29381e-12 5.1476e-12 5.13813e-12 4.97784e-12 4.89734e-12 4.82425e-12 4.79652e-12 4.68154e-12 4.63167e-12 4.53169e-12 4.4588e-12 4.41556e-12 4.32868e-12 4.29275e-12 4.19057e-12 4.11548e-12 4.02008e-12 3.97968e-12 3.93841e-12 3.84898e-12 3.72921e-12 3.66285e-12 3.59908e-12 3.56063e-12 3.48e-12 3.37638e-12 3.33863e-12 3.26507e-12 3.18899e-12 3.10062e-12 3.06316e-12 3.00453e-12 2.97976e-12 2.94594e-12 2.83621e-12 2.77813e-12 2.72591e-12 2.67029e-12 2.61699e-12 2.5666e-12 2.49457e-12 2.47714e-12 2.4372e-12 2.37685e-12 2.33161e-12 2.30659e-12 2.24263e-12 2.21768e-12 2.1415e-12 2.1314e-12 1.9936e-12 1.98225e-12 1.93959e-12 1.89675e-12 1.84232e-12 1.8375e-12 1.75251e-12 1.6588e-12 1.61799e-12 1.58681e-12 1.56003e-12 1.52131e-12 1.50476e-12 1.46223e-12 1.43422e-12 1.40598e-12 1.37978e-12 1.32589e-12 1.3049e-12 1.24837e-12 1.20679e-12 1.20302e-12 1.16225e-12 1.15229e-12 1.11321e-12 1.08741e-12 1.07572e-12 1.03118e-12 9.95066e-13 9.6071e-13 9.44611e-13 9.38644e-13 9.01184e-13 8.89295e-13 8.61739e-13 8.18741e-13 7.93326e-13 7.64134e-13 7.58101e-13 7.31556e-13 6.96536e-13 6.84815e-13 6.76163e-13 6.65161e-13 6.44676e-13 6.0626e-13 5.88007e-13 5.65442e-13 5.43494e-13 5.38411e-13 5.12856e-13 4.99757e-13 4.87929e-13 4.61777e-13 4.41923e-13 4.31617e-13 4.22456e-13 4.08345e-13 3.9768e-13 3.77143e-13 3.70577e-13 3.58922e-13 3.45225e-13 3.31515e-13 3.10234e-13 2.98156e-13 2.9378e-13 2.80455e-13 2.74832e-13 2.62717e-13 2.545e-13 2.48787e-13 2.43613e-13 2.34035e-13 2.07882e-13 2.05728e-13 1.95553e-13 1.86808e-13 1.84096e-13 1.77587e-13 1.71563e-13 1.62316e-13 1.54437e-13 1.50053e-13 1.3997e-13 1.35626e-13 1.26337e-13 1.23558e-13 1.1968e-13 1.12383e-13 1.02613e-13 9.82605e-14 9.51325e-14 8.87849e-14 8.67413e-14 8.27222e-14 7.68338e-14 7.34747e-14 7.28316e-14 6.4683e-14 6.32607e-14 5.92533e-14 5.8259e-14 5.41779e-14 5.26459e-14 5.00974e-14 4.78823e-14 4.38248e-14 4.2568e-14 3.98685e-14 3.85568e-14 3.42532e-14 3.32714e-14 3.08674e-14 2.871e-14 2.64161e-14 2.59336e-14 2.47206e-14 2.27356e-14 2.05709e-14 2.03413e-14 1.90146e-14 1.80811e-14 1.6708e-14 1.44957e-14 1.36066e-14 1.29644e-14 1.24864e-14 1.06508e-14 1.00594e-14 9.57377e-15 9.0738e-15 7.92316e-15 7.28889e-15 6.41913e-15 6.12398e-15 5.39913e-15 4.97701e-15 3.98205e-15 3.49914e-15 3.1913e-15 2.56876e-15 2.0396e-15 1.4198e-15 1.02199e-15 6.15529e-16 3.15916e-16 -1.96798e-16 -3.17994e-16 -5.90426e-16 -7.74202e-16 -1.11434e-15 -1.5514e-15 -2.05356e-15 -3.02328e-15 -3.13438e-15 -3.45145e-15 -4.22404e-15 -4.74058e-15 -5.23778e-15 -6.017e-15 -6.94501e-15 -7.65427e-15 -7.84041e-15 -8.74914e-15 -9.13164e-15 -9.87051e-15 -1.17554e-14 -1.19892e-14 -1.34698e-14 -1.51032e-14 -1.55803e-14 -1.759e-14 -1.88156e-14 -1.99412e-14 -2.13197e-14 -2.15938e-14 -2.30491e-14 -2.3581e-14 -2.49873e-14 -2.6116e-14 -2.86055e-14 -3.22324e-14 -3.3155e-14 -3.35879e-14 -3.68105e-14 -3.87799e-14 -4.09096e-14 -4.43543e-14 -4.86832e-14 -5.08205e-14 -5.21878e-14 -5.67198e-14 -5.96175e-14 -6.54364e-14 -6.73705e-14 -7.05449e-14 -7.60507e-14 -7.97494e-14 -8.15429e-14 -8.55435e-14 -8.77699e-14 -9.61526e-14 -9.99249e-14 -1.02573e-13 -1.04567e-13 -1.13184e-13 -1.16864e-13 -1.27924e-13 -1.33393e-13 -1.36094e-13 -1.42171e-13 -1.52787e-13 -1.54833e-13 -1.64929e-13 -1.699e-13 -1.79654e-13 -1.83442e-13 -1.97142e-13 -2.11987e-13 -2.2212e-13 -2.24497e-13 -2.29748e-13 -2.32274e-13 -2.42866e-13 -2.52672e-13 -2.72472e-13 -2.80419e-13 -2.89329e-13 -3.01063e-13 -3.12078e-13 -3.31314e-13 -3.44217e-13 -3.53446e-13 -3.59518e-13 -3.73339e-13 -3.91661e-13 -3.99794e-13 -4.22261e-13 -4.36173e-13 -4.581e-13 -4.6864e-13 -5.03118e-13 -5.14217e-13 -5.20797e-13 -5.56986e-13 -5.64542e-13 -5.82459e-13 -6.05782e-13 -6.25377e-13 -6.30576e-13 -6.55691e-13 -6.80004e-13 -6.83333e-13 -7.21023e-13 -7.25088e-13 -7.50613e-13 -7.70865e-13 -8.13039e-13 -8.25481e-13 -8.44359e-13 -8.4773e-13 -9.02988e-13 -9.20939e-13 -9.4504e-13 -9.96507e-13 -1.01588e-12 -1.03602e-12 -1.07917e-12 -1.09231e-12 -1.11465e-12 -1.1499e-12 -1.16233e-12 -1.19555e-12 -1.23182e-12 -1.26457e-12 -1.31607e-12 -1.33455e-12 -1.34232e-12 -1.35921e-12 -1.44e-12 -1.45164e-12 -1.48693e-12 -1.52027e-12 -1.55585e-12 -1.60597e-12 -1.61634e-12 -1.65232e-12 -1.69198e-12 -1.7438e-12 -1.77951e-12 -1.8239e-12 -1.85721e-12 -1.89554e-12 -1.95747e-12 -2.02753e-12 -2.06826e-12 -2.13031e-12 -2.13196e-12 -2.15219e-12 -2.27799e-12 -2.31803e-12 -2.34879e-12 -2.40175e-12 -2.47899e-12 -2.545e-12 -2.59225e-12 -2.60742e-12 -2.62943e-12 -2.66125e-12 -2.76783e-12 -2.81567e-12 -2.90772e-12 -2.96702e-12 -3.01653e-12 -3.15335e-12 -3.23855e-12 -3.24751e-12 -3.3031e-12 -3.33305e-12 -3.40871e-12 -3.45337e-12 -3.50447e-12 -3.61073e-12 -3.64742e-12 -3.72637e-12 -3.77371e-12 -3.84323e-12 -3.90057e-12 -3.97848e-12 -4.07818e-12 -4.12454e-12 -4.15046e-12 -4.23121e-12 -4.29324e-12 -4.37574e-12 -4.44523e-12 -4.54149e-12 -4.65499e-12 -4.76751e-12 -4.80435e-12 -4.88517e-12 -4.90975e-12 -5.01624e-12 -5.16949e-12 -5.22383e-12 -5.35253e-12 -5.36621e-12 -5.60203e-12 -5.66449e-12 -5.75557e-12 -5.87071e-12 -5.91888e-12 -5.98385e-12 -6.03217e-12 -6.2059e-12 -6.35759e-12 -6.49219e-12 -6.5615e-12 -6.6576e-12 -6.72985e-12 -6.80709e-12 -6.98579e-12 -7.06008e-12 -7.25475e-12 -7.41201e-12 -7.61434e-12 -7.68743e-12 -7.79264e-12 -7.90597e-12 -7.99185e-12 -8.1357e-12 -8.2733e-12 -8.52011e-12 -8.68321e-12 -8.72805e-12 -8.78545e-12 -9.14293e-12 -9.36166e-12 -9.41481e-12 -9.55606e-12 -9.68282e-12 -1.0036e-11 -1.01177e-11 -1.02118e-11 -1.02935e-11 -1.06209e-11 -1.0927e-11 -1.13614e-11 -1.14727e-11 -1.16499e-11 -1.21045e-11 -1.23256e-11 -1.28173e-11 -1.37283e-11 -1.45226e-11 -1.54851e-11 ]

LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:318) Wrote LDA transform to pretrained-model/lre17_train_30s/transform_100.mat
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 492731 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 1.53327 (should be close to zero), length divided by sqrt(dim) was 1.2004 (should probably be close to one)
ivector-mean ark:pretrained-model/lre17_train_30s/xvector_lda100.ark pretrained-model/lre17_train_30s/xvector_lda100.global.vec 
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_train_30s/xvector_lda100.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 492731 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.12033, standard deviation was 0.431066
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_eval_3s/xvector.scp ark:pretrained-model/lre17_eval_3s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 7310 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 2.19973 (should be close to zero), length divided by sqrt(dim) was 1.58456 (should probably be close to one)
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_eval_3s/xvector_lda100.ark ark:pretrained-model/lre17_eval_3s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 7310 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_eval_3s/xvector_lda100_submean.ark ark:pretrained-model/lre17_eval_3s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 7310 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.42317, standard deviation was 0.756873
logistic-regression-train --max-steps=20 --mix-up=0 ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark ark:data/lre17_train_30s/utt2label pretrained-model/lre17_train_30s/lr/lr.raw.model 
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance ara-acm-ar-20031217-063700-0-b-40228-40378
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance ara-apc-arb-lev-20040721-231950-a-03109-03260
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance ara-apc-fla-0585-b-22127-22303
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance eng-usg-en-4902-b-50689-50953
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance spa-car-20050929-220820-154-fsp-a-65613-65764
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance spa-lac-20051213-185913-737-fsp-b-35030-35185
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance zho-cmn-TM-003970-b-52704-52901
LOG (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:90) Retrieved 492731 vectors with 7 missing. There were 14 class labels.
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -2.6281
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -1.09791
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.756456
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.666002
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.619344
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.608191
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.59609
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.593833
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.589134
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.585039
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.580517
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.58006
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.577648
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.577063
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.576048
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.575238
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.574579
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.574178
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.574065
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.573982
LOG (logistic-regression-train[5.5.883~1-3eea3]:Train():logistic-regression.cc:58) Finished training parameters without mixture components.
logistic-regression-copy pretrained-model/lre17_train_30s/lr/lr.raw.model pretrained-model/lre17_train_30s/lr/lr.scale.model 
LOG (logistic-regression-copy[5.5.883~1-3eea3]:main():logistic-regression-copy.cc:71) Wrote model to pretrained-model/lre17_train_30s/lr/lr.scale.model
logistic-regression-eval --apply-log=true --max-steps=20 --mix-up=0 pretrained-model/lre17_train_30s/lr/lr.scale.model ark:pretrained-model/lre17_train_30s/lr/trials.label ark:pretrained-model/lre17_eval_3s/xvector_lda100_submean_norm.ark - 
LOG (logistic-regression-eval[5.5.883~1-3eea3]:ComputeScores():logistic-regression-eval.cc:111) Calculated scores for 102340 vectors with 0 missing. 
Cavg: 0.159

[ lre17_eval_3s ]
Cavg: 0.159	pretrained-model/lre17_eval_3s/score/lr_lre17_train_30s_lre17_eval_3s_lda100_submean_norm.Cavg


#LOG:: getScore use lr Done!
############################
lre17_train_30s
lre17_train_30s
lre17_eval_3s
true
############################
#LOG::Steps 01 Check and generate config Done
[Auto find] Your vectortype is xvector

#LOG Steps02::Determine Your vectortype is xvector Done
[Notice] It will set the default config lre17_train_30s[lre17_train_30s lre17_train_30s lre17_eval_3s] for lda, submean and whiten, if used.
#LOG Steps03::Configuration information Done
steps4 start
Vectordir:pretrained-model
lre17_eval_3s
lre17_train_30s
#LOG Steps04::Check x-vector or I-vector generation error Done
##Step5 start
lre17_train_30s[lre17_train_30s lre17_train_30s lre17_eval_3s]
#LOG Steps05:: Done
#LOG Steps06::Build connection between score process and enrollset Done
#LOG Steps07::False to speed up and true to clear files with error   Done
#LOG Steps08::Get Trials Done
#LOG Steps09:: Done
[ plda ]
ivector-mean ark:data/lre17_train_30s/spk2utt scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/spk_xvector_mean.ark ark,t:pretrained-model/lre17_train_30s/num_utts.ark 
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance ara-acm-ar-20031217-063700-0-b-40228-40378
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance ara-apc-arb-lev-20040721-231950-a-03109-03260
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance ara-apc-fla-0585-b-22127-22303
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance eng-usg-en-4902-b-50689-50953
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance spa-car-20050929-220820-154-fsp-a-65613-65764
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance spa-lac-20051213-185913-737-fsp-b-35030-35185
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance zho-cmn-TM-003970-b-52704-52901
LOG (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:134) Computed mean of 14 speakers (0 with no utterances), consisting of 492731 utterances (7 absent from input).
LOG (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:145) Norm of mean of speakers is 17.7422, root-mean-square speaker-iVector length divided by sqrt(dim) is 1.29274
ivector-normalize-length scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.26335, standard deviation was 0.352726
ivector-compute-lda --dim=100 --total-covariance-factor=0.1 ark:pretrained-model/lre17_train_30s/xvector_norm.ark ark:data/lre17_train_30s/utt2spk pretrained-model/lre17_train_30s/transform_100.mat 
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:288) Read 492731 utterances, 0 with errors.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:294) Computing within-class covariance.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:299) 2-norm of iVector mean is 15.9953
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:136) Stats have 14 speakers, 492731 utterances. 
WARNING (ivector-compute-lda[5.5.883~1-3eea3]:ComputeNormalizingTransform():ivector-compute-lda.cc:98) Floored 129 eigenvalues of covariance to 1.98841e-05
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:171) Singular values of between-class covariance after projecting with interpolated [total/within] covariance with a weight of 0.1 on the total covariance, are:  [ 2.90588 2.40519 2.15112 1.57986 1.19346 1.04706 1.00489 0.626699 0.492955 0.430551 0.339654 0.262772 0.222067 3.39918e-10 1.50561e-11 1.41766e-11 1.3787e-11 1.29261e-11 1.23991e-11 1.20515e-11 1.17956e-11 1.15073e-11 1.10855e-11 1.08423e-11 1.06726e-11 1.043e-11 1.01446e-11 9.95466e-12 9.83686e-12 9.57723e-12 9.48959e-12 9.26464e-12 9.12586e-12 8.87946e-12 8.80499e-12 8.68645e-12 8.51801e-12 8.23287e-12 8.02002e-12 7.93907e-12 7.75004e-12 7.65597e-12 7.50592e-12 7.38407e-12 7.29823e-12 7.26841e-12 7.11195e-12 7.03757e-12 6.84603e-12 6.69938e-12 6.68551e-12 6.52209e-12 6.43416e-12 6.36334e-12 6.18289e-12 6.10256e-12 6.02968e-12 6.02539e-12 5.94713e-12 5.87727e-12 5.75655e-12 5.65056e-12 5.59136e-12 5.47039e-12 5.42325e-12 5.29381e-12 5.1476e-12 5.13813e-12 4.97784e-12 4.89734e-12 4.82425e-12 4.79652e-12 4.68154e-12 4.63167e-12 4.53169e-12 4.4588e-12 4.41556e-12 4.32868e-12 4.29275e-12 4.19057e-12 4.11548e-12 4.02008e-12 3.97968e-12 3.93841e-12 3.84898e-12 3.72921e-12 3.66285e-12 3.59908e-12 3.56063e-12 3.48e-12 3.37638e-12 3.33863e-12 3.26507e-12 3.18899e-12 3.10062e-12 3.06316e-12 3.00453e-12 2.97976e-12 2.94594e-12 2.83621e-12 2.77813e-12 2.72591e-12 2.67029e-12 2.61699e-12 2.5666e-12 2.49457e-12 2.47714e-12 2.4372e-12 2.37685e-12 2.33161e-12 2.30659e-12 2.24263e-12 2.21768e-12 2.1415e-12 2.1314e-12 1.9936e-12 1.98225e-12 1.93959e-12 1.89675e-12 1.84232e-12 1.8375e-12 1.75251e-12 1.6588e-12 1.61799e-12 1.58681e-12 1.56003e-12 1.52131e-12 1.50476e-12 1.46223e-12 1.43422e-12 1.40598e-12 1.37978e-12 1.32589e-12 1.3049e-12 1.24837e-12 1.20679e-12 1.20302e-12 1.16225e-12 1.15229e-12 1.11321e-12 1.08741e-12 1.07572e-12 1.03118e-12 9.95066e-13 9.6071e-13 9.44611e-13 9.38644e-13 9.01184e-13 8.89295e-13 8.61739e-13 8.18741e-13 7.93326e-13 7.64134e-13 7.58101e-13 7.31556e-13 6.96536e-13 6.84815e-13 6.76163e-13 6.65161e-13 6.44676e-13 6.0626e-13 5.88007e-13 5.65442e-13 5.43494e-13 5.38411e-13 5.12856e-13 4.99757e-13 4.87929e-13 4.61777e-13 4.41923e-13 4.31617e-13 4.22456e-13 4.08345e-13 3.9768e-13 3.77143e-13 3.70577e-13 3.58922e-13 3.45225e-13 3.31515e-13 3.10234e-13 2.98156e-13 2.9378e-13 2.80455e-13 2.74832e-13 2.62717e-13 2.545e-13 2.48787e-13 2.43613e-13 2.34035e-13 2.07882e-13 2.05728e-13 1.95553e-13 1.86808e-13 1.84096e-13 1.77587e-13 1.71563e-13 1.62316e-13 1.54437e-13 1.50053e-13 1.3997e-13 1.35626e-13 1.26337e-13 1.23558e-13 1.1968e-13 1.12383e-13 1.02613e-13 9.82605e-14 9.51325e-14 8.87849e-14 8.67413e-14 8.27222e-14 7.68338e-14 7.34747e-14 7.28316e-14 6.4683e-14 6.32607e-14 5.92533e-14 5.8259e-14 5.41779e-14 5.26459e-14 5.00974e-14 4.78823e-14 4.38248e-14 4.2568e-14 3.98685e-14 3.85568e-14 3.42532e-14 3.32714e-14 3.08674e-14 2.871e-14 2.64161e-14 2.59336e-14 2.47206e-14 2.27356e-14 2.05709e-14 2.03413e-14 1.90146e-14 1.80811e-14 1.6708e-14 1.44957e-14 1.36066e-14 1.29644e-14 1.24864e-14 1.06508e-14 1.00594e-14 9.57377e-15 9.0738e-15 7.92316e-15 7.28889e-15 6.41913e-15 6.12398e-15 5.39913e-15 4.97701e-15 3.98205e-15 3.49914e-15 3.1913e-15 2.56876e-15 2.0396e-15 1.4198e-15 1.02199e-15 6.15529e-16 3.15916e-16 -1.96798e-16 -3.17994e-16 -5.90426e-16 -7.74202e-16 -1.11434e-15 -1.5514e-15 -2.05356e-15 -3.02328e-15 -3.13438e-15 -3.45145e-15 -4.22404e-15 -4.74058e-15 -5.23778e-15 -6.017e-15 -6.94501e-15 -7.65427e-15 -7.84041e-15 -8.74914e-15 -9.13164e-15 -9.87051e-15 -1.17554e-14 -1.19892e-14 -1.34698e-14 -1.51032e-14 -1.55803e-14 -1.759e-14 -1.88156e-14 -1.99412e-14 -2.13197e-14 -2.15938e-14 -2.30491e-14 -2.3581e-14 -2.49873e-14 -2.6116e-14 -2.86055e-14 -3.22324e-14 -3.3155e-14 -3.35879e-14 -3.68105e-14 -3.87799e-14 -4.09096e-14 -4.43543e-14 -4.86832e-14 -5.08205e-14 -5.21878e-14 -5.67198e-14 -5.96175e-14 -6.54364e-14 -6.73705e-14 -7.05449e-14 -7.60507e-14 -7.97494e-14 -8.15429e-14 -8.55435e-14 -8.77699e-14 -9.61526e-14 -9.99249e-14 -1.02573e-13 -1.04567e-13 -1.13184e-13 -1.16864e-13 -1.27924e-13 -1.33393e-13 -1.36094e-13 -1.42171e-13 -1.52787e-13 -1.54833e-13 -1.64929e-13 -1.699e-13 -1.79654e-13 -1.83442e-13 -1.97142e-13 -2.11987e-13 -2.2212e-13 -2.24497e-13 -2.29748e-13 -2.32274e-13 -2.42866e-13 -2.52672e-13 -2.72472e-13 -2.80419e-13 -2.89329e-13 -3.01063e-13 -3.12078e-13 -3.31314e-13 -3.44217e-13 -3.53446e-13 -3.59518e-13 -3.73339e-13 -3.91661e-13 -3.99794e-13 -4.22261e-13 -4.36173e-13 -4.581e-13 -4.6864e-13 -5.03118e-13 -5.14217e-13 -5.20797e-13 -5.56986e-13 -5.64542e-13 -5.82459e-13 -6.05782e-13 -6.25377e-13 -6.30576e-13 -6.55691e-13 -6.80004e-13 -6.83333e-13 -7.21023e-13 -7.25088e-13 -7.50613e-13 -7.70865e-13 -8.13039e-13 -8.25481e-13 -8.44359e-13 -8.4773e-13 -9.02988e-13 -9.20939e-13 -9.4504e-13 -9.96507e-13 -1.01588e-12 -1.03602e-12 -1.07917e-12 -1.09231e-12 -1.11465e-12 -1.1499e-12 -1.16233e-12 -1.19555e-12 -1.23182e-12 -1.26457e-12 -1.31607e-12 -1.33455e-12 -1.34232e-12 -1.35921e-12 -1.44e-12 -1.45164e-12 -1.48693e-12 -1.52027e-12 -1.55585e-12 -1.60597e-12 -1.61634e-12 -1.65232e-12 -1.69198e-12 -1.7438e-12 -1.77951e-12 -1.8239e-12 -1.85721e-12 -1.89554e-12 -1.95747e-12 -2.02753e-12 -2.06826e-12 -2.13031e-12 -2.13196e-12 -2.15219e-12 -2.27799e-12 -2.31803e-12 -2.34879e-12 -2.40175e-12 -2.47899e-12 -2.545e-12 -2.59225e-12 -2.60742e-12 -2.62943e-12 -2.66125e-12 -2.76783e-12 -2.81567e-12 -2.90772e-12 -2.96702e-12 -3.01653e-12 -3.15335e-12 -3.23855e-12 -3.24751e-12 -3.3031e-12 -3.33305e-12 -3.40871e-12 -3.45337e-12 -3.50447e-12 -3.61073e-12 -3.64742e-12 -3.72637e-12 -3.77371e-12 -3.84323e-12 -3.90057e-12 -3.97848e-12 -4.07818e-12 -4.12454e-12 -4.15046e-12 -4.23121e-12 -4.29324e-12 -4.37574e-12 -4.44523e-12 -4.54149e-12 -4.65499e-12 -4.76751e-12 -4.80435e-12 -4.88517e-12 -4.90975e-12 -5.01624e-12 -5.16949e-12 -5.22383e-12 -5.35253e-12 -5.36621e-12 -5.60203e-12 -5.66449e-12 -5.75557e-12 -5.87071e-12 -5.91888e-12 -5.98385e-12 -6.03217e-12 -6.2059e-12 -6.35759e-12 -6.49219e-12 -6.5615e-12 -6.6576e-12 -6.72985e-12 -6.80709e-12 -6.98579e-12 -7.06008e-12 -7.25475e-12 -7.41201e-12 -7.61434e-12 -7.68743e-12 -7.79264e-12 -7.90597e-12 -7.99185e-12 -8.1357e-12 -8.2733e-12 -8.52011e-12 -8.68321e-12 -8.72805e-12 -8.78545e-12 -9.14293e-12 -9.36166e-12 -9.41481e-12 -9.55606e-12 -9.68282e-12 -1.0036e-11 -1.01177e-11 -1.02118e-11 -1.02935e-11 -1.06209e-11 -1.0927e-11 -1.13614e-11 -1.14727e-11 -1.16499e-11 -1.21045e-11 -1.23256e-11 -1.28173e-11 -1.37283e-11 -1.45226e-11 -1.54851e-11 ]

LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:318) Wrote LDA transform to pretrained-model/lre17_train_30s/transform_100.mat
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat ark:pretrained-model/lre17_train_30s/spk_xvector_mean.ark ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 14 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 2.75929 (should be close to zero), length divided by sqrt(dim) was 1.40767 (should probably be close to one)
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 492731 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 1.53327 (should be close to zero), length divided by sqrt(dim) was 1.2004 (should probably be close to one)
ivector-mean ark:pretrained-model/lre17_train_30s/xvector_lda100.ark pretrained-model/lre17_train_30s/xvector_lda100.global.vec 
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100.ark ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 14 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean.ark ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 14 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.00249, standard deviation was 1.05391
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_eval_3s/xvector.scp ark:pretrained-model/lre17_eval_3s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 7310 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 2.19973 (should be close to zero), length divided by sqrt(dim) was 1.58456 (should probably be close to one)
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_eval_3s/xvector_lda100.ark ark:pretrained-model/lre17_eval_3s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 7310 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_eval_3s/xvector_lda100_submean.ark ark:pretrained-model/lre17_eval_3s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 7310 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.42317, standard deviation was 0.756873
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_train_30s/xvector_lda100.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 492731 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.12033, standard deviation was 0.431066
ivector-compute-plda ark:data/lre17_train_30s/spk2utt ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark pretrained-model/lre17_train_30s/plda 
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance ara-acm-ar-20031217-063700-0-b-40228-40378
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance ara-apc-arb-lev-20040721-231950-a-03109-03260
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance ara-apc-fla-0585-b-22127-22303
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance eng-usg-en-4902-b-50689-50953
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance spa-car-20050929-220820-154-fsp-a-65613-65764
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance spa-lac-20051213-185913-737-fsp-b-35030-35185
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance zho-cmn-TM-003970-b-52704-52901
LOG (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:109) Accumulated stats from 14 speakers (0 with no utterances), consisting of 492731 utterances (7 absent from input).
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 0 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0484
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9578
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 1 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0474
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9649
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 2 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.047
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.962
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 3 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0468
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9611
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 4 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0466
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9607
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 5 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0465
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9604
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 6 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0464
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9603
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 7 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0463
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9602
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 8 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0463
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9601
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 9 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0462
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9601
LOG (ivector-compute-plda[5.5.883~1-3eea3]:GetOutput():plda.cc:540) Norm of mean of iVector distribution is 1.36617
LOG (ivector-compute-plda[5.5.883~1-3eea3]:GetOutput():plda.cc:577) Diagonal of between-class variance in normalized space is  [ 6.80719 5.07642 4.21652 3.83125 3.16111 2.9845 1.87321 1.66839 1.41967 1.1445 1.04717 0.410991 0.353115 4.81664e-06 4.80518e-06 4.79952e-06 4.78659e-06 4.78533e-06 4.76916e-06 4.76006e-06 4.75583e-06 4.74886e-06 4.74146e-06 4.73521e-06 4.7278e-06 4.72638e-06 4.72055e-06 4.71931e-06 4.71476e-06 4.71429e-06 4.70611e-06 4.70559e-06 4.70065e-06 4.69463e-06 4.68495e-06 4.68266e-06 4.67751e-06 4.67639e-06 4.67314e-06 4.66825e-06 4.66532e-06 4.6579e-06 4.65547e-06 4.65337e-06 4.64883e-06 4.64267e-06 4.63883e-06 4.63503e-06 4.63125e-06 4.62836e-06 4.62584e-06 4.62273e-06 4.61574e-06 4.61444e-06 4.61382e-06 4.61116e-06 4.60593e-06 4.6043e-06 4.60154e-06 4.59664e-06 4.5945e-06 4.59077e-06 4.5904e-06 4.58557e-06 4.58489e-06 4.58178e-06 4.57969e-06 4.57774e-06 4.5726e-06 4.56954e-06 4.56696e-06 4.56051e-06 4.55849e-06 4.55698e-06 4.55087e-06 4.54951e-06 4.54885e-06 4.54784e-06 4.54052e-06 4.53963e-06 4.53844e-06 4.53773e-06 4.53246e-06 4.52768e-06 4.52645e-06 4.52513e-06 4.52103e-06 4.51922e-06 4.51895e-06 4.51649e-06 4.51228e-06 4.51026e-06 4.50859e-06 4.5083e-06 4.5071e-06 4.5048e-06 4.50242e-06 4.49714e-06 4.48988e-06 4.43171e-06 ]

ivector-plda-scoring --normalize-length=true --num-utts=ark:pretrained-model/lre17_train_30s/num_utts.ark 'ivector-copy-plda --smoothing=0.0 pretrained-model/lre17_train_30s/plda - |' ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean_norm.ark ark:pretrained-model/lre17_eval_3s/xvector_lda100_submean_norm.ark - pretrained-model/lre17_eval_3s/score/plda_lre17_train_30s_lre17_eval_3s_lda100_submean_norm.score 
ivector-copy-plda --smoothing=0.0 pretrained-model/lre17_train_30s/plda - 
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:96) Reading train iVectors
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:122) Read 14 training iVectors, errors on 0
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:126) Average renormalization scale on training iVectors was 0.375309
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:129) Reading test iVectors
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:147) Read 7310 test iVectors.
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:150) Average renormalization scale on test iVectors was 1.04389
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:214) Mean score was -21.1485, standard deviation was 32.7746
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:217) Processed 102340 trials, 0 had errors.
compute-eer - 
LOG (compute-eer[5.5.671~1-e5a5a]:main():compute-eer.cc:136) Equal error rate is 26.0328%, at threshold -1.33052
EER% 26.03

[ lre17_eval_3s ]
26.03	pretrained-model/lre17_eval_3s/score/plda_lre17_train_30s_lre17_eval_3s_lda100_submean_norm.eer


############################
lre17_train_30s
lre17_train_30s
lre17_eval_3s
true
############################
#LOG::Steps 01 Check and generate config Done
[Auto find] Your vectortype is xvector

#LOG Steps02::Determine Your vectortype is xvector Done
[Notice] It will set the default config lre17_train_30s[lre17_train_30s lre17_train_30s lre17_eval_3s] for lda, submean and whiten, if used.
#LOG Steps03::Configuration information Done
steps4 start
Vectordir:pretrained-model
lre17_eval_3s
lre17_train_30s
#LOG Steps04::Check x-vector or I-vector generation error Done
##Step5 start
lre17_train_30s[lre17_train_30s lre17_train_30s lre17_eval_3s]
#LOG Steps05:: Done
#LOG Steps06::Build connection between score process and enrollset Done
#LOG Steps07::False to speed up and true to clear files with error   Done
#LOG Steps08::Get Trials Done
#LOG Steps09:: Done
[ plda ]
ivector-mean ark:data/lre17_train_30s/spk2utt scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/spk_xvector_mean.ark ark,t:pretrained-model/lre17_train_30s/num_utts.ark 
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance ara-acm-ar-20031217-063700-0-b-40228-40378
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance ara-apc-arb-lev-20040721-231950-a-03109-03260
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance ara-apc-fla-0585-b-22127-22303
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance eng-usg-en-4902-b-50689-50953
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance spa-car-20050929-220820-154-fsp-a-65613-65764
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance spa-lac-20051213-185913-737-fsp-b-35030-35185
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance zho-cmn-TM-003970-b-52704-52901
LOG (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:134) Computed mean of 14 speakers (0 with no utterances), consisting of 492731 utterances (7 absent from input).
LOG (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:145) Norm of mean of speakers is 17.7422, root-mean-square speaker-iVector length divided by sqrt(dim) is 1.29274
ivector-normalize-length scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.26335, standard deviation was 0.352726
ivector-compute-lda --dim=100 --total-covariance-factor=0.1 ark:pretrained-model/lre17_train_30s/xvector_norm.ark ark:data/lre17_train_30s/utt2spk pretrained-model/lre17_train_30s/transform_100.mat 
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:288) Read 492731 utterances, 0 with errors.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:294) Computing within-class covariance.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:299) 2-norm of iVector mean is 15.9953
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:136) Stats have 14 speakers, 492731 utterances. 
WARNING (ivector-compute-lda[5.5.883~1-3eea3]:ComputeNormalizingTransform():ivector-compute-lda.cc:98) Floored 129 eigenvalues of covariance to 1.98841e-05
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:171) Singular values of between-class covariance after projecting with interpolated [total/within] covariance with a weight of 0.1 on the total covariance, are:  [ 2.90588 2.40519 2.15112 1.57986 1.19346 1.04706 1.00489 0.626699 0.492955 0.430551 0.339654 0.262772 0.222067 3.39918e-10 1.50561e-11 1.41766e-11 1.3787e-11 1.29261e-11 1.23991e-11 1.20515e-11 1.17956e-11 1.15073e-11 1.10855e-11 1.08423e-11 1.06726e-11 1.043e-11 1.01446e-11 9.95466e-12 9.83686e-12 9.57723e-12 9.48959e-12 9.26464e-12 9.12586e-12 8.87946e-12 8.80499e-12 8.68645e-12 8.51801e-12 8.23287e-12 8.02002e-12 7.93907e-12 7.75004e-12 7.65597e-12 7.50592e-12 7.38407e-12 7.29823e-12 7.26841e-12 7.11195e-12 7.03757e-12 6.84603e-12 6.69938e-12 6.68551e-12 6.52209e-12 6.43416e-12 6.36334e-12 6.18289e-12 6.10256e-12 6.02968e-12 6.02539e-12 5.94713e-12 5.87727e-12 5.75655e-12 5.65056e-12 5.59136e-12 5.47039e-12 5.42325e-12 5.29381e-12 5.1476e-12 5.13813e-12 4.97784e-12 4.89734e-12 4.82425e-12 4.79652e-12 4.68154e-12 4.63167e-12 4.53169e-12 4.4588e-12 4.41556e-12 4.32868e-12 4.29275e-12 4.19057e-12 4.11548e-12 4.02008e-12 3.97968e-12 3.93841e-12 3.84898e-12 3.72921e-12 3.66285e-12 3.59908e-12 3.56063e-12 3.48e-12 3.37638e-12 3.33863e-12 3.26507e-12 3.18899e-12 3.10062e-12 3.06316e-12 3.00453e-12 2.97976e-12 2.94594e-12 2.83621e-12 2.77813e-12 2.72591e-12 2.67029e-12 2.61699e-12 2.5666e-12 2.49457e-12 2.47714e-12 2.4372e-12 2.37685e-12 2.33161e-12 2.30659e-12 2.24263e-12 2.21768e-12 2.1415e-12 2.1314e-12 1.9936e-12 1.98225e-12 1.93959e-12 1.89675e-12 1.84232e-12 1.8375e-12 1.75251e-12 1.6588e-12 1.61799e-12 1.58681e-12 1.56003e-12 1.52131e-12 1.50476e-12 1.46223e-12 1.43422e-12 1.40598e-12 1.37978e-12 1.32589e-12 1.3049e-12 1.24837e-12 1.20679e-12 1.20302e-12 1.16225e-12 1.15229e-12 1.11321e-12 1.08741e-12 1.07572e-12 1.03118e-12 9.95066e-13 9.6071e-13 9.44611e-13 9.38644e-13 9.01184e-13 8.89295e-13 8.61739e-13 8.18741e-13 7.93326e-13 7.64134e-13 7.58101e-13 7.31556e-13 6.96536e-13 6.84815e-13 6.76163e-13 6.65161e-13 6.44676e-13 6.0626e-13 5.88007e-13 5.65442e-13 5.43494e-13 5.38411e-13 5.12856e-13 4.99757e-13 4.87929e-13 4.61777e-13 4.41923e-13 4.31617e-13 4.22456e-13 4.08345e-13 3.9768e-13 3.77143e-13 3.70577e-13 3.58922e-13 3.45225e-13 3.31515e-13 3.10234e-13 2.98156e-13 2.9378e-13 2.80455e-13 2.74832e-13 2.62717e-13 2.545e-13 2.48787e-13 2.43613e-13 2.34035e-13 2.07882e-13 2.05728e-13 1.95553e-13 1.86808e-13 1.84096e-13 1.77587e-13 1.71563e-13 1.62316e-13 1.54437e-13 1.50053e-13 1.3997e-13 1.35626e-13 1.26337e-13 1.23558e-13 1.1968e-13 1.12383e-13 1.02613e-13 9.82605e-14 9.51325e-14 8.87849e-14 8.67413e-14 8.27222e-14 7.68338e-14 7.34747e-14 7.28316e-14 6.4683e-14 6.32607e-14 5.92533e-14 5.8259e-14 5.41779e-14 5.26459e-14 5.00974e-14 4.78823e-14 4.38248e-14 4.2568e-14 3.98685e-14 3.85568e-14 3.42532e-14 3.32714e-14 3.08674e-14 2.871e-14 2.64161e-14 2.59336e-14 2.47206e-14 2.27356e-14 2.05709e-14 2.03413e-14 1.90146e-14 1.80811e-14 1.6708e-14 1.44957e-14 1.36066e-14 1.29644e-14 1.24864e-14 1.06508e-14 1.00594e-14 9.57377e-15 9.0738e-15 7.92316e-15 7.28889e-15 6.41913e-15 6.12398e-15 5.39913e-15 4.97701e-15 3.98205e-15 3.49914e-15 3.1913e-15 2.56876e-15 2.0396e-15 1.4198e-15 1.02199e-15 6.15529e-16 3.15916e-16 -1.96798e-16 -3.17994e-16 -5.90426e-16 -7.74202e-16 -1.11434e-15 -1.5514e-15 -2.05356e-15 -3.02328e-15 -3.13438e-15 -3.45145e-15 -4.22404e-15 -4.74058e-15 -5.23778e-15 -6.017e-15 -6.94501e-15 -7.65427e-15 -7.84041e-15 -8.74914e-15 -9.13164e-15 -9.87051e-15 -1.17554e-14 -1.19892e-14 -1.34698e-14 -1.51032e-14 -1.55803e-14 -1.759e-14 -1.88156e-14 -1.99412e-14 -2.13197e-14 -2.15938e-14 -2.30491e-14 -2.3581e-14 -2.49873e-14 -2.6116e-14 -2.86055e-14 -3.22324e-14 -3.3155e-14 -3.35879e-14 -3.68105e-14 -3.87799e-14 -4.09096e-14 -4.43543e-14 -4.86832e-14 -5.08205e-14 -5.21878e-14 -5.67198e-14 -5.96175e-14 -6.54364e-14 -6.73705e-14 -7.05449e-14 -7.60507e-14 -7.97494e-14 -8.15429e-14 -8.55435e-14 -8.77699e-14 -9.61526e-14 -9.99249e-14 -1.02573e-13 -1.04567e-13 -1.13184e-13 -1.16864e-13 -1.27924e-13 -1.33393e-13 -1.36094e-13 -1.42171e-13 -1.52787e-13 -1.54833e-13 -1.64929e-13 -1.699e-13 -1.79654e-13 -1.83442e-13 -1.97142e-13 -2.11987e-13 -2.2212e-13 -2.24497e-13 -2.29748e-13 -2.32274e-13 -2.42866e-13 -2.52672e-13 -2.72472e-13 -2.80419e-13 -2.89329e-13 -3.01063e-13 -3.12078e-13 -3.31314e-13 -3.44217e-13 -3.53446e-13 -3.59518e-13 -3.73339e-13 -3.91661e-13 -3.99794e-13 -4.22261e-13 -4.36173e-13 -4.581e-13 -4.6864e-13 -5.03118e-13 -5.14217e-13 -5.20797e-13 -5.56986e-13 -5.64542e-13 -5.82459e-13 -6.05782e-13 -6.25377e-13 -6.30576e-13 -6.55691e-13 -6.80004e-13 -6.83333e-13 -7.21023e-13 -7.25088e-13 -7.50613e-13 -7.70865e-13 -8.13039e-13 -8.25481e-13 -8.44359e-13 -8.4773e-13 -9.02988e-13 -9.20939e-13 -9.4504e-13 -9.96507e-13 -1.01588e-12 -1.03602e-12 -1.07917e-12 -1.09231e-12 -1.11465e-12 -1.1499e-12 -1.16233e-12 -1.19555e-12 -1.23182e-12 -1.26457e-12 -1.31607e-12 -1.33455e-12 -1.34232e-12 -1.35921e-12 -1.44e-12 -1.45164e-12 -1.48693e-12 -1.52027e-12 -1.55585e-12 -1.60597e-12 -1.61634e-12 -1.65232e-12 -1.69198e-12 -1.7438e-12 -1.77951e-12 -1.8239e-12 -1.85721e-12 -1.89554e-12 -1.95747e-12 -2.02753e-12 -2.06826e-12 -2.13031e-12 -2.13196e-12 -2.15219e-12 -2.27799e-12 -2.31803e-12 -2.34879e-12 -2.40175e-12 -2.47899e-12 -2.545e-12 -2.59225e-12 -2.60742e-12 -2.62943e-12 -2.66125e-12 -2.76783e-12 -2.81567e-12 -2.90772e-12 -2.96702e-12 -3.01653e-12 -3.15335e-12 -3.23855e-12 -3.24751e-12 -3.3031e-12 -3.33305e-12 -3.40871e-12 -3.45337e-12 -3.50447e-12 -3.61073e-12 -3.64742e-12 -3.72637e-12 -3.77371e-12 -3.84323e-12 -3.90057e-12 -3.97848e-12 -4.07818e-12 -4.12454e-12 -4.15046e-12 -4.23121e-12 -4.29324e-12 -4.37574e-12 -4.44523e-12 -4.54149e-12 -4.65499e-12 -4.76751e-12 -4.80435e-12 -4.88517e-12 -4.90975e-12 -5.01624e-12 -5.16949e-12 -5.22383e-12 -5.35253e-12 -5.36621e-12 -5.60203e-12 -5.66449e-12 -5.75557e-12 -5.87071e-12 -5.91888e-12 -5.98385e-12 -6.03217e-12 -6.2059e-12 -6.35759e-12 -6.49219e-12 -6.5615e-12 -6.6576e-12 -6.72985e-12 -6.80709e-12 -6.98579e-12 -7.06008e-12 -7.25475e-12 -7.41201e-12 -7.61434e-12 -7.68743e-12 -7.79264e-12 -7.90597e-12 -7.99185e-12 -8.1357e-12 -8.2733e-12 -8.52011e-12 -8.68321e-12 -8.72805e-12 -8.78545e-12 -9.14293e-12 -9.36166e-12 -9.41481e-12 -9.55606e-12 -9.68282e-12 -1.0036e-11 -1.01177e-11 -1.02118e-11 -1.02935e-11 -1.06209e-11 -1.0927e-11 -1.13614e-11 -1.14727e-11 -1.16499e-11 -1.21045e-11 -1.23256e-11 -1.28173e-11 -1.37283e-11 -1.45226e-11 -1.54851e-11 ]

LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:318) Wrote LDA transform to pretrained-model/lre17_train_30s/transform_100.mat
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat ark:pretrained-model/lre17_train_30s/spk_xvector_mean.ark ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 14 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 2.75929 (should be close to zero), length divided by sqrt(dim) was 1.40767 (should probably be close to one)
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 492731 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 1.53327 (should be close to zero), length divided by sqrt(dim) was 1.2004 (should probably be close to one)
ivector-mean ark:pretrained-model/lre17_train_30s/xvector_lda100.ark pretrained-model/lre17_train_30s/xvector_lda100.global.vec 
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100.ark ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 14 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean.ark ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 14 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.00249, standard deviation was 1.05391
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_eval_3s/xvector.scp ark:pretrained-model/lre17_eval_3s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 7310 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 2.19973 (should be close to zero), length divided by sqrt(dim) was 1.58456 (should probably be close to one)
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_eval_3s/xvector_lda100.ark ark:pretrained-model/lre17_eval_3s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 7310 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_eval_3s/xvector_lda100_submean.ark ark:pretrained-model/lre17_eval_3s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 7310 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.42317, standard deviation was 0.756873
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_train_30s/xvector_lda100.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 492731 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.12033, standard deviation was 0.431066
ivector-compute-plda ark:data/lre17_train_30s/spk2utt ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark pretrained-model/lre17_train_30s/plda 
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance ara-acm-ar-20031217-063700-0-b-40228-40378
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance ara-apc-arb-lev-20040721-231950-a-03109-03260
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance ara-apc-fla-0585-b-22127-22303
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance eng-usg-en-4902-b-50689-50953
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance spa-car-20050929-220820-154-fsp-a-65613-65764
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance spa-lac-20051213-185913-737-fsp-b-35030-35185
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance zho-cmn-TM-003970-b-52704-52901
LOG (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:109) Accumulated stats from 14 speakers (0 with no utterances), consisting of 492731 utterances (7 absent from input).
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 0 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0484
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9578
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 1 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0474
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9649
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 2 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.047
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.962
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 3 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0468
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9611
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 4 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0466
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9607
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 5 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0465
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9604
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 6 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0464
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9603
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 7 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0463
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9602
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 8 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0463
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9601
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 9 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0462
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9601
LOG (ivector-compute-plda[5.5.883~1-3eea3]:GetOutput():plda.cc:540) Norm of mean of iVector distribution is 1.36617
LOG (ivector-compute-plda[5.5.883~1-3eea3]:GetOutput():plda.cc:577) Diagonal of between-class variance in normalized space is  [ 6.80719 5.07642 4.21652 3.83125 3.16111 2.9845 1.87321 1.66839 1.41967 1.1445 1.04717 0.410991 0.353115 4.81664e-06 4.80518e-06 4.79952e-06 4.78659e-06 4.78533e-06 4.76916e-06 4.76006e-06 4.75583e-06 4.74886e-06 4.74146e-06 4.73521e-06 4.7278e-06 4.72638e-06 4.72055e-06 4.71931e-06 4.71476e-06 4.71429e-06 4.70611e-06 4.70559e-06 4.70065e-06 4.69463e-06 4.68495e-06 4.68266e-06 4.67751e-06 4.67639e-06 4.67314e-06 4.66825e-06 4.66532e-06 4.6579e-06 4.65547e-06 4.65337e-06 4.64883e-06 4.64267e-06 4.63883e-06 4.63503e-06 4.63125e-06 4.62836e-06 4.62584e-06 4.62273e-06 4.61574e-06 4.61444e-06 4.61382e-06 4.61116e-06 4.60593e-06 4.6043e-06 4.60154e-06 4.59664e-06 4.5945e-06 4.59077e-06 4.5904e-06 4.58557e-06 4.58489e-06 4.58178e-06 4.57969e-06 4.57774e-06 4.5726e-06 4.56954e-06 4.56696e-06 4.56051e-06 4.55849e-06 4.55698e-06 4.55087e-06 4.54951e-06 4.54885e-06 4.54784e-06 4.54052e-06 4.53963e-06 4.53844e-06 4.53773e-06 4.53246e-06 4.52768e-06 4.52645e-06 4.52513e-06 4.52103e-06 4.51922e-06 4.51895e-06 4.51649e-06 4.51228e-06 4.51026e-06 4.50859e-06 4.5083e-06 4.5071e-06 4.5048e-06 4.50242e-06 4.49714e-06 4.48988e-06 4.43171e-06 ]

ivector-plda-scoring --normalize-length=true --num-utts=ark:pretrained-model/lre17_train_30s/num_utts.ark 'ivector-copy-plda --smoothing=0.0 pretrained-model/lre17_train_30s/plda - |' ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean_norm.ark ark:pretrained-model/lre17_eval_3s/xvector_lda100_submean_norm.ark - pretrained-model/lre17_eval_3s/score/plda_lre17_train_30s_lre17_eval_3s_lda100_submean_norm.score 
ivector-copy-plda --smoothing=0.0 pretrained-model/lre17_train_30s/plda - 
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:96) Reading train iVectors
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:122) Read 14 training iVectors, errors on 0
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:126) Average renormalization scale on training iVectors was 0.375309
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:129) Reading test iVectors
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:147) Read 7310 test iVectors.
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:150) Average renormalization scale on test iVectors was 1.04389
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:214) Mean score was -21.1485, standard deviation was 32.7746
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:217) Processed 102340 trials, 0 had errors.
Cavg: 0.2607

[ lre17_eval_3s ]
Cavg: 0.2607	pretrained-model/lre17_eval_3s/score/plda_lre17_train_30s_lre17_eval_3s_lda100_submean_norm.Cavg


#LOG:: getScore use plda Done!
############################
lre17_train_30s
lre17_train_30s
lre17_eval_10s
true
############################
#LOG::Steps 01 Check and generate config Done
[Auto find] Your vectortype is xvector

#LOG Steps02::Determine Your vectortype is xvector Done
[Notice] It will set the default config lre17_train_30s[lre17_train_30s lre17_train_30s lre17_eval_10s] for lda, submean and whiten, if used.
#LOG Steps03::Configuration information Done
steps4 start
Vectordir:pretrained-model
lre17_eval_10s
lre17_train_30s
#LOG Steps04::Check x-vector or I-vector generation error Done
##Step5 start
lre17_train_30s[lre17_train_30s lre17_train_30s lre17_eval_10s]
#LOG Steps05:: Done
#LOG Steps06::Build connection between score process and enrollset Done
#LOG Steps07::False to speed up and true to clear files with error   Done
Get raw trials...
[Note] You should check it by yourself to avoid spks-lable-discrepancy in two set.
test spks are [ ara-acm ara-apc ara-ary ara-arz eng-gbr eng-usg por-brz qsl-pol qsl-rus spa-car spa-eur spa-lac zho-cmn zho-nan ]
register spks are [ ara-acm ara-apc ara-ary ara-arz eng-gbr eng-usg por-brz qsl-pol qsl-rus spa-car spa-eur spa-lac zho-cmn zho-nan ]
Write to pretrained-model/lre17_eval_10s/trials.raw done.
Remove invalid trials...
0 utt's trials have been removed and the invalid utts is in pretrained-model/lre17_eval_10s/trials.invalid.utt.list
Write to pretrained-model/lre17_eval_10s/trials done.
Make lre17_eval_10s trials done.
#LOG Steps08::Get Trials Done
#LOG Steps09:: Done
[ lr ]
ivector-normalize-length scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.26335, standard deviation was 0.352726
ivector-compute-lda --dim=100 --total-covariance-factor=0.1 ark:pretrained-model/lre17_train_30s/xvector_norm.ark ark:data/lre17_train_30s/utt2spk pretrained-model/lre17_train_30s/transform_100.mat 
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:288) Read 492731 utterances, 0 with errors.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:294) Computing within-class covariance.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:299) 2-norm of iVector mean is 15.9953
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:136) Stats have 14 speakers, 492731 utterances. 
WARNING (ivector-compute-lda[5.5.883~1-3eea3]:ComputeNormalizingTransform():ivector-compute-lda.cc:98) Floored 129 eigenvalues of covariance to 1.98841e-05
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:171) Singular values of between-class covariance after projecting with interpolated [total/within] covariance with a weight of 0.1 on the total covariance, are:  [ 2.90588 2.40519 2.15112 1.57986 1.19346 1.04706 1.00489 0.626699 0.492955 0.430551 0.339654 0.262772 0.222067 3.39918e-10 1.50561e-11 1.41766e-11 1.3787e-11 1.29261e-11 1.23991e-11 1.20515e-11 1.17956e-11 1.15073e-11 1.10855e-11 1.08423e-11 1.06726e-11 1.043e-11 1.01446e-11 9.95466e-12 9.83686e-12 9.57723e-12 9.48959e-12 9.26464e-12 9.12586e-12 8.87946e-12 8.80499e-12 8.68645e-12 8.51801e-12 8.23287e-12 8.02002e-12 7.93907e-12 7.75004e-12 7.65597e-12 7.50592e-12 7.38407e-12 7.29823e-12 7.26841e-12 7.11195e-12 7.03757e-12 6.84603e-12 6.69938e-12 6.68551e-12 6.52209e-12 6.43416e-12 6.36334e-12 6.18289e-12 6.10256e-12 6.02968e-12 6.02539e-12 5.94713e-12 5.87727e-12 5.75655e-12 5.65056e-12 5.59136e-12 5.47039e-12 5.42325e-12 5.29381e-12 5.1476e-12 5.13813e-12 4.97784e-12 4.89734e-12 4.82425e-12 4.79652e-12 4.68154e-12 4.63167e-12 4.53169e-12 4.4588e-12 4.41556e-12 4.32868e-12 4.29275e-12 4.19057e-12 4.11548e-12 4.02008e-12 3.97968e-12 3.93841e-12 3.84898e-12 3.72921e-12 3.66285e-12 3.59908e-12 3.56063e-12 3.48e-12 3.37638e-12 3.33863e-12 3.26507e-12 3.18899e-12 3.10062e-12 3.06316e-12 3.00453e-12 2.97976e-12 2.94594e-12 2.83621e-12 2.77813e-12 2.72591e-12 2.67029e-12 2.61699e-12 2.5666e-12 2.49457e-12 2.47714e-12 2.4372e-12 2.37685e-12 2.33161e-12 2.30659e-12 2.24263e-12 2.21768e-12 2.1415e-12 2.1314e-12 1.9936e-12 1.98225e-12 1.93959e-12 1.89675e-12 1.84232e-12 1.8375e-12 1.75251e-12 1.6588e-12 1.61799e-12 1.58681e-12 1.56003e-12 1.52131e-12 1.50476e-12 1.46223e-12 1.43422e-12 1.40598e-12 1.37978e-12 1.32589e-12 1.3049e-12 1.24837e-12 1.20679e-12 1.20302e-12 1.16225e-12 1.15229e-12 1.11321e-12 1.08741e-12 1.07572e-12 1.03118e-12 9.95066e-13 9.6071e-13 9.44611e-13 9.38644e-13 9.01184e-13 8.89295e-13 8.61739e-13 8.18741e-13 7.93326e-13 7.64134e-13 7.58101e-13 7.31556e-13 6.96536e-13 6.84815e-13 6.76163e-13 6.65161e-13 6.44676e-13 6.0626e-13 5.88007e-13 5.65442e-13 5.43494e-13 5.38411e-13 5.12856e-13 4.99757e-13 4.87929e-13 4.61777e-13 4.41923e-13 4.31617e-13 4.22456e-13 4.08345e-13 3.9768e-13 3.77143e-13 3.70577e-13 3.58922e-13 3.45225e-13 3.31515e-13 3.10234e-13 2.98156e-13 2.9378e-13 2.80455e-13 2.74832e-13 2.62717e-13 2.545e-13 2.48787e-13 2.43613e-13 2.34035e-13 2.07882e-13 2.05728e-13 1.95553e-13 1.86808e-13 1.84096e-13 1.77587e-13 1.71563e-13 1.62316e-13 1.54437e-13 1.50053e-13 1.3997e-13 1.35626e-13 1.26337e-13 1.23558e-13 1.1968e-13 1.12383e-13 1.02613e-13 9.82605e-14 9.51325e-14 8.87849e-14 8.67413e-14 8.27222e-14 7.68338e-14 7.34747e-14 7.28316e-14 6.4683e-14 6.32607e-14 5.92533e-14 5.8259e-14 5.41779e-14 5.26459e-14 5.00974e-14 4.78823e-14 4.38248e-14 4.2568e-14 3.98685e-14 3.85568e-14 3.42532e-14 3.32714e-14 3.08674e-14 2.871e-14 2.64161e-14 2.59336e-14 2.47206e-14 2.27356e-14 2.05709e-14 2.03413e-14 1.90146e-14 1.80811e-14 1.6708e-14 1.44957e-14 1.36066e-14 1.29644e-14 1.24864e-14 1.06508e-14 1.00594e-14 9.57377e-15 9.0738e-15 7.92316e-15 7.28889e-15 6.41913e-15 6.12398e-15 5.39913e-15 4.97701e-15 3.98205e-15 3.49914e-15 3.1913e-15 2.56876e-15 2.0396e-15 1.4198e-15 1.02199e-15 6.15529e-16 3.15916e-16 -1.96798e-16 -3.17994e-16 -5.90426e-16 -7.74202e-16 -1.11434e-15 -1.5514e-15 -2.05356e-15 -3.02328e-15 -3.13438e-15 -3.45145e-15 -4.22404e-15 -4.74058e-15 -5.23778e-15 -6.017e-15 -6.94501e-15 -7.65427e-15 -7.84041e-15 -8.74914e-15 -9.13164e-15 -9.87051e-15 -1.17554e-14 -1.19892e-14 -1.34698e-14 -1.51032e-14 -1.55803e-14 -1.759e-14 -1.88156e-14 -1.99412e-14 -2.13197e-14 -2.15938e-14 -2.30491e-14 -2.3581e-14 -2.49873e-14 -2.6116e-14 -2.86055e-14 -3.22324e-14 -3.3155e-14 -3.35879e-14 -3.68105e-14 -3.87799e-14 -4.09096e-14 -4.43543e-14 -4.86832e-14 -5.08205e-14 -5.21878e-14 -5.67198e-14 -5.96175e-14 -6.54364e-14 -6.73705e-14 -7.05449e-14 -7.60507e-14 -7.97494e-14 -8.15429e-14 -8.55435e-14 -8.77699e-14 -9.61526e-14 -9.99249e-14 -1.02573e-13 -1.04567e-13 -1.13184e-13 -1.16864e-13 -1.27924e-13 -1.33393e-13 -1.36094e-13 -1.42171e-13 -1.52787e-13 -1.54833e-13 -1.64929e-13 -1.699e-13 -1.79654e-13 -1.83442e-13 -1.97142e-13 -2.11987e-13 -2.2212e-13 -2.24497e-13 -2.29748e-13 -2.32274e-13 -2.42866e-13 -2.52672e-13 -2.72472e-13 -2.80419e-13 -2.89329e-13 -3.01063e-13 -3.12078e-13 -3.31314e-13 -3.44217e-13 -3.53446e-13 -3.59518e-13 -3.73339e-13 -3.91661e-13 -3.99794e-13 -4.22261e-13 -4.36173e-13 -4.581e-13 -4.6864e-13 -5.03118e-13 -5.14217e-13 -5.20797e-13 -5.56986e-13 -5.64542e-13 -5.82459e-13 -6.05782e-13 -6.25377e-13 -6.30576e-13 -6.55691e-13 -6.80004e-13 -6.83333e-13 -7.21023e-13 -7.25088e-13 -7.50613e-13 -7.70865e-13 -8.13039e-13 -8.25481e-13 -8.44359e-13 -8.4773e-13 -9.02988e-13 -9.20939e-13 -9.4504e-13 -9.96507e-13 -1.01588e-12 -1.03602e-12 -1.07917e-12 -1.09231e-12 -1.11465e-12 -1.1499e-12 -1.16233e-12 -1.19555e-12 -1.23182e-12 -1.26457e-12 -1.31607e-12 -1.33455e-12 -1.34232e-12 -1.35921e-12 -1.44e-12 -1.45164e-12 -1.48693e-12 -1.52027e-12 -1.55585e-12 -1.60597e-12 -1.61634e-12 -1.65232e-12 -1.69198e-12 -1.7438e-12 -1.77951e-12 -1.8239e-12 -1.85721e-12 -1.89554e-12 -1.95747e-12 -2.02753e-12 -2.06826e-12 -2.13031e-12 -2.13196e-12 -2.15219e-12 -2.27799e-12 -2.31803e-12 -2.34879e-12 -2.40175e-12 -2.47899e-12 -2.545e-12 -2.59225e-12 -2.60742e-12 -2.62943e-12 -2.66125e-12 -2.76783e-12 -2.81567e-12 -2.90772e-12 -2.96702e-12 -3.01653e-12 -3.15335e-12 -3.23855e-12 -3.24751e-12 -3.3031e-12 -3.33305e-12 -3.40871e-12 -3.45337e-12 -3.50447e-12 -3.61073e-12 -3.64742e-12 -3.72637e-12 -3.77371e-12 -3.84323e-12 -3.90057e-12 -3.97848e-12 -4.07818e-12 -4.12454e-12 -4.15046e-12 -4.23121e-12 -4.29324e-12 -4.37574e-12 -4.44523e-12 -4.54149e-12 -4.65499e-12 -4.76751e-12 -4.80435e-12 -4.88517e-12 -4.90975e-12 -5.01624e-12 -5.16949e-12 -5.22383e-12 -5.35253e-12 -5.36621e-12 -5.60203e-12 -5.66449e-12 -5.75557e-12 -5.87071e-12 -5.91888e-12 -5.98385e-12 -6.03217e-12 -6.2059e-12 -6.35759e-12 -6.49219e-12 -6.5615e-12 -6.6576e-12 -6.72985e-12 -6.80709e-12 -6.98579e-12 -7.06008e-12 -7.25475e-12 -7.41201e-12 -7.61434e-12 -7.68743e-12 -7.79264e-12 -7.90597e-12 -7.99185e-12 -8.1357e-12 -8.2733e-12 -8.52011e-12 -8.68321e-12 -8.72805e-12 -8.78545e-12 -9.14293e-12 -9.36166e-12 -9.41481e-12 -9.55606e-12 -9.68282e-12 -1.0036e-11 -1.01177e-11 -1.02118e-11 -1.02935e-11 -1.06209e-11 -1.0927e-11 -1.13614e-11 -1.14727e-11 -1.16499e-11 -1.21045e-11 -1.23256e-11 -1.28173e-11 -1.37283e-11 -1.45226e-11 -1.54851e-11 ]

LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:318) Wrote LDA transform to pretrained-model/lre17_train_30s/transform_100.mat
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 492731 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 1.53327 (should be close to zero), length divided by sqrt(dim) was 1.2004 (should probably be close to one)
ivector-mean ark:pretrained-model/lre17_train_30s/xvector_lda100.ark pretrained-model/lre17_train_30s/xvector_lda100.global.vec 
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_train_30s/xvector_lda100.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 492731 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.12033, standard deviation was 0.431066
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_eval_10s/xvector.scp ark:pretrained-model/lre17_eval_10s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 7310 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 1.77802 (should be close to zero), length divided by sqrt(dim) was 1.13097 (should probably be close to one)
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_eval_10s/xvector_lda100.ark ark:pretrained-model/lre17_eval_10s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 7310 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_eval_10s/xvector_lda100_submean.ark ark:pretrained-model/lre17_eval_10s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 7310 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.00648, standard deviation was 0.576888
logistic-regression-train --max-steps=20 --mix-up=0 ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark ark:data/lre17_train_30s/utt2label pretrained-model/lre17_train_30s/lr/lr.raw.model 
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance ara-acm-ar-20031217-063700-0-b-40228-40378
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance ara-apc-arb-lev-20040721-231950-a-03109-03260
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance ara-apc-fla-0585-b-22127-22303
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance eng-usg-en-4902-b-50689-50953
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance spa-car-20050929-220820-154-fsp-a-65613-65764
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance spa-lac-20051213-185913-737-fsp-b-35030-35185
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance zho-cmn-TM-003970-b-52704-52901
LOG (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:90) Retrieved 492731 vectors with 7 missing. There were 14 class labels.
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -2.6281
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -1.09791
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.756456
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.666002
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.619344
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.608191
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.59609
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.593833
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.589134
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.585039
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.580517
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.58006
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.577648
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.577063
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.576048
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.575238
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.574579
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.574178
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.574065
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.573982
LOG (logistic-regression-train[5.5.883~1-3eea3]:Train():logistic-regression.cc:58) Finished training parameters without mixture components.
logistic-regression-copy pretrained-model/lre17_train_30s/lr/lr.raw.model pretrained-model/lre17_train_30s/lr/lr.scale.model 
LOG (logistic-regression-copy[5.5.883~1-3eea3]:main():logistic-regression-copy.cc:71) Wrote model to pretrained-model/lre17_train_30s/lr/lr.scale.model
logistic-regression-eval --apply-log=true --max-steps=20 --mix-up=0 pretrained-model/lre17_train_30s/lr/lr.scale.model ark:pretrained-model/lre17_train_30s/lr/trials.label ark:pretrained-model/lre17_eval_10s/xvector_lda100_submean_norm.ark - 
LOG (logistic-regression-eval[5.5.883~1-3eea3]:ComputeScores():logistic-regression-eval.cc:111) Calculated scores for 102340 vectors with 0 missing. 
compute-eer - 
LOG (compute-eer[5.5.671~1-e5a5a]:main():compute-eer.cc:136) Equal error rate is 10.7387%, at threshold -3.12609
EER% 10.74

[ lre17_eval_10s ]
10.74	pretrained-model/lre17_eval_10s/score/lr_lre17_train_30s_lre17_eval_10s_lda100_submean_norm.eer


############################
lre17_train_30s
lre17_train_30s
lre17_eval_10s
true
############################
#LOG::Steps 01 Check and generate config Done
[Auto find] Your vectortype is xvector

#LOG Steps02::Determine Your vectortype is xvector Done
[Notice] It will set the default config lre17_train_30s[lre17_train_30s lre17_train_30s lre17_eval_10s] for lda, submean and whiten, if used.
#LOG Steps03::Configuration information Done
steps4 start
Vectordir:pretrained-model
lre17_eval_10s
lre17_train_30s
#LOG Steps04::Check x-vector or I-vector generation error Done
##Step5 start
lre17_train_30s[lre17_train_30s lre17_train_30s lre17_eval_10s]
#LOG Steps05:: Done
#LOG Steps06::Build connection between score process and enrollset Done
#LOG Steps07::False to speed up and true to clear files with error   Done
#LOG Steps08::Get Trials Done
#LOG Steps09:: Done
[ lr ]
ivector-normalize-length scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.26335, standard deviation was 0.352726
ivector-compute-lda --dim=100 --total-covariance-factor=0.1 ark:pretrained-model/lre17_train_30s/xvector_norm.ark ark:data/lre17_train_30s/utt2spk pretrained-model/lre17_train_30s/transform_100.mat 
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:288) Read 492731 utterances, 0 with errors.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:294) Computing within-class covariance.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:299) 2-norm of iVector mean is 15.9953
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:136) Stats have 14 speakers, 492731 utterances. 
WARNING (ivector-compute-lda[5.5.883~1-3eea3]:ComputeNormalizingTransform():ivector-compute-lda.cc:98) Floored 129 eigenvalues of covariance to 1.98841e-05
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:171) Singular values of between-class covariance after projecting with interpolated [total/within] covariance with a weight of 0.1 on the total covariance, are:  [ 2.90588 2.40519 2.15112 1.57986 1.19346 1.04706 1.00489 0.626699 0.492955 0.430551 0.339654 0.262772 0.222067 3.39918e-10 1.50561e-11 1.41766e-11 1.3787e-11 1.29261e-11 1.23991e-11 1.20515e-11 1.17956e-11 1.15073e-11 1.10855e-11 1.08423e-11 1.06726e-11 1.043e-11 1.01446e-11 9.95466e-12 9.83686e-12 9.57723e-12 9.48959e-12 9.26464e-12 9.12586e-12 8.87946e-12 8.80499e-12 8.68645e-12 8.51801e-12 8.23287e-12 8.02002e-12 7.93907e-12 7.75004e-12 7.65597e-12 7.50592e-12 7.38407e-12 7.29823e-12 7.26841e-12 7.11195e-12 7.03757e-12 6.84603e-12 6.69938e-12 6.68551e-12 6.52209e-12 6.43416e-12 6.36334e-12 6.18289e-12 6.10256e-12 6.02968e-12 6.02539e-12 5.94713e-12 5.87727e-12 5.75655e-12 5.65056e-12 5.59136e-12 5.47039e-12 5.42325e-12 5.29381e-12 5.1476e-12 5.13813e-12 4.97784e-12 4.89734e-12 4.82425e-12 4.79652e-12 4.68154e-12 4.63167e-12 4.53169e-12 4.4588e-12 4.41556e-12 4.32868e-12 4.29275e-12 4.19057e-12 4.11548e-12 4.02008e-12 3.97968e-12 3.93841e-12 3.84898e-12 3.72921e-12 3.66285e-12 3.59908e-12 3.56063e-12 3.48e-12 3.37638e-12 3.33863e-12 3.26507e-12 3.18899e-12 3.10062e-12 3.06316e-12 3.00453e-12 2.97976e-12 2.94594e-12 2.83621e-12 2.77813e-12 2.72591e-12 2.67029e-12 2.61699e-12 2.5666e-12 2.49457e-12 2.47714e-12 2.4372e-12 2.37685e-12 2.33161e-12 2.30659e-12 2.24263e-12 2.21768e-12 2.1415e-12 2.1314e-12 1.9936e-12 1.98225e-12 1.93959e-12 1.89675e-12 1.84232e-12 1.8375e-12 1.75251e-12 1.6588e-12 1.61799e-12 1.58681e-12 1.56003e-12 1.52131e-12 1.50476e-12 1.46223e-12 1.43422e-12 1.40598e-12 1.37978e-12 1.32589e-12 1.3049e-12 1.24837e-12 1.20679e-12 1.20302e-12 1.16225e-12 1.15229e-12 1.11321e-12 1.08741e-12 1.07572e-12 1.03118e-12 9.95066e-13 9.6071e-13 9.44611e-13 9.38644e-13 9.01184e-13 8.89295e-13 8.61739e-13 8.18741e-13 7.93326e-13 7.64134e-13 7.58101e-13 7.31556e-13 6.96536e-13 6.84815e-13 6.76163e-13 6.65161e-13 6.44676e-13 6.0626e-13 5.88007e-13 5.65442e-13 5.43494e-13 5.38411e-13 5.12856e-13 4.99757e-13 4.87929e-13 4.61777e-13 4.41923e-13 4.31617e-13 4.22456e-13 4.08345e-13 3.9768e-13 3.77143e-13 3.70577e-13 3.58922e-13 3.45225e-13 3.31515e-13 3.10234e-13 2.98156e-13 2.9378e-13 2.80455e-13 2.74832e-13 2.62717e-13 2.545e-13 2.48787e-13 2.43613e-13 2.34035e-13 2.07882e-13 2.05728e-13 1.95553e-13 1.86808e-13 1.84096e-13 1.77587e-13 1.71563e-13 1.62316e-13 1.54437e-13 1.50053e-13 1.3997e-13 1.35626e-13 1.26337e-13 1.23558e-13 1.1968e-13 1.12383e-13 1.02613e-13 9.82605e-14 9.51325e-14 8.87849e-14 8.67413e-14 8.27222e-14 7.68338e-14 7.34747e-14 7.28316e-14 6.4683e-14 6.32607e-14 5.92533e-14 5.8259e-14 5.41779e-14 5.26459e-14 5.00974e-14 4.78823e-14 4.38248e-14 4.2568e-14 3.98685e-14 3.85568e-14 3.42532e-14 3.32714e-14 3.08674e-14 2.871e-14 2.64161e-14 2.59336e-14 2.47206e-14 2.27356e-14 2.05709e-14 2.03413e-14 1.90146e-14 1.80811e-14 1.6708e-14 1.44957e-14 1.36066e-14 1.29644e-14 1.24864e-14 1.06508e-14 1.00594e-14 9.57377e-15 9.0738e-15 7.92316e-15 7.28889e-15 6.41913e-15 6.12398e-15 5.39913e-15 4.97701e-15 3.98205e-15 3.49914e-15 3.1913e-15 2.56876e-15 2.0396e-15 1.4198e-15 1.02199e-15 6.15529e-16 3.15916e-16 -1.96798e-16 -3.17994e-16 -5.90426e-16 -7.74202e-16 -1.11434e-15 -1.5514e-15 -2.05356e-15 -3.02328e-15 -3.13438e-15 -3.45145e-15 -4.22404e-15 -4.74058e-15 -5.23778e-15 -6.017e-15 -6.94501e-15 -7.65427e-15 -7.84041e-15 -8.74914e-15 -9.13164e-15 -9.87051e-15 -1.17554e-14 -1.19892e-14 -1.34698e-14 -1.51032e-14 -1.55803e-14 -1.759e-14 -1.88156e-14 -1.99412e-14 -2.13197e-14 -2.15938e-14 -2.30491e-14 -2.3581e-14 -2.49873e-14 -2.6116e-14 -2.86055e-14 -3.22324e-14 -3.3155e-14 -3.35879e-14 -3.68105e-14 -3.87799e-14 -4.09096e-14 -4.43543e-14 -4.86832e-14 -5.08205e-14 -5.21878e-14 -5.67198e-14 -5.96175e-14 -6.54364e-14 -6.73705e-14 -7.05449e-14 -7.60507e-14 -7.97494e-14 -8.15429e-14 -8.55435e-14 -8.77699e-14 -9.61526e-14 -9.99249e-14 -1.02573e-13 -1.04567e-13 -1.13184e-13 -1.16864e-13 -1.27924e-13 -1.33393e-13 -1.36094e-13 -1.42171e-13 -1.52787e-13 -1.54833e-13 -1.64929e-13 -1.699e-13 -1.79654e-13 -1.83442e-13 -1.97142e-13 -2.11987e-13 -2.2212e-13 -2.24497e-13 -2.29748e-13 -2.32274e-13 -2.42866e-13 -2.52672e-13 -2.72472e-13 -2.80419e-13 -2.89329e-13 -3.01063e-13 -3.12078e-13 -3.31314e-13 -3.44217e-13 -3.53446e-13 -3.59518e-13 -3.73339e-13 -3.91661e-13 -3.99794e-13 -4.22261e-13 -4.36173e-13 -4.581e-13 -4.6864e-13 -5.03118e-13 -5.14217e-13 -5.20797e-13 -5.56986e-13 -5.64542e-13 -5.82459e-13 -6.05782e-13 -6.25377e-13 -6.30576e-13 -6.55691e-13 -6.80004e-13 -6.83333e-13 -7.21023e-13 -7.25088e-13 -7.50613e-13 -7.70865e-13 -8.13039e-13 -8.25481e-13 -8.44359e-13 -8.4773e-13 -9.02988e-13 -9.20939e-13 -9.4504e-13 -9.96507e-13 -1.01588e-12 -1.03602e-12 -1.07917e-12 -1.09231e-12 -1.11465e-12 -1.1499e-12 -1.16233e-12 -1.19555e-12 -1.23182e-12 -1.26457e-12 -1.31607e-12 -1.33455e-12 -1.34232e-12 -1.35921e-12 -1.44e-12 -1.45164e-12 -1.48693e-12 -1.52027e-12 -1.55585e-12 -1.60597e-12 -1.61634e-12 -1.65232e-12 -1.69198e-12 -1.7438e-12 -1.77951e-12 -1.8239e-12 -1.85721e-12 -1.89554e-12 -1.95747e-12 -2.02753e-12 -2.06826e-12 -2.13031e-12 -2.13196e-12 -2.15219e-12 -2.27799e-12 -2.31803e-12 -2.34879e-12 -2.40175e-12 -2.47899e-12 -2.545e-12 -2.59225e-12 -2.60742e-12 -2.62943e-12 -2.66125e-12 -2.76783e-12 -2.81567e-12 -2.90772e-12 -2.96702e-12 -3.01653e-12 -3.15335e-12 -3.23855e-12 -3.24751e-12 -3.3031e-12 -3.33305e-12 -3.40871e-12 -3.45337e-12 -3.50447e-12 -3.61073e-12 -3.64742e-12 -3.72637e-12 -3.77371e-12 -3.84323e-12 -3.90057e-12 -3.97848e-12 -4.07818e-12 -4.12454e-12 -4.15046e-12 -4.23121e-12 -4.29324e-12 -4.37574e-12 -4.44523e-12 -4.54149e-12 -4.65499e-12 -4.76751e-12 -4.80435e-12 -4.88517e-12 -4.90975e-12 -5.01624e-12 -5.16949e-12 -5.22383e-12 -5.35253e-12 -5.36621e-12 -5.60203e-12 -5.66449e-12 -5.75557e-12 -5.87071e-12 -5.91888e-12 -5.98385e-12 -6.03217e-12 -6.2059e-12 -6.35759e-12 -6.49219e-12 -6.5615e-12 -6.6576e-12 -6.72985e-12 -6.80709e-12 -6.98579e-12 -7.06008e-12 -7.25475e-12 -7.41201e-12 -7.61434e-12 -7.68743e-12 -7.79264e-12 -7.90597e-12 -7.99185e-12 -8.1357e-12 -8.2733e-12 -8.52011e-12 -8.68321e-12 -8.72805e-12 -8.78545e-12 -9.14293e-12 -9.36166e-12 -9.41481e-12 -9.55606e-12 -9.68282e-12 -1.0036e-11 -1.01177e-11 -1.02118e-11 -1.02935e-11 -1.06209e-11 -1.0927e-11 -1.13614e-11 -1.14727e-11 -1.16499e-11 -1.21045e-11 -1.23256e-11 -1.28173e-11 -1.37283e-11 -1.45226e-11 -1.54851e-11 ]

LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:318) Wrote LDA transform to pretrained-model/lre17_train_30s/transform_100.mat
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 492731 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 1.53327 (should be close to zero), length divided by sqrt(dim) was 1.2004 (should probably be close to one)
ivector-mean ark:pretrained-model/lre17_train_30s/xvector_lda100.ark pretrained-model/lre17_train_30s/xvector_lda100.global.vec 
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_train_30s/xvector_lda100.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 492731 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.12033, standard deviation was 0.431066
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_eval_10s/xvector.scp ark:pretrained-model/lre17_eval_10s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 7310 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 1.77802 (should be close to zero), length divided by sqrt(dim) was 1.13097 (should probably be close to one)
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_eval_10s/xvector_lda100.ark ark:pretrained-model/lre17_eval_10s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 7310 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_eval_10s/xvector_lda100_submean.ark ark:pretrained-model/lre17_eval_10s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 7310 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.00648, standard deviation was 0.576888
logistic-regression-train --max-steps=20 --mix-up=0 ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark ark:data/lre17_train_30s/utt2label pretrained-model/lre17_train_30s/lr/lr.raw.model 
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance ara-acm-ar-20031217-063700-0-b-40228-40378
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance ara-apc-arb-lev-20040721-231950-a-03109-03260
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance ara-apc-fla-0585-b-22127-22303
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance eng-usg-en-4902-b-50689-50953
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance spa-car-20050929-220820-154-fsp-a-65613-65764
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance spa-lac-20051213-185913-737-fsp-b-35030-35185
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance zho-cmn-TM-003970-b-52704-52901
LOG (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:90) Retrieved 492731 vectors with 7 missing. There were 14 class labels.
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -2.6281
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -1.09791
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.756456
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.666002
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.619344
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.608191
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.59609
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.593833
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.589134
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.585039
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.580517
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.58006
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.577648
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.577063
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.576048
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.575238
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.574579
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.574178
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.574065
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.573982
LOG (logistic-regression-train[5.5.883~1-3eea3]:Train():logistic-regression.cc:58) Finished training parameters without mixture components.
logistic-regression-copy pretrained-model/lre17_train_30s/lr/lr.raw.model pretrained-model/lre17_train_30s/lr/lr.scale.model 
LOG (logistic-regression-copy[5.5.883~1-3eea3]:main():logistic-regression-copy.cc:71) Wrote model to pretrained-model/lre17_train_30s/lr/lr.scale.model
logistic-regression-eval --apply-log=true --max-steps=20 --mix-up=0 pretrained-model/lre17_train_30s/lr/lr.scale.model ark:pretrained-model/lre17_train_30s/lr/trials.label ark:pretrained-model/lre17_eval_10s/xvector_lda100_submean_norm.ark - 
LOG (logistic-regression-eval[5.5.883~1-3eea3]:ComputeScores():logistic-regression-eval.cc:111) Calculated scores for 102340 vectors with 0 missing. 
Cavg: 0.1009

[ lre17_eval_10s ]
Cavg: 0.1009	pretrained-model/lre17_eval_10s/score/lr_lre17_train_30s_lre17_eval_10s_lda100_submean_norm.Cavg


#LOG:: getScore use lr Done!
############################
lre17_train_30s
lre17_train_30s
lre17_eval_10s
true
############################
#LOG::Steps 01 Check and generate config Done
[Auto find] Your vectortype is xvector

#LOG Steps02::Determine Your vectortype is xvector Done
[Notice] It will set the default config lre17_train_30s[lre17_train_30s lre17_train_30s lre17_eval_10s] for lda, submean and whiten, if used.
#LOG Steps03::Configuration information Done
steps4 start
Vectordir:pretrained-model
lre17_eval_10s
lre17_train_30s
#LOG Steps04::Check x-vector or I-vector generation error Done
##Step5 start
lre17_train_30s[lre17_train_30s lre17_train_30s lre17_eval_10s]
#LOG Steps05:: Done
#LOG Steps06::Build connection between score process and enrollset Done
#LOG Steps07::False to speed up and true to clear files with error   Done
#LOG Steps08::Get Trials Done
#LOG Steps09:: Done
[ plda ]
ivector-mean ark:data/lre17_train_30s/spk2utt scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/spk_xvector_mean.ark ark,t:pretrained-model/lre17_train_30s/num_utts.ark 
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance ara-acm-ar-20031217-063700-0-b-40228-40378
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance ara-apc-arb-lev-20040721-231950-a-03109-03260
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance ara-apc-fla-0585-b-22127-22303
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance eng-usg-en-4902-b-50689-50953
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance spa-car-20050929-220820-154-fsp-a-65613-65764
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance spa-lac-20051213-185913-737-fsp-b-35030-35185
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance zho-cmn-TM-003970-b-52704-52901
LOG (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:134) Computed mean of 14 speakers (0 with no utterances), consisting of 492731 utterances (7 absent from input).
LOG (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:145) Norm of mean of speakers is 17.7422, root-mean-square speaker-iVector length divided by sqrt(dim) is 1.29274
ivector-normalize-length scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.26335, standard deviation was 0.352726
ivector-compute-lda --dim=100 --total-covariance-factor=0.1 ark:pretrained-model/lre17_train_30s/xvector_norm.ark ark:data/lre17_train_30s/utt2spk pretrained-model/lre17_train_30s/transform_100.mat 
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:288) Read 492731 utterances, 0 with errors.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:294) Computing within-class covariance.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:299) 2-norm of iVector mean is 15.9953
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:136) Stats have 14 speakers, 492731 utterances. 
WARNING (ivector-compute-lda[5.5.883~1-3eea3]:ComputeNormalizingTransform():ivector-compute-lda.cc:98) Floored 129 eigenvalues of covariance to 1.98841e-05
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:171) Singular values of between-class covariance after projecting with interpolated [total/within] covariance with a weight of 0.1 on the total covariance, are:  [ 2.90588 2.40519 2.15112 1.57986 1.19346 1.04706 1.00489 0.626699 0.492955 0.430551 0.339654 0.262772 0.222067 3.39918e-10 1.50561e-11 1.41766e-11 1.3787e-11 1.29261e-11 1.23991e-11 1.20515e-11 1.17956e-11 1.15073e-11 1.10855e-11 1.08423e-11 1.06726e-11 1.043e-11 1.01446e-11 9.95466e-12 9.83686e-12 9.57723e-12 9.48959e-12 9.26464e-12 9.12586e-12 8.87946e-12 8.80499e-12 8.68645e-12 8.51801e-12 8.23287e-12 8.02002e-12 7.93907e-12 7.75004e-12 7.65597e-12 7.50592e-12 7.38407e-12 7.29823e-12 7.26841e-12 7.11195e-12 7.03757e-12 6.84603e-12 6.69938e-12 6.68551e-12 6.52209e-12 6.43416e-12 6.36334e-12 6.18289e-12 6.10256e-12 6.02968e-12 6.02539e-12 5.94713e-12 5.87727e-12 5.75655e-12 5.65056e-12 5.59136e-12 5.47039e-12 5.42325e-12 5.29381e-12 5.1476e-12 5.13813e-12 4.97784e-12 4.89734e-12 4.82425e-12 4.79652e-12 4.68154e-12 4.63167e-12 4.53169e-12 4.4588e-12 4.41556e-12 4.32868e-12 4.29275e-12 4.19057e-12 4.11548e-12 4.02008e-12 3.97968e-12 3.93841e-12 3.84898e-12 3.72921e-12 3.66285e-12 3.59908e-12 3.56063e-12 3.48e-12 3.37638e-12 3.33863e-12 3.26507e-12 3.18899e-12 3.10062e-12 3.06316e-12 3.00453e-12 2.97976e-12 2.94594e-12 2.83621e-12 2.77813e-12 2.72591e-12 2.67029e-12 2.61699e-12 2.5666e-12 2.49457e-12 2.47714e-12 2.4372e-12 2.37685e-12 2.33161e-12 2.30659e-12 2.24263e-12 2.21768e-12 2.1415e-12 2.1314e-12 1.9936e-12 1.98225e-12 1.93959e-12 1.89675e-12 1.84232e-12 1.8375e-12 1.75251e-12 1.6588e-12 1.61799e-12 1.58681e-12 1.56003e-12 1.52131e-12 1.50476e-12 1.46223e-12 1.43422e-12 1.40598e-12 1.37978e-12 1.32589e-12 1.3049e-12 1.24837e-12 1.20679e-12 1.20302e-12 1.16225e-12 1.15229e-12 1.11321e-12 1.08741e-12 1.07572e-12 1.03118e-12 9.95066e-13 9.6071e-13 9.44611e-13 9.38644e-13 9.01184e-13 8.89295e-13 8.61739e-13 8.18741e-13 7.93326e-13 7.64134e-13 7.58101e-13 7.31556e-13 6.96536e-13 6.84815e-13 6.76163e-13 6.65161e-13 6.44676e-13 6.0626e-13 5.88007e-13 5.65442e-13 5.43494e-13 5.38411e-13 5.12856e-13 4.99757e-13 4.87929e-13 4.61777e-13 4.41923e-13 4.31617e-13 4.22456e-13 4.08345e-13 3.9768e-13 3.77143e-13 3.70577e-13 3.58922e-13 3.45225e-13 3.31515e-13 3.10234e-13 2.98156e-13 2.9378e-13 2.80455e-13 2.74832e-13 2.62717e-13 2.545e-13 2.48787e-13 2.43613e-13 2.34035e-13 2.07882e-13 2.05728e-13 1.95553e-13 1.86808e-13 1.84096e-13 1.77587e-13 1.71563e-13 1.62316e-13 1.54437e-13 1.50053e-13 1.3997e-13 1.35626e-13 1.26337e-13 1.23558e-13 1.1968e-13 1.12383e-13 1.02613e-13 9.82605e-14 9.51325e-14 8.87849e-14 8.67413e-14 8.27222e-14 7.68338e-14 7.34747e-14 7.28316e-14 6.4683e-14 6.32607e-14 5.92533e-14 5.8259e-14 5.41779e-14 5.26459e-14 5.00974e-14 4.78823e-14 4.38248e-14 4.2568e-14 3.98685e-14 3.85568e-14 3.42532e-14 3.32714e-14 3.08674e-14 2.871e-14 2.64161e-14 2.59336e-14 2.47206e-14 2.27356e-14 2.05709e-14 2.03413e-14 1.90146e-14 1.80811e-14 1.6708e-14 1.44957e-14 1.36066e-14 1.29644e-14 1.24864e-14 1.06508e-14 1.00594e-14 9.57377e-15 9.0738e-15 7.92316e-15 7.28889e-15 6.41913e-15 6.12398e-15 5.39913e-15 4.97701e-15 3.98205e-15 3.49914e-15 3.1913e-15 2.56876e-15 2.0396e-15 1.4198e-15 1.02199e-15 6.15529e-16 3.15916e-16 -1.96798e-16 -3.17994e-16 -5.90426e-16 -7.74202e-16 -1.11434e-15 -1.5514e-15 -2.05356e-15 -3.02328e-15 -3.13438e-15 -3.45145e-15 -4.22404e-15 -4.74058e-15 -5.23778e-15 -6.017e-15 -6.94501e-15 -7.65427e-15 -7.84041e-15 -8.74914e-15 -9.13164e-15 -9.87051e-15 -1.17554e-14 -1.19892e-14 -1.34698e-14 -1.51032e-14 -1.55803e-14 -1.759e-14 -1.88156e-14 -1.99412e-14 -2.13197e-14 -2.15938e-14 -2.30491e-14 -2.3581e-14 -2.49873e-14 -2.6116e-14 -2.86055e-14 -3.22324e-14 -3.3155e-14 -3.35879e-14 -3.68105e-14 -3.87799e-14 -4.09096e-14 -4.43543e-14 -4.86832e-14 -5.08205e-14 -5.21878e-14 -5.67198e-14 -5.96175e-14 -6.54364e-14 -6.73705e-14 -7.05449e-14 -7.60507e-14 -7.97494e-14 -8.15429e-14 -8.55435e-14 -8.77699e-14 -9.61526e-14 -9.99249e-14 -1.02573e-13 -1.04567e-13 -1.13184e-13 -1.16864e-13 -1.27924e-13 -1.33393e-13 -1.36094e-13 -1.42171e-13 -1.52787e-13 -1.54833e-13 -1.64929e-13 -1.699e-13 -1.79654e-13 -1.83442e-13 -1.97142e-13 -2.11987e-13 -2.2212e-13 -2.24497e-13 -2.29748e-13 -2.32274e-13 -2.42866e-13 -2.52672e-13 -2.72472e-13 -2.80419e-13 -2.89329e-13 -3.01063e-13 -3.12078e-13 -3.31314e-13 -3.44217e-13 -3.53446e-13 -3.59518e-13 -3.73339e-13 -3.91661e-13 -3.99794e-13 -4.22261e-13 -4.36173e-13 -4.581e-13 -4.6864e-13 -5.03118e-13 -5.14217e-13 -5.20797e-13 -5.56986e-13 -5.64542e-13 -5.82459e-13 -6.05782e-13 -6.25377e-13 -6.30576e-13 -6.55691e-13 -6.80004e-13 -6.83333e-13 -7.21023e-13 -7.25088e-13 -7.50613e-13 -7.70865e-13 -8.13039e-13 -8.25481e-13 -8.44359e-13 -8.4773e-13 -9.02988e-13 -9.20939e-13 -9.4504e-13 -9.96507e-13 -1.01588e-12 -1.03602e-12 -1.07917e-12 -1.09231e-12 -1.11465e-12 -1.1499e-12 -1.16233e-12 -1.19555e-12 -1.23182e-12 -1.26457e-12 -1.31607e-12 -1.33455e-12 -1.34232e-12 -1.35921e-12 -1.44e-12 -1.45164e-12 -1.48693e-12 -1.52027e-12 -1.55585e-12 -1.60597e-12 -1.61634e-12 -1.65232e-12 -1.69198e-12 -1.7438e-12 -1.77951e-12 -1.8239e-12 -1.85721e-12 -1.89554e-12 -1.95747e-12 -2.02753e-12 -2.06826e-12 -2.13031e-12 -2.13196e-12 -2.15219e-12 -2.27799e-12 -2.31803e-12 -2.34879e-12 -2.40175e-12 -2.47899e-12 -2.545e-12 -2.59225e-12 -2.60742e-12 -2.62943e-12 -2.66125e-12 -2.76783e-12 -2.81567e-12 -2.90772e-12 -2.96702e-12 -3.01653e-12 -3.15335e-12 -3.23855e-12 -3.24751e-12 -3.3031e-12 -3.33305e-12 -3.40871e-12 -3.45337e-12 -3.50447e-12 -3.61073e-12 -3.64742e-12 -3.72637e-12 -3.77371e-12 -3.84323e-12 -3.90057e-12 -3.97848e-12 -4.07818e-12 -4.12454e-12 -4.15046e-12 -4.23121e-12 -4.29324e-12 -4.37574e-12 -4.44523e-12 -4.54149e-12 -4.65499e-12 -4.76751e-12 -4.80435e-12 -4.88517e-12 -4.90975e-12 -5.01624e-12 -5.16949e-12 -5.22383e-12 -5.35253e-12 -5.36621e-12 -5.60203e-12 -5.66449e-12 -5.75557e-12 -5.87071e-12 -5.91888e-12 -5.98385e-12 -6.03217e-12 -6.2059e-12 -6.35759e-12 -6.49219e-12 -6.5615e-12 -6.6576e-12 -6.72985e-12 -6.80709e-12 -6.98579e-12 -7.06008e-12 -7.25475e-12 -7.41201e-12 -7.61434e-12 -7.68743e-12 -7.79264e-12 -7.90597e-12 -7.99185e-12 -8.1357e-12 -8.2733e-12 -8.52011e-12 -8.68321e-12 -8.72805e-12 -8.78545e-12 -9.14293e-12 -9.36166e-12 -9.41481e-12 -9.55606e-12 -9.68282e-12 -1.0036e-11 -1.01177e-11 -1.02118e-11 -1.02935e-11 -1.06209e-11 -1.0927e-11 -1.13614e-11 -1.14727e-11 -1.16499e-11 -1.21045e-11 -1.23256e-11 -1.28173e-11 -1.37283e-11 -1.45226e-11 -1.54851e-11 ]

LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:318) Wrote LDA transform to pretrained-model/lre17_train_30s/transform_100.mat
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat ark:pretrained-model/lre17_train_30s/spk_xvector_mean.ark ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 14 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 2.75929 (should be close to zero), length divided by sqrt(dim) was 1.40767 (should probably be close to one)
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 492731 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 1.53327 (should be close to zero), length divided by sqrt(dim) was 1.2004 (should probably be close to one)
ivector-mean ark:pretrained-model/lre17_train_30s/xvector_lda100.ark pretrained-model/lre17_train_30s/xvector_lda100.global.vec 
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100.ark ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 14 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean.ark ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 14 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.00249, standard deviation was 1.05391
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_eval_10s/xvector.scp ark:pretrained-model/lre17_eval_10s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 7310 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 1.77802 (should be close to zero), length divided by sqrt(dim) was 1.13097 (should probably be close to one)
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_eval_10s/xvector_lda100.ark ark:pretrained-model/lre17_eval_10s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 7310 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_eval_10s/xvector_lda100_submean.ark ark:pretrained-model/lre17_eval_10s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 7310 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.00648, standard deviation was 0.576888
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_train_30s/xvector_lda100.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 492731 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.12033, standard deviation was 0.431066
ivector-compute-plda ark:data/lre17_train_30s/spk2utt ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark pretrained-model/lre17_train_30s/plda 
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance ara-acm-ar-20031217-063700-0-b-40228-40378
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance ara-apc-arb-lev-20040721-231950-a-03109-03260
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance ara-apc-fla-0585-b-22127-22303
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance eng-usg-en-4902-b-50689-50953
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance spa-car-20050929-220820-154-fsp-a-65613-65764
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance spa-lac-20051213-185913-737-fsp-b-35030-35185
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance zho-cmn-TM-003970-b-52704-52901
LOG (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:109) Accumulated stats from 14 speakers (0 with no utterances), consisting of 492731 utterances (7 absent from input).
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 0 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0484
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9578
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 1 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0474
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9649
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 2 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.047
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.962
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 3 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0468
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9611
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 4 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0466
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9607
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 5 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0465
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9604
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 6 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0464
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9603
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 7 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0463
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9602
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 8 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0463
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9601
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 9 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0462
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9601
LOG (ivector-compute-plda[5.5.883~1-3eea3]:GetOutput():plda.cc:540) Norm of mean of iVector distribution is 1.36617
LOG (ivector-compute-plda[5.5.883~1-3eea3]:GetOutput():plda.cc:577) Diagonal of between-class variance in normalized space is  [ 6.80719 5.07642 4.21652 3.83125 3.16111 2.9845 1.87321 1.66839 1.41967 1.1445 1.04717 0.410991 0.353115 4.81664e-06 4.80518e-06 4.79952e-06 4.78659e-06 4.78533e-06 4.76916e-06 4.76006e-06 4.75583e-06 4.74886e-06 4.74146e-06 4.73521e-06 4.7278e-06 4.72638e-06 4.72055e-06 4.71931e-06 4.71476e-06 4.71429e-06 4.70611e-06 4.70559e-06 4.70065e-06 4.69463e-06 4.68495e-06 4.68266e-06 4.67751e-06 4.67639e-06 4.67314e-06 4.66825e-06 4.66532e-06 4.6579e-06 4.65547e-06 4.65337e-06 4.64883e-06 4.64267e-06 4.63883e-06 4.63503e-06 4.63125e-06 4.62836e-06 4.62584e-06 4.62273e-06 4.61574e-06 4.61444e-06 4.61382e-06 4.61116e-06 4.60593e-06 4.6043e-06 4.60154e-06 4.59664e-06 4.5945e-06 4.59077e-06 4.5904e-06 4.58557e-06 4.58489e-06 4.58178e-06 4.57969e-06 4.57774e-06 4.5726e-06 4.56954e-06 4.56696e-06 4.56051e-06 4.55849e-06 4.55698e-06 4.55087e-06 4.54951e-06 4.54885e-06 4.54784e-06 4.54052e-06 4.53963e-06 4.53844e-06 4.53773e-06 4.53246e-06 4.52768e-06 4.52645e-06 4.52513e-06 4.52103e-06 4.51922e-06 4.51895e-06 4.51649e-06 4.51228e-06 4.51026e-06 4.50859e-06 4.5083e-06 4.5071e-06 4.5048e-06 4.50242e-06 4.49714e-06 4.48988e-06 4.43171e-06 ]

ivector-plda-scoring --normalize-length=true --num-utts=ark:pretrained-model/lre17_train_30s/num_utts.ark 'ivector-copy-plda --smoothing=0.0 pretrained-model/lre17_train_30s/plda - |' ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean_norm.ark ark:pretrained-model/lre17_eval_10s/xvector_lda100_submean_norm.ark - pretrained-model/lre17_eval_10s/score/plda_lre17_train_30s_lre17_eval_10s_lda100_submean_norm.score 
ivector-copy-plda --smoothing=0.0 pretrained-model/lre17_train_30s/plda - 
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:96) Reading train iVectors
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:122) Read 14 training iVectors, errors on 0
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:126) Average renormalization scale on training iVectors was 0.375309
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:129) Reading test iVectors
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:147) Read 7310 test iVectors.
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:150) Average renormalization scale on test iVectors was 1.15939
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:214) Mean score was -32.3848, standard deviation was 41.7881
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:217) Processed 102340 trials, 0 had errors.
compute-eer - 
LOG (compute-eer[5.5.671~1-e5a5a]:main():compute-eer.cc:136) Equal error rate is 22.435%, at threshold -5.62141
EER% 22.44

[ lre17_eval_10s ]
22.44	pretrained-model/lre17_eval_10s/score/plda_lre17_train_30s_lre17_eval_10s_lda100_submean_norm.eer


############################
lre17_train_30s
lre17_train_30s
lre17_eval_10s
true
############################
#LOG::Steps 01 Check and generate config Done
[Auto find] Your vectortype is xvector

#LOG Steps02::Determine Your vectortype is xvector Done
[Notice] It will set the default config lre17_train_30s[lre17_train_30s lre17_train_30s lre17_eval_10s] for lda, submean and whiten, if used.
#LOG Steps03::Configuration information Done
steps4 start
Vectordir:pretrained-model
lre17_eval_10s
lre17_train_30s
#LOG Steps04::Check x-vector or I-vector generation error Done
##Step5 start
lre17_train_30s[lre17_train_30s lre17_train_30s lre17_eval_10s]
#LOG Steps05:: Done
#LOG Steps06::Build connection between score process and enrollset Done
#LOG Steps07::False to speed up and true to clear files with error   Done
#LOG Steps08::Get Trials Done
#LOG Steps09:: Done
[ plda ]
ivector-mean ark:data/lre17_train_30s/spk2utt scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/spk_xvector_mean.ark ark,t:pretrained-model/lre17_train_30s/num_utts.ark 
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance ara-acm-ar-20031217-063700-0-b-40228-40378
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance ara-apc-arb-lev-20040721-231950-a-03109-03260
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance ara-apc-fla-0585-b-22127-22303
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance eng-usg-en-4902-b-50689-50953
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance spa-car-20050929-220820-154-fsp-a-65613-65764
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance spa-lac-20051213-185913-737-fsp-b-35030-35185
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance zho-cmn-TM-003970-b-52704-52901
LOG (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:134) Computed mean of 14 speakers (0 with no utterances), consisting of 492731 utterances (7 absent from input).
LOG (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:145) Norm of mean of speakers is 17.7422, root-mean-square speaker-iVector length divided by sqrt(dim) is 1.29274
ivector-normalize-length scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.26335, standard deviation was 0.352726
ivector-compute-lda --dim=100 --total-covariance-factor=0.1 ark:pretrained-model/lre17_train_30s/xvector_norm.ark ark:data/lre17_train_30s/utt2spk pretrained-model/lre17_train_30s/transform_100.mat 
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:288) Read 492731 utterances, 0 with errors.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:294) Computing within-class covariance.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:299) 2-norm of iVector mean is 15.9953
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:136) Stats have 14 speakers, 492731 utterances. 
WARNING (ivector-compute-lda[5.5.883~1-3eea3]:ComputeNormalizingTransform():ivector-compute-lda.cc:98) Floored 129 eigenvalues of covariance to 1.98841e-05
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:171) Singular values of between-class covariance after projecting with interpolated [total/within] covariance with a weight of 0.1 on the total covariance, are:  [ 2.90588 2.40519 2.15112 1.57986 1.19346 1.04706 1.00489 0.626699 0.492955 0.430551 0.339654 0.262772 0.222067 3.39918e-10 1.50561e-11 1.41766e-11 1.3787e-11 1.29261e-11 1.23991e-11 1.20515e-11 1.17956e-11 1.15073e-11 1.10855e-11 1.08423e-11 1.06726e-11 1.043e-11 1.01446e-11 9.95466e-12 9.83686e-12 9.57723e-12 9.48959e-12 9.26464e-12 9.12586e-12 8.87946e-12 8.80499e-12 8.68645e-12 8.51801e-12 8.23287e-12 8.02002e-12 7.93907e-12 7.75004e-12 7.65597e-12 7.50592e-12 7.38407e-12 7.29823e-12 7.26841e-12 7.11195e-12 7.03757e-12 6.84603e-12 6.69938e-12 6.68551e-12 6.52209e-12 6.43416e-12 6.36334e-12 6.18289e-12 6.10256e-12 6.02968e-12 6.02539e-12 5.94713e-12 5.87727e-12 5.75655e-12 5.65056e-12 5.59136e-12 5.47039e-12 5.42325e-12 5.29381e-12 5.1476e-12 5.13813e-12 4.97784e-12 4.89734e-12 4.82425e-12 4.79652e-12 4.68154e-12 4.63167e-12 4.53169e-12 4.4588e-12 4.41556e-12 4.32868e-12 4.29275e-12 4.19057e-12 4.11548e-12 4.02008e-12 3.97968e-12 3.93841e-12 3.84898e-12 3.72921e-12 3.66285e-12 3.59908e-12 3.56063e-12 3.48e-12 3.37638e-12 3.33863e-12 3.26507e-12 3.18899e-12 3.10062e-12 3.06316e-12 3.00453e-12 2.97976e-12 2.94594e-12 2.83621e-12 2.77813e-12 2.72591e-12 2.67029e-12 2.61699e-12 2.5666e-12 2.49457e-12 2.47714e-12 2.4372e-12 2.37685e-12 2.33161e-12 2.30659e-12 2.24263e-12 2.21768e-12 2.1415e-12 2.1314e-12 1.9936e-12 1.98225e-12 1.93959e-12 1.89675e-12 1.84232e-12 1.8375e-12 1.75251e-12 1.6588e-12 1.61799e-12 1.58681e-12 1.56003e-12 1.52131e-12 1.50476e-12 1.46223e-12 1.43422e-12 1.40598e-12 1.37978e-12 1.32589e-12 1.3049e-12 1.24837e-12 1.20679e-12 1.20302e-12 1.16225e-12 1.15229e-12 1.11321e-12 1.08741e-12 1.07572e-12 1.03118e-12 9.95066e-13 9.6071e-13 9.44611e-13 9.38644e-13 9.01184e-13 8.89295e-13 8.61739e-13 8.18741e-13 7.93326e-13 7.64134e-13 7.58101e-13 7.31556e-13 6.96536e-13 6.84815e-13 6.76163e-13 6.65161e-13 6.44676e-13 6.0626e-13 5.88007e-13 5.65442e-13 5.43494e-13 5.38411e-13 5.12856e-13 4.99757e-13 4.87929e-13 4.61777e-13 4.41923e-13 4.31617e-13 4.22456e-13 4.08345e-13 3.9768e-13 3.77143e-13 3.70577e-13 3.58922e-13 3.45225e-13 3.31515e-13 3.10234e-13 2.98156e-13 2.9378e-13 2.80455e-13 2.74832e-13 2.62717e-13 2.545e-13 2.48787e-13 2.43613e-13 2.34035e-13 2.07882e-13 2.05728e-13 1.95553e-13 1.86808e-13 1.84096e-13 1.77587e-13 1.71563e-13 1.62316e-13 1.54437e-13 1.50053e-13 1.3997e-13 1.35626e-13 1.26337e-13 1.23558e-13 1.1968e-13 1.12383e-13 1.02613e-13 9.82605e-14 9.51325e-14 8.87849e-14 8.67413e-14 8.27222e-14 7.68338e-14 7.34747e-14 7.28316e-14 6.4683e-14 6.32607e-14 5.92533e-14 5.8259e-14 5.41779e-14 5.26459e-14 5.00974e-14 4.78823e-14 4.38248e-14 4.2568e-14 3.98685e-14 3.85568e-14 3.42532e-14 3.32714e-14 3.08674e-14 2.871e-14 2.64161e-14 2.59336e-14 2.47206e-14 2.27356e-14 2.05709e-14 2.03413e-14 1.90146e-14 1.80811e-14 1.6708e-14 1.44957e-14 1.36066e-14 1.29644e-14 1.24864e-14 1.06508e-14 1.00594e-14 9.57377e-15 9.0738e-15 7.92316e-15 7.28889e-15 6.41913e-15 6.12398e-15 5.39913e-15 4.97701e-15 3.98205e-15 3.49914e-15 3.1913e-15 2.56876e-15 2.0396e-15 1.4198e-15 1.02199e-15 6.15529e-16 3.15916e-16 -1.96798e-16 -3.17994e-16 -5.90426e-16 -7.74202e-16 -1.11434e-15 -1.5514e-15 -2.05356e-15 -3.02328e-15 -3.13438e-15 -3.45145e-15 -4.22404e-15 -4.74058e-15 -5.23778e-15 -6.017e-15 -6.94501e-15 -7.65427e-15 -7.84041e-15 -8.74914e-15 -9.13164e-15 -9.87051e-15 -1.17554e-14 -1.19892e-14 -1.34698e-14 -1.51032e-14 -1.55803e-14 -1.759e-14 -1.88156e-14 -1.99412e-14 -2.13197e-14 -2.15938e-14 -2.30491e-14 -2.3581e-14 -2.49873e-14 -2.6116e-14 -2.86055e-14 -3.22324e-14 -3.3155e-14 -3.35879e-14 -3.68105e-14 -3.87799e-14 -4.09096e-14 -4.43543e-14 -4.86832e-14 -5.08205e-14 -5.21878e-14 -5.67198e-14 -5.96175e-14 -6.54364e-14 -6.73705e-14 -7.05449e-14 -7.60507e-14 -7.97494e-14 -8.15429e-14 -8.55435e-14 -8.77699e-14 -9.61526e-14 -9.99249e-14 -1.02573e-13 -1.04567e-13 -1.13184e-13 -1.16864e-13 -1.27924e-13 -1.33393e-13 -1.36094e-13 -1.42171e-13 -1.52787e-13 -1.54833e-13 -1.64929e-13 -1.699e-13 -1.79654e-13 -1.83442e-13 -1.97142e-13 -2.11987e-13 -2.2212e-13 -2.24497e-13 -2.29748e-13 -2.32274e-13 -2.42866e-13 -2.52672e-13 -2.72472e-13 -2.80419e-13 -2.89329e-13 -3.01063e-13 -3.12078e-13 -3.31314e-13 -3.44217e-13 -3.53446e-13 -3.59518e-13 -3.73339e-13 -3.91661e-13 -3.99794e-13 -4.22261e-13 -4.36173e-13 -4.581e-13 -4.6864e-13 -5.03118e-13 -5.14217e-13 -5.20797e-13 -5.56986e-13 -5.64542e-13 -5.82459e-13 -6.05782e-13 -6.25377e-13 -6.30576e-13 -6.55691e-13 -6.80004e-13 -6.83333e-13 -7.21023e-13 -7.25088e-13 -7.50613e-13 -7.70865e-13 -8.13039e-13 -8.25481e-13 -8.44359e-13 -8.4773e-13 -9.02988e-13 -9.20939e-13 -9.4504e-13 -9.96507e-13 -1.01588e-12 -1.03602e-12 -1.07917e-12 -1.09231e-12 -1.11465e-12 -1.1499e-12 -1.16233e-12 -1.19555e-12 -1.23182e-12 -1.26457e-12 -1.31607e-12 -1.33455e-12 -1.34232e-12 -1.35921e-12 -1.44e-12 -1.45164e-12 -1.48693e-12 -1.52027e-12 -1.55585e-12 -1.60597e-12 -1.61634e-12 -1.65232e-12 -1.69198e-12 -1.7438e-12 -1.77951e-12 -1.8239e-12 -1.85721e-12 -1.89554e-12 -1.95747e-12 -2.02753e-12 -2.06826e-12 -2.13031e-12 -2.13196e-12 -2.15219e-12 -2.27799e-12 -2.31803e-12 -2.34879e-12 -2.40175e-12 -2.47899e-12 -2.545e-12 -2.59225e-12 -2.60742e-12 -2.62943e-12 -2.66125e-12 -2.76783e-12 -2.81567e-12 -2.90772e-12 -2.96702e-12 -3.01653e-12 -3.15335e-12 -3.23855e-12 -3.24751e-12 -3.3031e-12 -3.33305e-12 -3.40871e-12 -3.45337e-12 -3.50447e-12 -3.61073e-12 -3.64742e-12 -3.72637e-12 -3.77371e-12 -3.84323e-12 -3.90057e-12 -3.97848e-12 -4.07818e-12 -4.12454e-12 -4.15046e-12 -4.23121e-12 -4.29324e-12 -4.37574e-12 -4.44523e-12 -4.54149e-12 -4.65499e-12 -4.76751e-12 -4.80435e-12 -4.88517e-12 -4.90975e-12 -5.01624e-12 -5.16949e-12 -5.22383e-12 -5.35253e-12 -5.36621e-12 -5.60203e-12 -5.66449e-12 -5.75557e-12 -5.87071e-12 -5.91888e-12 -5.98385e-12 -6.03217e-12 -6.2059e-12 -6.35759e-12 -6.49219e-12 -6.5615e-12 -6.6576e-12 -6.72985e-12 -6.80709e-12 -6.98579e-12 -7.06008e-12 -7.25475e-12 -7.41201e-12 -7.61434e-12 -7.68743e-12 -7.79264e-12 -7.90597e-12 -7.99185e-12 -8.1357e-12 -8.2733e-12 -8.52011e-12 -8.68321e-12 -8.72805e-12 -8.78545e-12 -9.14293e-12 -9.36166e-12 -9.41481e-12 -9.55606e-12 -9.68282e-12 -1.0036e-11 -1.01177e-11 -1.02118e-11 -1.02935e-11 -1.06209e-11 -1.0927e-11 -1.13614e-11 -1.14727e-11 -1.16499e-11 -1.21045e-11 -1.23256e-11 -1.28173e-11 -1.37283e-11 -1.45226e-11 -1.54851e-11 ]

LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:318) Wrote LDA transform to pretrained-model/lre17_train_30s/transform_100.mat
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat ark:pretrained-model/lre17_train_30s/spk_xvector_mean.ark ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 14 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 2.75929 (should be close to zero), length divided by sqrt(dim) was 1.40767 (should probably be close to one)
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 492731 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 1.53327 (should be close to zero), length divided by sqrt(dim) was 1.2004 (should probably be close to one)
ivector-mean ark:pretrained-model/lre17_train_30s/xvector_lda100.ark pretrained-model/lre17_train_30s/xvector_lda100.global.vec 
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100.ark ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 14 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean.ark ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 14 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.00249, standard deviation was 1.05391
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_eval_10s/xvector.scp ark:pretrained-model/lre17_eval_10s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 7310 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 1.77802 (should be close to zero), length divided by sqrt(dim) was 1.13097 (should probably be close to one)
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_eval_10s/xvector_lda100.ark ark:pretrained-model/lre17_eval_10s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 7310 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_eval_10s/xvector_lda100_submean.ark ark:pretrained-model/lre17_eval_10s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 7310 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.00648, standard deviation was 0.576888
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_train_30s/xvector_lda100.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 492731 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.12033, standard deviation was 0.431066
ivector-compute-plda ark:data/lre17_train_30s/spk2utt ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark pretrained-model/lre17_train_30s/plda 
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance ara-acm-ar-20031217-063700-0-b-40228-40378
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance ara-apc-arb-lev-20040721-231950-a-03109-03260
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance ara-apc-fla-0585-b-22127-22303
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance eng-usg-en-4902-b-50689-50953
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance spa-car-20050929-220820-154-fsp-a-65613-65764
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance spa-lac-20051213-185913-737-fsp-b-35030-35185
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance zho-cmn-TM-003970-b-52704-52901
LOG (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:109) Accumulated stats from 14 speakers (0 with no utterances), consisting of 492731 utterances (7 absent from input).
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 0 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0484
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9578
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 1 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0474
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9649
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 2 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.047
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.962
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 3 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0468
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9611
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 4 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0466
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9607
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 5 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0465
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9604
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 6 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0464
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9603
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 7 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0463
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9602
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 8 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0463
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9601
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 9 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0462
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9601
LOG (ivector-compute-plda[5.5.883~1-3eea3]:GetOutput():plda.cc:540) Norm of mean of iVector distribution is 1.36617
LOG (ivector-compute-plda[5.5.883~1-3eea3]:GetOutput():plda.cc:577) Diagonal of between-class variance in normalized space is  [ 6.80719 5.07642 4.21652 3.83125 3.16111 2.9845 1.87321 1.66839 1.41967 1.1445 1.04717 0.410991 0.353115 4.81664e-06 4.80518e-06 4.79952e-06 4.78659e-06 4.78533e-06 4.76916e-06 4.76006e-06 4.75583e-06 4.74886e-06 4.74146e-06 4.73521e-06 4.7278e-06 4.72638e-06 4.72055e-06 4.71931e-06 4.71476e-06 4.71429e-06 4.70611e-06 4.70559e-06 4.70065e-06 4.69463e-06 4.68495e-06 4.68266e-06 4.67751e-06 4.67639e-06 4.67314e-06 4.66825e-06 4.66532e-06 4.6579e-06 4.65547e-06 4.65337e-06 4.64883e-06 4.64267e-06 4.63883e-06 4.63503e-06 4.63125e-06 4.62836e-06 4.62584e-06 4.62273e-06 4.61574e-06 4.61444e-06 4.61382e-06 4.61116e-06 4.60593e-06 4.6043e-06 4.60154e-06 4.59664e-06 4.5945e-06 4.59077e-06 4.5904e-06 4.58557e-06 4.58489e-06 4.58178e-06 4.57969e-06 4.57774e-06 4.5726e-06 4.56954e-06 4.56696e-06 4.56051e-06 4.55849e-06 4.55698e-06 4.55087e-06 4.54951e-06 4.54885e-06 4.54784e-06 4.54052e-06 4.53963e-06 4.53844e-06 4.53773e-06 4.53246e-06 4.52768e-06 4.52645e-06 4.52513e-06 4.52103e-06 4.51922e-06 4.51895e-06 4.51649e-06 4.51228e-06 4.51026e-06 4.50859e-06 4.5083e-06 4.5071e-06 4.5048e-06 4.50242e-06 4.49714e-06 4.48988e-06 4.43171e-06 ]

ivector-plda-scoring --normalize-length=true --num-utts=ark:pretrained-model/lre17_train_30s/num_utts.ark 'ivector-copy-plda --smoothing=0.0 pretrained-model/lre17_train_30s/plda - |' ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean_norm.ark ark:pretrained-model/lre17_eval_10s/xvector_lda100_submean_norm.ark - pretrained-model/lre17_eval_10s/score/plda_lre17_train_30s_lre17_eval_10s_lda100_submean_norm.score 
ivector-copy-plda --smoothing=0.0 pretrained-model/lre17_train_30s/plda - 
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:96) Reading train iVectors
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:122) Read 14 training iVectors, errors on 0
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:126) Average renormalization scale on training iVectors was 0.375309
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:129) Reading test iVectors
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:147) Read 7310 test iVectors.
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:150) Average renormalization scale on test iVectors was 1.15939
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:214) Mean score was -32.3848, standard deviation was 41.7881
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:217) Processed 102340 trials, 0 had errors.
Cavg: 0.2408

[ lre17_eval_10s ]
Cavg: 0.2408	pretrained-model/lre17_eval_10s/score/plda_lre17_train_30s_lre17_eval_10s_lda100_submean_norm.Cavg


#LOG:: getScore use plda Done!
############################
lre17_train_30s
lre17_train_30s
lre17_eval_30s
true
############################
#LOG::Steps 01 Check and generate config Done
[Auto find] Your vectortype is xvector

#LOG Steps02::Determine Your vectortype is xvector Done
[Notice] It will set the default config lre17_train_30s[lre17_train_30s lre17_train_30s lre17_eval_30s] for lda, submean and whiten, if used.
#LOG Steps03::Configuration information Done
steps4 start
Vectordir:pretrained-model
lre17_eval_30s
lre17_train_30s
#LOG Steps04::Check x-vector or I-vector generation error Done
##Step5 start
lre17_train_30s[lre17_train_30s lre17_train_30s lre17_eval_30s]
#LOG Steps05:: Done
#LOG Steps06::Build connection between score process and enrollset Done
#LOG Steps07::False to speed up and true to clear files with error   Done
Get raw trials...
[Note] You should check it by yourself to avoid spks-lable-discrepancy in two set.
test spks are [ ara-acm ara-apc ara-ary ara-arz eng-gbr eng-usg por-brz qsl-pol qsl-rus spa-car spa-eur spa-lac zho-cmn zho-nan ]
register spks are [ ara-acm ara-apc ara-ary ara-arz eng-gbr eng-usg por-brz qsl-pol qsl-rus spa-car spa-eur spa-lac zho-cmn zho-nan ]
Write to pretrained-model/lre17_eval_30s/trials.raw done.
Remove invalid trials...
0 utt's trials have been removed and the invalid utts is in pretrained-model/lre17_eval_30s/trials.invalid.utt.list
Write to pretrained-model/lre17_eval_30s/trials done.
Make lre17_eval_30s trials done.
#LOG Steps08::Get Trials Done
#LOG Steps09:: Done
[ lr ]
ivector-normalize-length scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.26335, standard deviation was 0.352726
ivector-compute-lda --dim=100 --total-covariance-factor=0.1 ark:pretrained-model/lre17_train_30s/xvector_norm.ark ark:data/lre17_train_30s/utt2spk pretrained-model/lre17_train_30s/transform_100.mat 
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:288) Read 492731 utterances, 0 with errors.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:294) Computing within-class covariance.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:299) 2-norm of iVector mean is 15.9953
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:136) Stats have 14 speakers, 492731 utterances. 
WARNING (ivector-compute-lda[5.5.883~1-3eea3]:ComputeNormalizingTransform():ivector-compute-lda.cc:98) Floored 129 eigenvalues of covariance to 1.98841e-05
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:171) Singular values of between-class covariance after projecting with interpolated [total/within] covariance with a weight of 0.1 on the total covariance, are:  [ 2.90588 2.40519 2.15112 1.57986 1.19346 1.04706 1.00489 0.626699 0.492955 0.430551 0.339654 0.262772 0.222067 3.39918e-10 1.50561e-11 1.41766e-11 1.3787e-11 1.29261e-11 1.23991e-11 1.20515e-11 1.17956e-11 1.15073e-11 1.10855e-11 1.08423e-11 1.06726e-11 1.043e-11 1.01446e-11 9.95466e-12 9.83686e-12 9.57723e-12 9.48959e-12 9.26464e-12 9.12586e-12 8.87946e-12 8.80499e-12 8.68645e-12 8.51801e-12 8.23287e-12 8.02002e-12 7.93907e-12 7.75004e-12 7.65597e-12 7.50592e-12 7.38407e-12 7.29823e-12 7.26841e-12 7.11195e-12 7.03757e-12 6.84603e-12 6.69938e-12 6.68551e-12 6.52209e-12 6.43416e-12 6.36334e-12 6.18289e-12 6.10256e-12 6.02968e-12 6.02539e-12 5.94713e-12 5.87727e-12 5.75655e-12 5.65056e-12 5.59136e-12 5.47039e-12 5.42325e-12 5.29381e-12 5.1476e-12 5.13813e-12 4.97784e-12 4.89734e-12 4.82425e-12 4.79652e-12 4.68154e-12 4.63167e-12 4.53169e-12 4.4588e-12 4.41556e-12 4.32868e-12 4.29275e-12 4.19057e-12 4.11548e-12 4.02008e-12 3.97968e-12 3.93841e-12 3.84898e-12 3.72921e-12 3.66285e-12 3.59908e-12 3.56063e-12 3.48e-12 3.37638e-12 3.33863e-12 3.26507e-12 3.18899e-12 3.10062e-12 3.06316e-12 3.00453e-12 2.97976e-12 2.94594e-12 2.83621e-12 2.77813e-12 2.72591e-12 2.67029e-12 2.61699e-12 2.5666e-12 2.49457e-12 2.47714e-12 2.4372e-12 2.37685e-12 2.33161e-12 2.30659e-12 2.24263e-12 2.21768e-12 2.1415e-12 2.1314e-12 1.9936e-12 1.98225e-12 1.93959e-12 1.89675e-12 1.84232e-12 1.8375e-12 1.75251e-12 1.6588e-12 1.61799e-12 1.58681e-12 1.56003e-12 1.52131e-12 1.50476e-12 1.46223e-12 1.43422e-12 1.40598e-12 1.37978e-12 1.32589e-12 1.3049e-12 1.24837e-12 1.20679e-12 1.20302e-12 1.16225e-12 1.15229e-12 1.11321e-12 1.08741e-12 1.07572e-12 1.03118e-12 9.95066e-13 9.6071e-13 9.44611e-13 9.38644e-13 9.01184e-13 8.89295e-13 8.61739e-13 8.18741e-13 7.93326e-13 7.64134e-13 7.58101e-13 7.31556e-13 6.96536e-13 6.84815e-13 6.76163e-13 6.65161e-13 6.44676e-13 6.0626e-13 5.88007e-13 5.65442e-13 5.43494e-13 5.38411e-13 5.12856e-13 4.99757e-13 4.87929e-13 4.61777e-13 4.41923e-13 4.31617e-13 4.22456e-13 4.08345e-13 3.9768e-13 3.77143e-13 3.70577e-13 3.58922e-13 3.45225e-13 3.31515e-13 3.10234e-13 2.98156e-13 2.9378e-13 2.80455e-13 2.74832e-13 2.62717e-13 2.545e-13 2.48787e-13 2.43613e-13 2.34035e-13 2.07882e-13 2.05728e-13 1.95553e-13 1.86808e-13 1.84096e-13 1.77587e-13 1.71563e-13 1.62316e-13 1.54437e-13 1.50053e-13 1.3997e-13 1.35626e-13 1.26337e-13 1.23558e-13 1.1968e-13 1.12383e-13 1.02613e-13 9.82605e-14 9.51325e-14 8.87849e-14 8.67413e-14 8.27222e-14 7.68338e-14 7.34747e-14 7.28316e-14 6.4683e-14 6.32607e-14 5.92533e-14 5.8259e-14 5.41779e-14 5.26459e-14 5.00974e-14 4.78823e-14 4.38248e-14 4.2568e-14 3.98685e-14 3.85568e-14 3.42532e-14 3.32714e-14 3.08674e-14 2.871e-14 2.64161e-14 2.59336e-14 2.47206e-14 2.27356e-14 2.05709e-14 2.03413e-14 1.90146e-14 1.80811e-14 1.6708e-14 1.44957e-14 1.36066e-14 1.29644e-14 1.24864e-14 1.06508e-14 1.00594e-14 9.57377e-15 9.0738e-15 7.92316e-15 7.28889e-15 6.41913e-15 6.12398e-15 5.39913e-15 4.97701e-15 3.98205e-15 3.49914e-15 3.1913e-15 2.56876e-15 2.0396e-15 1.4198e-15 1.02199e-15 6.15529e-16 3.15916e-16 -1.96798e-16 -3.17994e-16 -5.90426e-16 -7.74202e-16 -1.11434e-15 -1.5514e-15 -2.05356e-15 -3.02328e-15 -3.13438e-15 -3.45145e-15 -4.22404e-15 -4.74058e-15 -5.23778e-15 -6.017e-15 -6.94501e-15 -7.65427e-15 -7.84041e-15 -8.74914e-15 -9.13164e-15 -9.87051e-15 -1.17554e-14 -1.19892e-14 -1.34698e-14 -1.51032e-14 -1.55803e-14 -1.759e-14 -1.88156e-14 -1.99412e-14 -2.13197e-14 -2.15938e-14 -2.30491e-14 -2.3581e-14 -2.49873e-14 -2.6116e-14 -2.86055e-14 -3.22324e-14 -3.3155e-14 -3.35879e-14 -3.68105e-14 -3.87799e-14 -4.09096e-14 -4.43543e-14 -4.86832e-14 -5.08205e-14 -5.21878e-14 -5.67198e-14 -5.96175e-14 -6.54364e-14 -6.73705e-14 -7.05449e-14 -7.60507e-14 -7.97494e-14 -8.15429e-14 -8.55435e-14 -8.77699e-14 -9.61526e-14 -9.99249e-14 -1.02573e-13 -1.04567e-13 -1.13184e-13 -1.16864e-13 -1.27924e-13 -1.33393e-13 -1.36094e-13 -1.42171e-13 -1.52787e-13 -1.54833e-13 -1.64929e-13 -1.699e-13 -1.79654e-13 -1.83442e-13 -1.97142e-13 -2.11987e-13 -2.2212e-13 -2.24497e-13 -2.29748e-13 -2.32274e-13 -2.42866e-13 -2.52672e-13 -2.72472e-13 -2.80419e-13 -2.89329e-13 -3.01063e-13 -3.12078e-13 -3.31314e-13 -3.44217e-13 -3.53446e-13 -3.59518e-13 -3.73339e-13 -3.91661e-13 -3.99794e-13 -4.22261e-13 -4.36173e-13 -4.581e-13 -4.6864e-13 -5.03118e-13 -5.14217e-13 -5.20797e-13 -5.56986e-13 -5.64542e-13 -5.82459e-13 -6.05782e-13 -6.25377e-13 -6.30576e-13 -6.55691e-13 -6.80004e-13 -6.83333e-13 -7.21023e-13 -7.25088e-13 -7.50613e-13 -7.70865e-13 -8.13039e-13 -8.25481e-13 -8.44359e-13 -8.4773e-13 -9.02988e-13 -9.20939e-13 -9.4504e-13 -9.96507e-13 -1.01588e-12 -1.03602e-12 -1.07917e-12 -1.09231e-12 -1.11465e-12 -1.1499e-12 -1.16233e-12 -1.19555e-12 -1.23182e-12 -1.26457e-12 -1.31607e-12 -1.33455e-12 -1.34232e-12 -1.35921e-12 -1.44e-12 -1.45164e-12 -1.48693e-12 -1.52027e-12 -1.55585e-12 -1.60597e-12 -1.61634e-12 -1.65232e-12 -1.69198e-12 -1.7438e-12 -1.77951e-12 -1.8239e-12 -1.85721e-12 -1.89554e-12 -1.95747e-12 -2.02753e-12 -2.06826e-12 -2.13031e-12 -2.13196e-12 -2.15219e-12 -2.27799e-12 -2.31803e-12 -2.34879e-12 -2.40175e-12 -2.47899e-12 -2.545e-12 -2.59225e-12 -2.60742e-12 -2.62943e-12 -2.66125e-12 -2.76783e-12 -2.81567e-12 -2.90772e-12 -2.96702e-12 -3.01653e-12 -3.15335e-12 -3.23855e-12 -3.24751e-12 -3.3031e-12 -3.33305e-12 -3.40871e-12 -3.45337e-12 -3.50447e-12 -3.61073e-12 -3.64742e-12 -3.72637e-12 -3.77371e-12 -3.84323e-12 -3.90057e-12 -3.97848e-12 -4.07818e-12 -4.12454e-12 -4.15046e-12 -4.23121e-12 -4.29324e-12 -4.37574e-12 -4.44523e-12 -4.54149e-12 -4.65499e-12 -4.76751e-12 -4.80435e-12 -4.88517e-12 -4.90975e-12 -5.01624e-12 -5.16949e-12 -5.22383e-12 -5.35253e-12 -5.36621e-12 -5.60203e-12 -5.66449e-12 -5.75557e-12 -5.87071e-12 -5.91888e-12 -5.98385e-12 -6.03217e-12 -6.2059e-12 -6.35759e-12 -6.49219e-12 -6.5615e-12 -6.6576e-12 -6.72985e-12 -6.80709e-12 -6.98579e-12 -7.06008e-12 -7.25475e-12 -7.41201e-12 -7.61434e-12 -7.68743e-12 -7.79264e-12 -7.90597e-12 -7.99185e-12 -8.1357e-12 -8.2733e-12 -8.52011e-12 -8.68321e-12 -8.72805e-12 -8.78545e-12 -9.14293e-12 -9.36166e-12 -9.41481e-12 -9.55606e-12 -9.68282e-12 -1.0036e-11 -1.01177e-11 -1.02118e-11 -1.02935e-11 -1.06209e-11 -1.0927e-11 -1.13614e-11 -1.14727e-11 -1.16499e-11 -1.21045e-11 -1.23256e-11 -1.28173e-11 -1.37283e-11 -1.45226e-11 -1.54851e-11 ]

LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:318) Wrote LDA transform to pretrained-model/lre17_train_30s/transform_100.mat
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 492731 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 1.53327 (should be close to zero), length divided by sqrt(dim) was 1.2004 (should probably be close to one)
ivector-mean ark:pretrained-model/lre17_train_30s/xvector_lda100.ark pretrained-model/lre17_train_30s/xvector_lda100.global.vec 
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_train_30s/xvector_lda100.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 492731 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.12033, standard deviation was 0.431066
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_eval_30s/xvector.scp ark:pretrained-model/lre17_eval_30s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 7310 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 1.67826 (should be close to zero), length divided by sqrt(dim) was 0.94537 (should probably be close to one)
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_eval_30s/xvector_lda100.ark ark:pretrained-model/lre17_eval_30s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 7310 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_eval_30s/xvector_lda100_submean.ark ark:pretrained-model/lre17_eval_30s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 7310 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 0.825703, standard deviation was 0.524452
logistic-regression-train --max-steps=20 --mix-up=0 ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark ark:data/lre17_train_30s/utt2label pretrained-model/lre17_train_30s/lr/lr.raw.model 
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance ara-acm-ar-20031217-063700-0-b-40228-40378
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance ara-apc-arb-lev-20040721-231950-a-03109-03260
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance ara-apc-fla-0585-b-22127-22303
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance eng-usg-en-4902-b-50689-50953
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance spa-car-20050929-220820-154-fsp-a-65613-65764
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance spa-lac-20051213-185913-737-fsp-b-35030-35185
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance zho-cmn-TM-003970-b-52704-52901
LOG (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:90) Retrieved 492731 vectors with 7 missing. There were 14 class labels.
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -2.6281
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -1.09791
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.756456
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.666002
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.619344
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.608191
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.59609
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.593833
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.589134
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.585039
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.580517
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.58006
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.577648
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.577063
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.576048
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.575238
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.574579
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.574178
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.574065
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.573982
LOG (logistic-regression-train[5.5.883~1-3eea3]:Train():logistic-regression.cc:58) Finished training parameters without mixture components.
logistic-regression-copy pretrained-model/lre17_train_30s/lr/lr.raw.model pretrained-model/lre17_train_30s/lr/lr.scale.model 
LOG (logistic-regression-copy[5.5.883~1-3eea3]:main():logistic-regression-copy.cc:71) Wrote model to pretrained-model/lre17_train_30s/lr/lr.scale.model
logistic-regression-eval --apply-log=true --max-steps=20 --mix-up=0 pretrained-model/lre17_train_30s/lr/lr.scale.model ark:pretrained-model/lre17_train_30s/lr/trials.label ark:pretrained-model/lre17_eval_30s/xvector_lda100_submean_norm.ark - 
LOG (logistic-regression-eval[5.5.883~1-3eea3]:ComputeScores():logistic-regression-eval.cc:111) Calculated scores for 102340 vectors with 0 missing. 
compute-eer - 
LOG (compute-eer[5.5.671~1-e5a5a]:main():compute-eer.cc:136) Equal error rate is 10%, at threshold -3.38573
EER% 10

[ lre17_eval_30s ]
10	pretrained-model/lre17_eval_30s/score/lr_lre17_train_30s_lre17_eval_30s_lda100_submean_norm.eer


############################
lre17_train_30s
lre17_train_30s
lre17_eval_30s
true
############################
#LOG::Steps 01 Check and generate config Done
[Auto find] Your vectortype is xvector

#LOG Steps02::Determine Your vectortype is xvector Done
[Notice] It will set the default config lre17_train_30s[lre17_train_30s lre17_train_30s lre17_eval_30s] for lda, submean and whiten, if used.
#LOG Steps03::Configuration information Done
steps4 start
Vectordir:pretrained-model
lre17_eval_30s
lre17_train_30s
#LOG Steps04::Check x-vector or I-vector generation error Done
##Step5 start
lre17_train_30s[lre17_train_30s lre17_train_30s lre17_eval_30s]
#LOG Steps05:: Done
#LOG Steps06::Build connection between score process and enrollset Done
#LOG Steps07::False to speed up and true to clear files with error   Done
#LOG Steps08::Get Trials Done
#LOG Steps09:: Done
[ lr ]
ivector-normalize-length scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.26335, standard deviation was 0.352726
ivector-compute-lda --dim=100 --total-covariance-factor=0.1 ark:pretrained-model/lre17_train_30s/xvector_norm.ark ark:data/lre17_train_30s/utt2spk pretrained-model/lre17_train_30s/transform_100.mat 
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:288) Read 492731 utterances, 0 with errors.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:294) Computing within-class covariance.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:299) 2-norm of iVector mean is 15.9953
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:136) Stats have 14 speakers, 492731 utterances. 
WARNING (ivector-compute-lda[5.5.883~1-3eea3]:ComputeNormalizingTransform():ivector-compute-lda.cc:98) Floored 129 eigenvalues of covariance to 1.98841e-05
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:171) Singular values of between-class covariance after projecting with interpolated [total/within] covariance with a weight of 0.1 on the total covariance, are:  [ 2.90588 2.40519 2.15112 1.57986 1.19346 1.04706 1.00489 0.626699 0.492955 0.430551 0.339654 0.262772 0.222067 3.39918e-10 1.50561e-11 1.41766e-11 1.3787e-11 1.29261e-11 1.23991e-11 1.20515e-11 1.17956e-11 1.15073e-11 1.10855e-11 1.08423e-11 1.06726e-11 1.043e-11 1.01446e-11 9.95466e-12 9.83686e-12 9.57723e-12 9.48959e-12 9.26464e-12 9.12586e-12 8.87946e-12 8.80499e-12 8.68645e-12 8.51801e-12 8.23287e-12 8.02002e-12 7.93907e-12 7.75004e-12 7.65597e-12 7.50592e-12 7.38407e-12 7.29823e-12 7.26841e-12 7.11195e-12 7.03757e-12 6.84603e-12 6.69938e-12 6.68551e-12 6.52209e-12 6.43416e-12 6.36334e-12 6.18289e-12 6.10256e-12 6.02968e-12 6.02539e-12 5.94713e-12 5.87727e-12 5.75655e-12 5.65056e-12 5.59136e-12 5.47039e-12 5.42325e-12 5.29381e-12 5.1476e-12 5.13813e-12 4.97784e-12 4.89734e-12 4.82425e-12 4.79652e-12 4.68154e-12 4.63167e-12 4.53169e-12 4.4588e-12 4.41556e-12 4.32868e-12 4.29275e-12 4.19057e-12 4.11548e-12 4.02008e-12 3.97968e-12 3.93841e-12 3.84898e-12 3.72921e-12 3.66285e-12 3.59908e-12 3.56063e-12 3.48e-12 3.37638e-12 3.33863e-12 3.26507e-12 3.18899e-12 3.10062e-12 3.06316e-12 3.00453e-12 2.97976e-12 2.94594e-12 2.83621e-12 2.77813e-12 2.72591e-12 2.67029e-12 2.61699e-12 2.5666e-12 2.49457e-12 2.47714e-12 2.4372e-12 2.37685e-12 2.33161e-12 2.30659e-12 2.24263e-12 2.21768e-12 2.1415e-12 2.1314e-12 1.9936e-12 1.98225e-12 1.93959e-12 1.89675e-12 1.84232e-12 1.8375e-12 1.75251e-12 1.6588e-12 1.61799e-12 1.58681e-12 1.56003e-12 1.52131e-12 1.50476e-12 1.46223e-12 1.43422e-12 1.40598e-12 1.37978e-12 1.32589e-12 1.3049e-12 1.24837e-12 1.20679e-12 1.20302e-12 1.16225e-12 1.15229e-12 1.11321e-12 1.08741e-12 1.07572e-12 1.03118e-12 9.95066e-13 9.6071e-13 9.44611e-13 9.38644e-13 9.01184e-13 8.89295e-13 8.61739e-13 8.18741e-13 7.93326e-13 7.64134e-13 7.58101e-13 7.31556e-13 6.96536e-13 6.84815e-13 6.76163e-13 6.65161e-13 6.44676e-13 6.0626e-13 5.88007e-13 5.65442e-13 5.43494e-13 5.38411e-13 5.12856e-13 4.99757e-13 4.87929e-13 4.61777e-13 4.41923e-13 4.31617e-13 4.22456e-13 4.08345e-13 3.9768e-13 3.77143e-13 3.70577e-13 3.58922e-13 3.45225e-13 3.31515e-13 3.10234e-13 2.98156e-13 2.9378e-13 2.80455e-13 2.74832e-13 2.62717e-13 2.545e-13 2.48787e-13 2.43613e-13 2.34035e-13 2.07882e-13 2.05728e-13 1.95553e-13 1.86808e-13 1.84096e-13 1.77587e-13 1.71563e-13 1.62316e-13 1.54437e-13 1.50053e-13 1.3997e-13 1.35626e-13 1.26337e-13 1.23558e-13 1.1968e-13 1.12383e-13 1.02613e-13 9.82605e-14 9.51325e-14 8.87849e-14 8.67413e-14 8.27222e-14 7.68338e-14 7.34747e-14 7.28316e-14 6.4683e-14 6.32607e-14 5.92533e-14 5.8259e-14 5.41779e-14 5.26459e-14 5.00974e-14 4.78823e-14 4.38248e-14 4.2568e-14 3.98685e-14 3.85568e-14 3.42532e-14 3.32714e-14 3.08674e-14 2.871e-14 2.64161e-14 2.59336e-14 2.47206e-14 2.27356e-14 2.05709e-14 2.03413e-14 1.90146e-14 1.80811e-14 1.6708e-14 1.44957e-14 1.36066e-14 1.29644e-14 1.24864e-14 1.06508e-14 1.00594e-14 9.57377e-15 9.0738e-15 7.92316e-15 7.28889e-15 6.41913e-15 6.12398e-15 5.39913e-15 4.97701e-15 3.98205e-15 3.49914e-15 3.1913e-15 2.56876e-15 2.0396e-15 1.4198e-15 1.02199e-15 6.15529e-16 3.15916e-16 -1.96798e-16 -3.17994e-16 -5.90426e-16 -7.74202e-16 -1.11434e-15 -1.5514e-15 -2.05356e-15 -3.02328e-15 -3.13438e-15 -3.45145e-15 -4.22404e-15 -4.74058e-15 -5.23778e-15 -6.017e-15 -6.94501e-15 -7.65427e-15 -7.84041e-15 -8.74914e-15 -9.13164e-15 -9.87051e-15 -1.17554e-14 -1.19892e-14 -1.34698e-14 -1.51032e-14 -1.55803e-14 -1.759e-14 -1.88156e-14 -1.99412e-14 -2.13197e-14 -2.15938e-14 -2.30491e-14 -2.3581e-14 -2.49873e-14 -2.6116e-14 -2.86055e-14 -3.22324e-14 -3.3155e-14 -3.35879e-14 -3.68105e-14 -3.87799e-14 -4.09096e-14 -4.43543e-14 -4.86832e-14 -5.08205e-14 -5.21878e-14 -5.67198e-14 -5.96175e-14 -6.54364e-14 -6.73705e-14 -7.05449e-14 -7.60507e-14 -7.97494e-14 -8.15429e-14 -8.55435e-14 -8.77699e-14 -9.61526e-14 -9.99249e-14 -1.02573e-13 -1.04567e-13 -1.13184e-13 -1.16864e-13 -1.27924e-13 -1.33393e-13 -1.36094e-13 -1.42171e-13 -1.52787e-13 -1.54833e-13 -1.64929e-13 -1.699e-13 -1.79654e-13 -1.83442e-13 -1.97142e-13 -2.11987e-13 -2.2212e-13 -2.24497e-13 -2.29748e-13 -2.32274e-13 -2.42866e-13 -2.52672e-13 -2.72472e-13 -2.80419e-13 -2.89329e-13 -3.01063e-13 -3.12078e-13 -3.31314e-13 -3.44217e-13 -3.53446e-13 -3.59518e-13 -3.73339e-13 -3.91661e-13 -3.99794e-13 -4.22261e-13 -4.36173e-13 -4.581e-13 -4.6864e-13 -5.03118e-13 -5.14217e-13 -5.20797e-13 -5.56986e-13 -5.64542e-13 -5.82459e-13 -6.05782e-13 -6.25377e-13 -6.30576e-13 -6.55691e-13 -6.80004e-13 -6.83333e-13 -7.21023e-13 -7.25088e-13 -7.50613e-13 -7.70865e-13 -8.13039e-13 -8.25481e-13 -8.44359e-13 -8.4773e-13 -9.02988e-13 -9.20939e-13 -9.4504e-13 -9.96507e-13 -1.01588e-12 -1.03602e-12 -1.07917e-12 -1.09231e-12 -1.11465e-12 -1.1499e-12 -1.16233e-12 -1.19555e-12 -1.23182e-12 -1.26457e-12 -1.31607e-12 -1.33455e-12 -1.34232e-12 -1.35921e-12 -1.44e-12 -1.45164e-12 -1.48693e-12 -1.52027e-12 -1.55585e-12 -1.60597e-12 -1.61634e-12 -1.65232e-12 -1.69198e-12 -1.7438e-12 -1.77951e-12 -1.8239e-12 -1.85721e-12 -1.89554e-12 -1.95747e-12 -2.02753e-12 -2.06826e-12 -2.13031e-12 -2.13196e-12 -2.15219e-12 -2.27799e-12 -2.31803e-12 -2.34879e-12 -2.40175e-12 -2.47899e-12 -2.545e-12 -2.59225e-12 -2.60742e-12 -2.62943e-12 -2.66125e-12 -2.76783e-12 -2.81567e-12 -2.90772e-12 -2.96702e-12 -3.01653e-12 -3.15335e-12 -3.23855e-12 -3.24751e-12 -3.3031e-12 -3.33305e-12 -3.40871e-12 -3.45337e-12 -3.50447e-12 -3.61073e-12 -3.64742e-12 -3.72637e-12 -3.77371e-12 -3.84323e-12 -3.90057e-12 -3.97848e-12 -4.07818e-12 -4.12454e-12 -4.15046e-12 -4.23121e-12 -4.29324e-12 -4.37574e-12 -4.44523e-12 -4.54149e-12 -4.65499e-12 -4.76751e-12 -4.80435e-12 -4.88517e-12 -4.90975e-12 -5.01624e-12 -5.16949e-12 -5.22383e-12 -5.35253e-12 -5.36621e-12 -5.60203e-12 -5.66449e-12 -5.75557e-12 -5.87071e-12 -5.91888e-12 -5.98385e-12 -6.03217e-12 -6.2059e-12 -6.35759e-12 -6.49219e-12 -6.5615e-12 -6.6576e-12 -6.72985e-12 -6.80709e-12 -6.98579e-12 -7.06008e-12 -7.25475e-12 -7.41201e-12 -7.61434e-12 -7.68743e-12 -7.79264e-12 -7.90597e-12 -7.99185e-12 -8.1357e-12 -8.2733e-12 -8.52011e-12 -8.68321e-12 -8.72805e-12 -8.78545e-12 -9.14293e-12 -9.36166e-12 -9.41481e-12 -9.55606e-12 -9.68282e-12 -1.0036e-11 -1.01177e-11 -1.02118e-11 -1.02935e-11 -1.06209e-11 -1.0927e-11 -1.13614e-11 -1.14727e-11 -1.16499e-11 -1.21045e-11 -1.23256e-11 -1.28173e-11 -1.37283e-11 -1.45226e-11 -1.54851e-11 ]

LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:318) Wrote LDA transform to pretrained-model/lre17_train_30s/transform_100.mat
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 492731 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 1.53327 (should be close to zero), length divided by sqrt(dim) was 1.2004 (should probably be close to one)
ivector-mean ark:pretrained-model/lre17_train_30s/xvector_lda100.ark pretrained-model/lre17_train_30s/xvector_lda100.global.vec 
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_train_30s/xvector_lda100.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 492731 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.12033, standard deviation was 0.431066
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_eval_30s/xvector.scp ark:pretrained-model/lre17_eval_30s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 7310 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 1.67826 (should be close to zero), length divided by sqrt(dim) was 0.94537 (should probably be close to one)
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_eval_30s/xvector_lda100.ark ark:pretrained-model/lre17_eval_30s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 7310 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_eval_30s/xvector_lda100_submean.ark ark:pretrained-model/lre17_eval_30s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 7310 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 0.825703, standard deviation was 0.524452
logistic-regression-train --max-steps=20 --mix-up=0 ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark ark:data/lre17_train_30s/utt2label pretrained-model/lre17_train_30s/lr/lr.raw.model 
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance ara-acm-ar-20031217-063700-0-b-40228-40378
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance ara-apc-arb-lev-20040721-231950-a-03109-03260
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance ara-apc-fla-0585-b-22127-22303
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance eng-usg-en-4902-b-50689-50953
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance spa-car-20050929-220820-154-fsp-a-65613-65764
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance spa-lac-20051213-185913-737-fsp-b-35030-35185
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance zho-cmn-TM-003970-b-52704-52901
LOG (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:90) Retrieved 492731 vectors with 7 missing. There were 14 class labels.
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -2.6281
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -1.09791
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.756456
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.666002
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.619344
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.608191
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.59609
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.593833
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.589134
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.585039
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.580517
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.58006
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.577648
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.577063
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.576048
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.575238
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.574579
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.574178
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.574065
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.573982
LOG (logistic-regression-train[5.5.883~1-3eea3]:Train():logistic-regression.cc:58) Finished training parameters without mixture components.
logistic-regression-copy pretrained-model/lre17_train_30s/lr/lr.raw.model pretrained-model/lre17_train_30s/lr/lr.scale.model 
LOG (logistic-regression-copy[5.5.883~1-3eea3]:main():logistic-regression-copy.cc:71) Wrote model to pretrained-model/lre17_train_30s/lr/lr.scale.model
logistic-regression-eval --apply-log=true --max-steps=20 --mix-up=0 pretrained-model/lre17_train_30s/lr/lr.scale.model ark:pretrained-model/lre17_train_30s/lr/trials.label ark:pretrained-model/lre17_eval_30s/xvector_lda100_submean_norm.ark - 
LOG (logistic-regression-eval[5.5.883~1-3eea3]:ComputeScores():logistic-regression-eval.cc:111) Calculated scores for 102340 vectors with 0 missing. 
Cavg: 0.0938

[ lre17_eval_30s ]
Cavg: 0.0938	pretrained-model/lre17_eval_30s/score/lr_lre17_train_30s_lre17_eval_30s_lda100_submean_norm.Cavg


#LOG:: getScore use lr Done!
############################
lre17_train_30s
lre17_train_30s
lre17_eval_30s
true
############################
#LOG::Steps 01 Check and generate config Done
[Auto find] Your vectortype is xvector

#LOG Steps02::Determine Your vectortype is xvector Done
[Notice] It will set the default config lre17_train_30s[lre17_train_30s lre17_train_30s lre17_eval_30s] for lda, submean and whiten, if used.
#LOG Steps03::Configuration information Done
steps4 start
Vectordir:pretrained-model
lre17_eval_30s
lre17_train_30s
#LOG Steps04::Check x-vector or I-vector generation error Done
##Step5 start
lre17_train_30s[lre17_train_30s lre17_train_30s lre17_eval_30s]
#LOG Steps05:: Done
#LOG Steps06::Build connection between score process and enrollset Done
#LOG Steps07::False to speed up and true to clear files with error   Done
#LOG Steps08::Get Trials Done
#LOG Steps09:: Done
[ plda ]
ivector-mean ark:data/lre17_train_30s/spk2utt scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/spk_xvector_mean.ark ark,t:pretrained-model/lre17_train_30s/num_utts.ark 
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance ara-acm-ar-20031217-063700-0-b-40228-40378
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance ara-apc-arb-lev-20040721-231950-a-03109-03260
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance ara-apc-fla-0585-b-22127-22303
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance eng-usg-en-4902-b-50689-50953
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance spa-car-20050929-220820-154-fsp-a-65613-65764
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance spa-lac-20051213-185913-737-fsp-b-35030-35185
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance zho-cmn-TM-003970-b-52704-52901
LOG (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:134) Computed mean of 14 speakers (0 with no utterances), consisting of 492731 utterances (7 absent from input).
LOG (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:145) Norm of mean of speakers is 17.7422, root-mean-square speaker-iVector length divided by sqrt(dim) is 1.29274
ivector-normalize-length scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.26335, standard deviation was 0.352726
ivector-compute-lda --dim=100 --total-covariance-factor=0.1 ark:pretrained-model/lre17_train_30s/xvector_norm.ark ark:data/lre17_train_30s/utt2spk pretrained-model/lre17_train_30s/transform_100.mat 
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:288) Read 492731 utterances, 0 with errors.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:294) Computing within-class covariance.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:299) 2-norm of iVector mean is 15.9953
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:136) Stats have 14 speakers, 492731 utterances. 
WARNING (ivector-compute-lda[5.5.883~1-3eea3]:ComputeNormalizingTransform():ivector-compute-lda.cc:98) Floored 129 eigenvalues of covariance to 1.98841e-05
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:171) Singular values of between-class covariance after projecting with interpolated [total/within] covariance with a weight of 0.1 on the total covariance, are:  [ 2.90588 2.40519 2.15112 1.57986 1.19346 1.04706 1.00489 0.626699 0.492955 0.430551 0.339654 0.262772 0.222067 3.39918e-10 1.50561e-11 1.41766e-11 1.3787e-11 1.29261e-11 1.23991e-11 1.20515e-11 1.17956e-11 1.15073e-11 1.10855e-11 1.08423e-11 1.06726e-11 1.043e-11 1.01446e-11 9.95466e-12 9.83686e-12 9.57723e-12 9.48959e-12 9.26464e-12 9.12586e-12 8.87946e-12 8.80499e-12 8.68645e-12 8.51801e-12 8.23287e-12 8.02002e-12 7.93907e-12 7.75004e-12 7.65597e-12 7.50592e-12 7.38407e-12 7.29823e-12 7.26841e-12 7.11195e-12 7.03757e-12 6.84603e-12 6.69938e-12 6.68551e-12 6.52209e-12 6.43416e-12 6.36334e-12 6.18289e-12 6.10256e-12 6.02968e-12 6.02539e-12 5.94713e-12 5.87727e-12 5.75655e-12 5.65056e-12 5.59136e-12 5.47039e-12 5.42325e-12 5.29381e-12 5.1476e-12 5.13813e-12 4.97784e-12 4.89734e-12 4.82425e-12 4.79652e-12 4.68154e-12 4.63167e-12 4.53169e-12 4.4588e-12 4.41556e-12 4.32868e-12 4.29275e-12 4.19057e-12 4.11548e-12 4.02008e-12 3.97968e-12 3.93841e-12 3.84898e-12 3.72921e-12 3.66285e-12 3.59908e-12 3.56063e-12 3.48e-12 3.37638e-12 3.33863e-12 3.26507e-12 3.18899e-12 3.10062e-12 3.06316e-12 3.00453e-12 2.97976e-12 2.94594e-12 2.83621e-12 2.77813e-12 2.72591e-12 2.67029e-12 2.61699e-12 2.5666e-12 2.49457e-12 2.47714e-12 2.4372e-12 2.37685e-12 2.33161e-12 2.30659e-12 2.24263e-12 2.21768e-12 2.1415e-12 2.1314e-12 1.9936e-12 1.98225e-12 1.93959e-12 1.89675e-12 1.84232e-12 1.8375e-12 1.75251e-12 1.6588e-12 1.61799e-12 1.58681e-12 1.56003e-12 1.52131e-12 1.50476e-12 1.46223e-12 1.43422e-12 1.40598e-12 1.37978e-12 1.32589e-12 1.3049e-12 1.24837e-12 1.20679e-12 1.20302e-12 1.16225e-12 1.15229e-12 1.11321e-12 1.08741e-12 1.07572e-12 1.03118e-12 9.95066e-13 9.6071e-13 9.44611e-13 9.38644e-13 9.01184e-13 8.89295e-13 8.61739e-13 8.18741e-13 7.93326e-13 7.64134e-13 7.58101e-13 7.31556e-13 6.96536e-13 6.84815e-13 6.76163e-13 6.65161e-13 6.44676e-13 6.0626e-13 5.88007e-13 5.65442e-13 5.43494e-13 5.38411e-13 5.12856e-13 4.99757e-13 4.87929e-13 4.61777e-13 4.41923e-13 4.31617e-13 4.22456e-13 4.08345e-13 3.9768e-13 3.77143e-13 3.70577e-13 3.58922e-13 3.45225e-13 3.31515e-13 3.10234e-13 2.98156e-13 2.9378e-13 2.80455e-13 2.74832e-13 2.62717e-13 2.545e-13 2.48787e-13 2.43613e-13 2.34035e-13 2.07882e-13 2.05728e-13 1.95553e-13 1.86808e-13 1.84096e-13 1.77587e-13 1.71563e-13 1.62316e-13 1.54437e-13 1.50053e-13 1.3997e-13 1.35626e-13 1.26337e-13 1.23558e-13 1.1968e-13 1.12383e-13 1.02613e-13 9.82605e-14 9.51325e-14 8.87849e-14 8.67413e-14 8.27222e-14 7.68338e-14 7.34747e-14 7.28316e-14 6.4683e-14 6.32607e-14 5.92533e-14 5.8259e-14 5.41779e-14 5.26459e-14 5.00974e-14 4.78823e-14 4.38248e-14 4.2568e-14 3.98685e-14 3.85568e-14 3.42532e-14 3.32714e-14 3.08674e-14 2.871e-14 2.64161e-14 2.59336e-14 2.47206e-14 2.27356e-14 2.05709e-14 2.03413e-14 1.90146e-14 1.80811e-14 1.6708e-14 1.44957e-14 1.36066e-14 1.29644e-14 1.24864e-14 1.06508e-14 1.00594e-14 9.57377e-15 9.0738e-15 7.92316e-15 7.28889e-15 6.41913e-15 6.12398e-15 5.39913e-15 4.97701e-15 3.98205e-15 3.49914e-15 3.1913e-15 2.56876e-15 2.0396e-15 1.4198e-15 1.02199e-15 6.15529e-16 3.15916e-16 -1.96798e-16 -3.17994e-16 -5.90426e-16 -7.74202e-16 -1.11434e-15 -1.5514e-15 -2.05356e-15 -3.02328e-15 -3.13438e-15 -3.45145e-15 -4.22404e-15 -4.74058e-15 -5.23778e-15 -6.017e-15 -6.94501e-15 -7.65427e-15 -7.84041e-15 -8.74914e-15 -9.13164e-15 -9.87051e-15 -1.17554e-14 -1.19892e-14 -1.34698e-14 -1.51032e-14 -1.55803e-14 -1.759e-14 -1.88156e-14 -1.99412e-14 -2.13197e-14 -2.15938e-14 -2.30491e-14 -2.3581e-14 -2.49873e-14 -2.6116e-14 -2.86055e-14 -3.22324e-14 -3.3155e-14 -3.35879e-14 -3.68105e-14 -3.87799e-14 -4.09096e-14 -4.43543e-14 -4.86832e-14 -5.08205e-14 -5.21878e-14 -5.67198e-14 -5.96175e-14 -6.54364e-14 -6.73705e-14 -7.05449e-14 -7.60507e-14 -7.97494e-14 -8.15429e-14 -8.55435e-14 -8.77699e-14 -9.61526e-14 -9.99249e-14 -1.02573e-13 -1.04567e-13 -1.13184e-13 -1.16864e-13 -1.27924e-13 -1.33393e-13 -1.36094e-13 -1.42171e-13 -1.52787e-13 -1.54833e-13 -1.64929e-13 -1.699e-13 -1.79654e-13 -1.83442e-13 -1.97142e-13 -2.11987e-13 -2.2212e-13 -2.24497e-13 -2.29748e-13 -2.32274e-13 -2.42866e-13 -2.52672e-13 -2.72472e-13 -2.80419e-13 -2.89329e-13 -3.01063e-13 -3.12078e-13 -3.31314e-13 -3.44217e-13 -3.53446e-13 -3.59518e-13 -3.73339e-13 -3.91661e-13 -3.99794e-13 -4.22261e-13 -4.36173e-13 -4.581e-13 -4.6864e-13 -5.03118e-13 -5.14217e-13 -5.20797e-13 -5.56986e-13 -5.64542e-13 -5.82459e-13 -6.05782e-13 -6.25377e-13 -6.30576e-13 -6.55691e-13 -6.80004e-13 -6.83333e-13 -7.21023e-13 -7.25088e-13 -7.50613e-13 -7.70865e-13 -8.13039e-13 -8.25481e-13 -8.44359e-13 -8.4773e-13 -9.02988e-13 -9.20939e-13 -9.4504e-13 -9.96507e-13 -1.01588e-12 -1.03602e-12 -1.07917e-12 -1.09231e-12 -1.11465e-12 -1.1499e-12 -1.16233e-12 -1.19555e-12 -1.23182e-12 -1.26457e-12 -1.31607e-12 -1.33455e-12 -1.34232e-12 -1.35921e-12 -1.44e-12 -1.45164e-12 -1.48693e-12 -1.52027e-12 -1.55585e-12 -1.60597e-12 -1.61634e-12 -1.65232e-12 -1.69198e-12 -1.7438e-12 -1.77951e-12 -1.8239e-12 -1.85721e-12 -1.89554e-12 -1.95747e-12 -2.02753e-12 -2.06826e-12 -2.13031e-12 -2.13196e-12 -2.15219e-12 -2.27799e-12 -2.31803e-12 -2.34879e-12 -2.40175e-12 -2.47899e-12 -2.545e-12 -2.59225e-12 -2.60742e-12 -2.62943e-12 -2.66125e-12 -2.76783e-12 -2.81567e-12 -2.90772e-12 -2.96702e-12 -3.01653e-12 -3.15335e-12 -3.23855e-12 -3.24751e-12 -3.3031e-12 -3.33305e-12 -3.40871e-12 -3.45337e-12 -3.50447e-12 -3.61073e-12 -3.64742e-12 -3.72637e-12 -3.77371e-12 -3.84323e-12 -3.90057e-12 -3.97848e-12 -4.07818e-12 -4.12454e-12 -4.15046e-12 -4.23121e-12 -4.29324e-12 -4.37574e-12 -4.44523e-12 -4.54149e-12 -4.65499e-12 -4.76751e-12 -4.80435e-12 -4.88517e-12 -4.90975e-12 -5.01624e-12 -5.16949e-12 -5.22383e-12 -5.35253e-12 -5.36621e-12 -5.60203e-12 -5.66449e-12 -5.75557e-12 -5.87071e-12 -5.91888e-12 -5.98385e-12 -6.03217e-12 -6.2059e-12 -6.35759e-12 -6.49219e-12 -6.5615e-12 -6.6576e-12 -6.72985e-12 -6.80709e-12 -6.98579e-12 -7.06008e-12 -7.25475e-12 -7.41201e-12 -7.61434e-12 -7.68743e-12 -7.79264e-12 -7.90597e-12 -7.99185e-12 -8.1357e-12 -8.2733e-12 -8.52011e-12 -8.68321e-12 -8.72805e-12 -8.78545e-12 -9.14293e-12 -9.36166e-12 -9.41481e-12 -9.55606e-12 -9.68282e-12 -1.0036e-11 -1.01177e-11 -1.02118e-11 -1.02935e-11 -1.06209e-11 -1.0927e-11 -1.13614e-11 -1.14727e-11 -1.16499e-11 -1.21045e-11 -1.23256e-11 -1.28173e-11 -1.37283e-11 -1.45226e-11 -1.54851e-11 ]

LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:318) Wrote LDA transform to pretrained-model/lre17_train_30s/transform_100.mat
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat ark:pretrained-model/lre17_train_30s/spk_xvector_mean.ark ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 14 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 2.75929 (should be close to zero), length divided by sqrt(dim) was 1.40767 (should probably be close to one)
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 492731 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 1.53327 (should be close to zero), length divided by sqrt(dim) was 1.2004 (should probably be close to one)
ivector-mean ark:pretrained-model/lre17_train_30s/xvector_lda100.ark pretrained-model/lre17_train_30s/xvector_lda100.global.vec 
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100.ark ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 14 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean.ark ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 14 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.00249, standard deviation was 1.05391
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_eval_30s/xvector.scp ark:pretrained-model/lre17_eval_30s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 7310 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 1.67826 (should be close to zero), length divided by sqrt(dim) was 0.94537 (should probably be close to one)
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_eval_30s/xvector_lda100.ark ark:pretrained-model/lre17_eval_30s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 7310 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_eval_30s/xvector_lda100_submean.ark ark:pretrained-model/lre17_eval_30s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 7310 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 0.825703, standard deviation was 0.524452
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_train_30s/xvector_lda100.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 492731 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.12033, standard deviation was 0.431066
ivector-compute-plda ark:data/lre17_train_30s/spk2utt ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark pretrained-model/lre17_train_30s/plda 
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance ara-acm-ar-20031217-063700-0-b-40228-40378
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance ara-apc-arb-lev-20040721-231950-a-03109-03260
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance ara-apc-fla-0585-b-22127-22303
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance eng-usg-en-4902-b-50689-50953
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance spa-car-20050929-220820-154-fsp-a-65613-65764
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance spa-lac-20051213-185913-737-fsp-b-35030-35185
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance zho-cmn-TM-003970-b-52704-52901
LOG (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:109) Accumulated stats from 14 speakers (0 with no utterances), consisting of 492731 utterances (7 absent from input).
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 0 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0484
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9578
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 1 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0474
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9649
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 2 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.047
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.962
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 3 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0468
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9611
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 4 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0466
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9607
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 5 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0465
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9604
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 6 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0464
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9603
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 7 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0463
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9602
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 8 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0463
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9601
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 9 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0462
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9601
LOG (ivector-compute-plda[5.5.883~1-3eea3]:GetOutput():plda.cc:540) Norm of mean of iVector distribution is 1.36617
LOG (ivector-compute-plda[5.5.883~1-3eea3]:GetOutput():plda.cc:577) Diagonal of between-class variance in normalized space is  [ 6.80719 5.07642 4.21652 3.83125 3.16111 2.9845 1.87321 1.66839 1.41967 1.1445 1.04717 0.410991 0.353115 4.81664e-06 4.80518e-06 4.79952e-06 4.78659e-06 4.78533e-06 4.76916e-06 4.76006e-06 4.75583e-06 4.74886e-06 4.74146e-06 4.73521e-06 4.7278e-06 4.72638e-06 4.72055e-06 4.71931e-06 4.71476e-06 4.71429e-06 4.70611e-06 4.70559e-06 4.70065e-06 4.69463e-06 4.68495e-06 4.68266e-06 4.67751e-06 4.67639e-06 4.67314e-06 4.66825e-06 4.66532e-06 4.6579e-06 4.65547e-06 4.65337e-06 4.64883e-06 4.64267e-06 4.63883e-06 4.63503e-06 4.63125e-06 4.62836e-06 4.62584e-06 4.62273e-06 4.61574e-06 4.61444e-06 4.61382e-06 4.61116e-06 4.60593e-06 4.6043e-06 4.60154e-06 4.59664e-06 4.5945e-06 4.59077e-06 4.5904e-06 4.58557e-06 4.58489e-06 4.58178e-06 4.57969e-06 4.57774e-06 4.5726e-06 4.56954e-06 4.56696e-06 4.56051e-06 4.55849e-06 4.55698e-06 4.55087e-06 4.54951e-06 4.54885e-06 4.54784e-06 4.54052e-06 4.53963e-06 4.53844e-06 4.53773e-06 4.53246e-06 4.52768e-06 4.52645e-06 4.52513e-06 4.52103e-06 4.51922e-06 4.51895e-06 4.51649e-06 4.51228e-06 4.51026e-06 4.50859e-06 4.5083e-06 4.5071e-06 4.5048e-06 4.50242e-06 4.49714e-06 4.48988e-06 4.43171e-06 ]

ivector-plda-scoring --normalize-length=true --num-utts=ark:pretrained-model/lre17_train_30s/num_utts.ark 'ivector-copy-plda --smoothing=0.0 pretrained-model/lre17_train_30s/plda - |' ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean_norm.ark ark:pretrained-model/lre17_eval_30s/xvector_lda100_submean_norm.ark - pretrained-model/lre17_eval_30s/score/plda_lre17_train_30s_lre17_eval_30s_lda100_submean_norm.score 
ivector-copy-plda --smoothing=0.0 pretrained-model/lre17_train_30s/plda - 
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:96) Reading train iVectors
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:122) Read 14 training iVectors, errors on 0
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:126) Average renormalization scale on training iVectors was 0.375309
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:129) Reading test iVectors
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:147) Read 7310 test iVectors.
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:150) Average renormalization scale on test iVectors was 1.31189
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:214) Mean score was -48.0616, standard deviation was 51.1559
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:217) Processed 102340 trials, 0 had errors.
compute-eer - 
LOG (compute-eer[5.5.671~1-e5a5a]:main():compute-eer.cc:136) Equal error rate is 22.8454%, at threshold -14.449
EER% 22.85

[ lre17_eval_30s ]
22.85	pretrained-model/lre17_eval_30s/score/plda_lre17_train_30s_lre17_eval_30s_lda100_submean_norm.eer


############################
lre17_train_30s
lre17_train_30s
lre17_eval_30s
true
############################
#LOG::Steps 01 Check and generate config Done
[Auto find] Your vectortype is xvector

#LOG Steps02::Determine Your vectortype is xvector Done
[Notice] It will set the default config lre17_train_30s[lre17_train_30s lre17_train_30s lre17_eval_30s] for lda, submean and whiten, if used.
#LOG Steps03::Configuration information Done
steps4 start
Vectordir:pretrained-model
lre17_eval_30s
lre17_train_30s
#LOG Steps04::Check x-vector or I-vector generation error Done
##Step5 start
lre17_train_30s[lre17_train_30s lre17_train_30s lre17_eval_30s]
#LOG Steps05:: Done
#LOG Steps06::Build connection between score process and enrollset Done
#LOG Steps07::False to speed up and true to clear files with error   Done
#LOG Steps08::Get Trials Done
#LOG Steps09:: Done
[ plda ]
ivector-mean ark:data/lre17_train_30s/spk2utt scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/spk_xvector_mean.ark ark,t:pretrained-model/lre17_train_30s/num_utts.ark 
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance ara-acm-ar-20031217-063700-0-b-40228-40378
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance ara-apc-arb-lev-20040721-231950-a-03109-03260
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance ara-apc-fla-0585-b-22127-22303
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance eng-usg-en-4902-b-50689-50953
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance spa-car-20050929-220820-154-fsp-a-65613-65764
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance spa-lac-20051213-185913-737-fsp-b-35030-35185
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance zho-cmn-TM-003970-b-52704-52901
LOG (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:134) Computed mean of 14 speakers (0 with no utterances), consisting of 492731 utterances (7 absent from input).
LOG (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:145) Norm of mean of speakers is 17.7422, root-mean-square speaker-iVector length divided by sqrt(dim) is 1.29274
ivector-normalize-length scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.26335, standard deviation was 0.352726
ivector-compute-lda --dim=100 --total-covariance-factor=0.1 ark:pretrained-model/lre17_train_30s/xvector_norm.ark ark:data/lre17_train_30s/utt2spk pretrained-model/lre17_train_30s/transform_100.mat 
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:288) Read 492731 utterances, 0 with errors.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:294) Computing within-class covariance.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:299) 2-norm of iVector mean is 15.9953
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:136) Stats have 14 speakers, 492731 utterances. 
WARNING (ivector-compute-lda[5.5.883~1-3eea3]:ComputeNormalizingTransform():ivector-compute-lda.cc:98) Floored 129 eigenvalues of covariance to 1.98841e-05
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:171) Singular values of between-class covariance after projecting with interpolated [total/within] covariance with a weight of 0.1 on the total covariance, are:  [ 2.90588 2.40519 2.15112 1.57986 1.19346 1.04706 1.00489 0.626699 0.492955 0.430551 0.339654 0.262772 0.222067 3.39918e-10 1.50561e-11 1.41766e-11 1.3787e-11 1.29261e-11 1.23991e-11 1.20515e-11 1.17956e-11 1.15073e-11 1.10855e-11 1.08423e-11 1.06726e-11 1.043e-11 1.01446e-11 9.95466e-12 9.83686e-12 9.57723e-12 9.48959e-12 9.26464e-12 9.12586e-12 8.87946e-12 8.80499e-12 8.68645e-12 8.51801e-12 8.23287e-12 8.02002e-12 7.93907e-12 7.75004e-12 7.65597e-12 7.50592e-12 7.38407e-12 7.29823e-12 7.26841e-12 7.11195e-12 7.03757e-12 6.84603e-12 6.69938e-12 6.68551e-12 6.52209e-12 6.43416e-12 6.36334e-12 6.18289e-12 6.10256e-12 6.02968e-12 6.02539e-12 5.94713e-12 5.87727e-12 5.75655e-12 5.65056e-12 5.59136e-12 5.47039e-12 5.42325e-12 5.29381e-12 5.1476e-12 5.13813e-12 4.97784e-12 4.89734e-12 4.82425e-12 4.79652e-12 4.68154e-12 4.63167e-12 4.53169e-12 4.4588e-12 4.41556e-12 4.32868e-12 4.29275e-12 4.19057e-12 4.11548e-12 4.02008e-12 3.97968e-12 3.93841e-12 3.84898e-12 3.72921e-12 3.66285e-12 3.59908e-12 3.56063e-12 3.48e-12 3.37638e-12 3.33863e-12 3.26507e-12 3.18899e-12 3.10062e-12 3.06316e-12 3.00453e-12 2.97976e-12 2.94594e-12 2.83621e-12 2.77813e-12 2.72591e-12 2.67029e-12 2.61699e-12 2.5666e-12 2.49457e-12 2.47714e-12 2.4372e-12 2.37685e-12 2.33161e-12 2.30659e-12 2.24263e-12 2.21768e-12 2.1415e-12 2.1314e-12 1.9936e-12 1.98225e-12 1.93959e-12 1.89675e-12 1.84232e-12 1.8375e-12 1.75251e-12 1.6588e-12 1.61799e-12 1.58681e-12 1.56003e-12 1.52131e-12 1.50476e-12 1.46223e-12 1.43422e-12 1.40598e-12 1.37978e-12 1.32589e-12 1.3049e-12 1.24837e-12 1.20679e-12 1.20302e-12 1.16225e-12 1.15229e-12 1.11321e-12 1.08741e-12 1.07572e-12 1.03118e-12 9.95066e-13 9.6071e-13 9.44611e-13 9.38644e-13 9.01184e-13 8.89295e-13 8.61739e-13 8.18741e-13 7.93326e-13 7.64134e-13 7.58101e-13 7.31556e-13 6.96536e-13 6.84815e-13 6.76163e-13 6.65161e-13 6.44676e-13 6.0626e-13 5.88007e-13 5.65442e-13 5.43494e-13 5.38411e-13 5.12856e-13 4.99757e-13 4.87929e-13 4.61777e-13 4.41923e-13 4.31617e-13 4.22456e-13 4.08345e-13 3.9768e-13 3.77143e-13 3.70577e-13 3.58922e-13 3.45225e-13 3.31515e-13 3.10234e-13 2.98156e-13 2.9378e-13 2.80455e-13 2.74832e-13 2.62717e-13 2.545e-13 2.48787e-13 2.43613e-13 2.34035e-13 2.07882e-13 2.05728e-13 1.95553e-13 1.86808e-13 1.84096e-13 1.77587e-13 1.71563e-13 1.62316e-13 1.54437e-13 1.50053e-13 1.3997e-13 1.35626e-13 1.26337e-13 1.23558e-13 1.1968e-13 1.12383e-13 1.02613e-13 9.82605e-14 9.51325e-14 8.87849e-14 8.67413e-14 8.27222e-14 7.68338e-14 7.34747e-14 7.28316e-14 6.4683e-14 6.32607e-14 5.92533e-14 5.8259e-14 5.41779e-14 5.26459e-14 5.00974e-14 4.78823e-14 4.38248e-14 4.2568e-14 3.98685e-14 3.85568e-14 3.42532e-14 3.32714e-14 3.08674e-14 2.871e-14 2.64161e-14 2.59336e-14 2.47206e-14 2.27356e-14 2.05709e-14 2.03413e-14 1.90146e-14 1.80811e-14 1.6708e-14 1.44957e-14 1.36066e-14 1.29644e-14 1.24864e-14 1.06508e-14 1.00594e-14 9.57377e-15 9.0738e-15 7.92316e-15 7.28889e-15 6.41913e-15 6.12398e-15 5.39913e-15 4.97701e-15 3.98205e-15 3.49914e-15 3.1913e-15 2.56876e-15 2.0396e-15 1.4198e-15 1.02199e-15 6.15529e-16 3.15916e-16 -1.96798e-16 -3.17994e-16 -5.90426e-16 -7.74202e-16 -1.11434e-15 -1.5514e-15 -2.05356e-15 -3.02328e-15 -3.13438e-15 -3.45145e-15 -4.22404e-15 -4.74058e-15 -5.23778e-15 -6.017e-15 -6.94501e-15 -7.65427e-15 -7.84041e-15 -8.74914e-15 -9.13164e-15 -9.87051e-15 -1.17554e-14 -1.19892e-14 -1.34698e-14 -1.51032e-14 -1.55803e-14 -1.759e-14 -1.88156e-14 -1.99412e-14 -2.13197e-14 -2.15938e-14 -2.30491e-14 -2.3581e-14 -2.49873e-14 -2.6116e-14 -2.86055e-14 -3.22324e-14 -3.3155e-14 -3.35879e-14 -3.68105e-14 -3.87799e-14 -4.09096e-14 -4.43543e-14 -4.86832e-14 -5.08205e-14 -5.21878e-14 -5.67198e-14 -5.96175e-14 -6.54364e-14 -6.73705e-14 -7.05449e-14 -7.60507e-14 -7.97494e-14 -8.15429e-14 -8.55435e-14 -8.77699e-14 -9.61526e-14 -9.99249e-14 -1.02573e-13 -1.04567e-13 -1.13184e-13 -1.16864e-13 -1.27924e-13 -1.33393e-13 -1.36094e-13 -1.42171e-13 -1.52787e-13 -1.54833e-13 -1.64929e-13 -1.699e-13 -1.79654e-13 -1.83442e-13 -1.97142e-13 -2.11987e-13 -2.2212e-13 -2.24497e-13 -2.29748e-13 -2.32274e-13 -2.42866e-13 -2.52672e-13 -2.72472e-13 -2.80419e-13 -2.89329e-13 -3.01063e-13 -3.12078e-13 -3.31314e-13 -3.44217e-13 -3.53446e-13 -3.59518e-13 -3.73339e-13 -3.91661e-13 -3.99794e-13 -4.22261e-13 -4.36173e-13 -4.581e-13 -4.6864e-13 -5.03118e-13 -5.14217e-13 -5.20797e-13 -5.56986e-13 -5.64542e-13 -5.82459e-13 -6.05782e-13 -6.25377e-13 -6.30576e-13 -6.55691e-13 -6.80004e-13 -6.83333e-13 -7.21023e-13 -7.25088e-13 -7.50613e-13 -7.70865e-13 -8.13039e-13 -8.25481e-13 -8.44359e-13 -8.4773e-13 -9.02988e-13 -9.20939e-13 -9.4504e-13 -9.96507e-13 -1.01588e-12 -1.03602e-12 -1.07917e-12 -1.09231e-12 -1.11465e-12 -1.1499e-12 -1.16233e-12 -1.19555e-12 -1.23182e-12 -1.26457e-12 -1.31607e-12 -1.33455e-12 -1.34232e-12 -1.35921e-12 -1.44e-12 -1.45164e-12 -1.48693e-12 -1.52027e-12 -1.55585e-12 -1.60597e-12 -1.61634e-12 -1.65232e-12 -1.69198e-12 -1.7438e-12 -1.77951e-12 -1.8239e-12 -1.85721e-12 -1.89554e-12 -1.95747e-12 -2.02753e-12 -2.06826e-12 -2.13031e-12 -2.13196e-12 -2.15219e-12 -2.27799e-12 -2.31803e-12 -2.34879e-12 -2.40175e-12 -2.47899e-12 -2.545e-12 -2.59225e-12 -2.60742e-12 -2.62943e-12 -2.66125e-12 -2.76783e-12 -2.81567e-12 -2.90772e-12 -2.96702e-12 -3.01653e-12 -3.15335e-12 -3.23855e-12 -3.24751e-12 -3.3031e-12 -3.33305e-12 -3.40871e-12 -3.45337e-12 -3.50447e-12 -3.61073e-12 -3.64742e-12 -3.72637e-12 -3.77371e-12 -3.84323e-12 -3.90057e-12 -3.97848e-12 -4.07818e-12 -4.12454e-12 -4.15046e-12 -4.23121e-12 -4.29324e-12 -4.37574e-12 -4.44523e-12 -4.54149e-12 -4.65499e-12 -4.76751e-12 -4.80435e-12 -4.88517e-12 -4.90975e-12 -5.01624e-12 -5.16949e-12 -5.22383e-12 -5.35253e-12 -5.36621e-12 -5.60203e-12 -5.66449e-12 -5.75557e-12 -5.87071e-12 -5.91888e-12 -5.98385e-12 -6.03217e-12 -6.2059e-12 -6.35759e-12 -6.49219e-12 -6.5615e-12 -6.6576e-12 -6.72985e-12 -6.80709e-12 -6.98579e-12 -7.06008e-12 -7.25475e-12 -7.41201e-12 -7.61434e-12 -7.68743e-12 -7.79264e-12 -7.90597e-12 -7.99185e-12 -8.1357e-12 -8.2733e-12 -8.52011e-12 -8.68321e-12 -8.72805e-12 -8.78545e-12 -9.14293e-12 -9.36166e-12 -9.41481e-12 -9.55606e-12 -9.68282e-12 -1.0036e-11 -1.01177e-11 -1.02118e-11 -1.02935e-11 -1.06209e-11 -1.0927e-11 -1.13614e-11 -1.14727e-11 -1.16499e-11 -1.21045e-11 -1.23256e-11 -1.28173e-11 -1.37283e-11 -1.45226e-11 -1.54851e-11 ]

LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:318) Wrote LDA transform to pretrained-model/lre17_train_30s/transform_100.mat
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat ark:pretrained-model/lre17_train_30s/spk_xvector_mean.ark ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 14 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 2.75929 (should be close to zero), length divided by sqrt(dim) was 1.40767 (should probably be close to one)
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 492731 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 1.53327 (should be close to zero), length divided by sqrt(dim) was 1.2004 (should probably be close to one)
ivector-mean ark:pretrained-model/lre17_train_30s/xvector_lda100.ark pretrained-model/lre17_train_30s/xvector_lda100.global.vec 
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100.ark ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 14 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean.ark ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 14 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.00249, standard deviation was 1.05391
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_eval_30s/xvector.scp ark:pretrained-model/lre17_eval_30s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 7310 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 1.67826 (should be close to zero), length divided by sqrt(dim) was 0.94537 (should probably be close to one)
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_eval_30s/xvector_lda100.ark ark:pretrained-model/lre17_eval_30s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 7310 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_eval_30s/xvector_lda100_submean.ark ark:pretrained-model/lre17_eval_30s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 7310 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 0.825703, standard deviation was 0.524452
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_train_30s/xvector_lda100.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 492731 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.12033, standard deviation was 0.431066
ivector-compute-plda ark:data/lre17_train_30s/spk2utt ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark pretrained-model/lre17_train_30s/plda 
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance ara-acm-ar-20031217-063700-0-b-40228-40378
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance ara-apc-arb-lev-20040721-231950-a-03109-03260
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance ara-apc-fla-0585-b-22127-22303
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance eng-usg-en-4902-b-50689-50953
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance spa-car-20050929-220820-154-fsp-a-65613-65764
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance spa-lac-20051213-185913-737-fsp-b-35030-35185
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance zho-cmn-TM-003970-b-52704-52901
LOG (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:109) Accumulated stats from 14 speakers (0 with no utterances), consisting of 492731 utterances (7 absent from input).
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 0 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0484
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9578
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 1 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0474
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9649
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 2 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.047
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.962
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 3 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0468
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9611
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 4 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0466
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9607
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 5 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0465
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9604
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 6 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0464
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9603
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 7 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0463
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9602
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 8 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0463
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9601
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 9 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0462
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9601
LOG (ivector-compute-plda[5.5.883~1-3eea3]:GetOutput():plda.cc:540) Norm of mean of iVector distribution is 1.36617
LOG (ivector-compute-plda[5.5.883~1-3eea3]:GetOutput():plda.cc:577) Diagonal of between-class variance in normalized space is  [ 6.80719 5.07642 4.21652 3.83125 3.16111 2.9845 1.87321 1.66839 1.41967 1.1445 1.04717 0.410991 0.353115 4.81664e-06 4.80518e-06 4.79952e-06 4.78659e-06 4.78533e-06 4.76916e-06 4.76006e-06 4.75583e-06 4.74886e-06 4.74146e-06 4.73521e-06 4.7278e-06 4.72638e-06 4.72055e-06 4.71931e-06 4.71476e-06 4.71429e-06 4.70611e-06 4.70559e-06 4.70065e-06 4.69463e-06 4.68495e-06 4.68266e-06 4.67751e-06 4.67639e-06 4.67314e-06 4.66825e-06 4.66532e-06 4.6579e-06 4.65547e-06 4.65337e-06 4.64883e-06 4.64267e-06 4.63883e-06 4.63503e-06 4.63125e-06 4.62836e-06 4.62584e-06 4.62273e-06 4.61574e-06 4.61444e-06 4.61382e-06 4.61116e-06 4.60593e-06 4.6043e-06 4.60154e-06 4.59664e-06 4.5945e-06 4.59077e-06 4.5904e-06 4.58557e-06 4.58489e-06 4.58178e-06 4.57969e-06 4.57774e-06 4.5726e-06 4.56954e-06 4.56696e-06 4.56051e-06 4.55849e-06 4.55698e-06 4.55087e-06 4.54951e-06 4.54885e-06 4.54784e-06 4.54052e-06 4.53963e-06 4.53844e-06 4.53773e-06 4.53246e-06 4.52768e-06 4.52645e-06 4.52513e-06 4.52103e-06 4.51922e-06 4.51895e-06 4.51649e-06 4.51228e-06 4.51026e-06 4.50859e-06 4.5083e-06 4.5071e-06 4.5048e-06 4.50242e-06 4.49714e-06 4.48988e-06 4.43171e-06 ]

ivector-plda-scoring --normalize-length=true --num-utts=ark:pretrained-model/lre17_train_30s/num_utts.ark 'ivector-copy-plda --smoothing=0.0 pretrained-model/lre17_train_30s/plda - |' ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean_norm.ark ark:pretrained-model/lre17_eval_30s/xvector_lda100_submean_norm.ark - pretrained-model/lre17_eval_30s/score/plda_lre17_train_30s_lre17_eval_30s_lda100_submean_norm.score 
ivector-copy-plda --smoothing=0.0 pretrained-model/lre17_train_30s/plda - 
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:96) Reading train iVectors
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:122) Read 14 training iVectors, errors on 0
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:126) Average renormalization scale on training iVectors was 0.375309
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:129) Reading test iVectors
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:147) Read 7310 test iVectors.
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:150) Average renormalization scale on test iVectors was 1.31189
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:214) Mean score was -48.0616, standard deviation was 51.1559
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:217) Processed 102340 trials, 0 had errors.
Cavg: 0.2238

[ lre17_eval_30s ]
Cavg: 0.2238	pretrained-model/lre17_eval_30s/score/plda_lre17_train_30s_lre17_eval_30s_lda100_submean_norm.Cavg


#LOG:: getScore use plda Done!
############################
lre17_train_30s
lre17_train_30s
lre17_dev_3s
true
############################
#LOG::Steps 01 Check and generate config Done
[Auto find] Your vectortype is xvector

#LOG Steps02::Determine Your vectortype is xvector Done
[Notice] It will set the default config lre17_train_30s[lre17_train_30s lre17_train_30s lre17_dev_3s] for lda, submean and whiten, if used.
#LOG Steps03::Configuration information Done
steps4 start
Vectordir:pretrained-model
lre17_dev_3s
lre17_train_30s
#LOG Steps04::Check x-vector or I-vector generation error Done
##Step5 start
lre17_train_30s[lre17_train_30s lre17_train_30s lre17_dev_3s]
#LOG Steps05:: Done
#LOG Steps06::Build connection between score process and enrollset Done
#LOG Steps07::False to speed up and true to clear files with error   Done
Get raw trials...
[Note] You should check it by yourself to avoid spks-lable-discrepancy in two set.
test spks are [ ara-acm ara-apc ara-ary ara-arz eng-gbr eng-usg por-brz qsl-pol qsl-rus spa-car spa-eur spa-lac zho-cmn zho-nan ]
register spks are [ ara-acm ara-apc ara-ary ara-arz eng-gbr eng-usg por-brz qsl-pol qsl-rus spa-car spa-eur spa-lac zho-cmn zho-nan ]
Write to pretrained-model/lre17_dev_3s/trials.raw done.
Remove invalid trials...
0 utt's trials have been removed and the invalid utts is in pretrained-model/lre17_dev_3s/trials.invalid.utt.list
Write to pretrained-model/lre17_dev_3s/trials done.
Make lre17_dev_3s trials done.
#LOG Steps08::Get Trials Done
#LOG Steps09:: Done
[ lr ]
ivector-normalize-length scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.26335, standard deviation was 0.352726
ivector-compute-lda --dim=100 --total-covariance-factor=0.1 ark:pretrained-model/lre17_train_30s/xvector_norm.ark ark:data/lre17_train_30s/utt2spk pretrained-model/lre17_train_30s/transform_100.mat 
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:288) Read 492731 utterances, 0 with errors.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:294) Computing within-class covariance.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:299) 2-norm of iVector mean is 15.9953
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:136) Stats have 14 speakers, 492731 utterances. 
WARNING (ivector-compute-lda[5.5.883~1-3eea3]:ComputeNormalizingTransform():ivector-compute-lda.cc:98) Floored 129 eigenvalues of covariance to 1.98841e-05
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:171) Singular values of between-class covariance after projecting with interpolated [total/within] covariance with a weight of 0.1 on the total covariance, are:  [ 2.90588 2.40519 2.15112 1.57986 1.19346 1.04706 1.00489 0.626699 0.492955 0.430551 0.339654 0.262772 0.222067 3.39918e-10 1.50561e-11 1.41766e-11 1.3787e-11 1.29261e-11 1.23991e-11 1.20515e-11 1.17956e-11 1.15073e-11 1.10855e-11 1.08423e-11 1.06726e-11 1.043e-11 1.01446e-11 9.95466e-12 9.83686e-12 9.57723e-12 9.48959e-12 9.26464e-12 9.12586e-12 8.87946e-12 8.80499e-12 8.68645e-12 8.51801e-12 8.23287e-12 8.02002e-12 7.93907e-12 7.75004e-12 7.65597e-12 7.50592e-12 7.38407e-12 7.29823e-12 7.26841e-12 7.11195e-12 7.03757e-12 6.84603e-12 6.69938e-12 6.68551e-12 6.52209e-12 6.43416e-12 6.36334e-12 6.18289e-12 6.10256e-12 6.02968e-12 6.02539e-12 5.94713e-12 5.87727e-12 5.75655e-12 5.65056e-12 5.59136e-12 5.47039e-12 5.42325e-12 5.29381e-12 5.1476e-12 5.13813e-12 4.97784e-12 4.89734e-12 4.82425e-12 4.79652e-12 4.68154e-12 4.63167e-12 4.53169e-12 4.4588e-12 4.41556e-12 4.32868e-12 4.29275e-12 4.19057e-12 4.11548e-12 4.02008e-12 3.97968e-12 3.93841e-12 3.84898e-12 3.72921e-12 3.66285e-12 3.59908e-12 3.56063e-12 3.48e-12 3.37638e-12 3.33863e-12 3.26507e-12 3.18899e-12 3.10062e-12 3.06316e-12 3.00453e-12 2.97976e-12 2.94594e-12 2.83621e-12 2.77813e-12 2.72591e-12 2.67029e-12 2.61699e-12 2.5666e-12 2.49457e-12 2.47714e-12 2.4372e-12 2.37685e-12 2.33161e-12 2.30659e-12 2.24263e-12 2.21768e-12 2.1415e-12 2.1314e-12 1.9936e-12 1.98225e-12 1.93959e-12 1.89675e-12 1.84232e-12 1.8375e-12 1.75251e-12 1.6588e-12 1.61799e-12 1.58681e-12 1.56003e-12 1.52131e-12 1.50476e-12 1.46223e-12 1.43422e-12 1.40598e-12 1.37978e-12 1.32589e-12 1.3049e-12 1.24837e-12 1.20679e-12 1.20302e-12 1.16225e-12 1.15229e-12 1.11321e-12 1.08741e-12 1.07572e-12 1.03118e-12 9.95066e-13 9.6071e-13 9.44611e-13 9.38644e-13 9.01184e-13 8.89295e-13 8.61739e-13 8.18741e-13 7.93326e-13 7.64134e-13 7.58101e-13 7.31556e-13 6.96536e-13 6.84815e-13 6.76163e-13 6.65161e-13 6.44676e-13 6.0626e-13 5.88007e-13 5.65442e-13 5.43494e-13 5.38411e-13 5.12856e-13 4.99757e-13 4.87929e-13 4.61777e-13 4.41923e-13 4.31617e-13 4.22456e-13 4.08345e-13 3.9768e-13 3.77143e-13 3.70577e-13 3.58922e-13 3.45225e-13 3.31515e-13 3.10234e-13 2.98156e-13 2.9378e-13 2.80455e-13 2.74832e-13 2.62717e-13 2.545e-13 2.48787e-13 2.43613e-13 2.34035e-13 2.07882e-13 2.05728e-13 1.95553e-13 1.86808e-13 1.84096e-13 1.77587e-13 1.71563e-13 1.62316e-13 1.54437e-13 1.50053e-13 1.3997e-13 1.35626e-13 1.26337e-13 1.23558e-13 1.1968e-13 1.12383e-13 1.02613e-13 9.82605e-14 9.51325e-14 8.87849e-14 8.67413e-14 8.27222e-14 7.68338e-14 7.34747e-14 7.28316e-14 6.4683e-14 6.32607e-14 5.92533e-14 5.8259e-14 5.41779e-14 5.26459e-14 5.00974e-14 4.78823e-14 4.38248e-14 4.2568e-14 3.98685e-14 3.85568e-14 3.42532e-14 3.32714e-14 3.08674e-14 2.871e-14 2.64161e-14 2.59336e-14 2.47206e-14 2.27356e-14 2.05709e-14 2.03413e-14 1.90146e-14 1.80811e-14 1.6708e-14 1.44957e-14 1.36066e-14 1.29644e-14 1.24864e-14 1.06508e-14 1.00594e-14 9.57377e-15 9.0738e-15 7.92316e-15 7.28889e-15 6.41913e-15 6.12398e-15 5.39913e-15 4.97701e-15 3.98205e-15 3.49914e-15 3.1913e-15 2.56876e-15 2.0396e-15 1.4198e-15 1.02199e-15 6.15529e-16 3.15916e-16 -1.96798e-16 -3.17994e-16 -5.90426e-16 -7.74202e-16 -1.11434e-15 -1.5514e-15 -2.05356e-15 -3.02328e-15 -3.13438e-15 -3.45145e-15 -4.22404e-15 -4.74058e-15 -5.23778e-15 -6.017e-15 -6.94501e-15 -7.65427e-15 -7.84041e-15 -8.74914e-15 -9.13164e-15 -9.87051e-15 -1.17554e-14 -1.19892e-14 -1.34698e-14 -1.51032e-14 -1.55803e-14 -1.759e-14 -1.88156e-14 -1.99412e-14 -2.13197e-14 -2.15938e-14 -2.30491e-14 -2.3581e-14 -2.49873e-14 -2.6116e-14 -2.86055e-14 -3.22324e-14 -3.3155e-14 -3.35879e-14 -3.68105e-14 -3.87799e-14 -4.09096e-14 -4.43543e-14 -4.86832e-14 -5.08205e-14 -5.21878e-14 -5.67198e-14 -5.96175e-14 -6.54364e-14 -6.73705e-14 -7.05449e-14 -7.60507e-14 -7.97494e-14 -8.15429e-14 -8.55435e-14 -8.77699e-14 -9.61526e-14 -9.99249e-14 -1.02573e-13 -1.04567e-13 -1.13184e-13 -1.16864e-13 -1.27924e-13 -1.33393e-13 -1.36094e-13 -1.42171e-13 -1.52787e-13 -1.54833e-13 -1.64929e-13 -1.699e-13 -1.79654e-13 -1.83442e-13 -1.97142e-13 -2.11987e-13 -2.2212e-13 -2.24497e-13 -2.29748e-13 -2.32274e-13 -2.42866e-13 -2.52672e-13 -2.72472e-13 -2.80419e-13 -2.89329e-13 -3.01063e-13 -3.12078e-13 -3.31314e-13 -3.44217e-13 -3.53446e-13 -3.59518e-13 -3.73339e-13 -3.91661e-13 -3.99794e-13 -4.22261e-13 -4.36173e-13 -4.581e-13 -4.6864e-13 -5.03118e-13 -5.14217e-13 -5.20797e-13 -5.56986e-13 -5.64542e-13 -5.82459e-13 -6.05782e-13 -6.25377e-13 -6.30576e-13 -6.55691e-13 -6.80004e-13 -6.83333e-13 -7.21023e-13 -7.25088e-13 -7.50613e-13 -7.70865e-13 -8.13039e-13 -8.25481e-13 -8.44359e-13 -8.4773e-13 -9.02988e-13 -9.20939e-13 -9.4504e-13 -9.96507e-13 -1.01588e-12 -1.03602e-12 -1.07917e-12 -1.09231e-12 -1.11465e-12 -1.1499e-12 -1.16233e-12 -1.19555e-12 -1.23182e-12 -1.26457e-12 -1.31607e-12 -1.33455e-12 -1.34232e-12 -1.35921e-12 -1.44e-12 -1.45164e-12 -1.48693e-12 -1.52027e-12 -1.55585e-12 -1.60597e-12 -1.61634e-12 -1.65232e-12 -1.69198e-12 -1.7438e-12 -1.77951e-12 -1.8239e-12 -1.85721e-12 -1.89554e-12 -1.95747e-12 -2.02753e-12 -2.06826e-12 -2.13031e-12 -2.13196e-12 -2.15219e-12 -2.27799e-12 -2.31803e-12 -2.34879e-12 -2.40175e-12 -2.47899e-12 -2.545e-12 -2.59225e-12 -2.60742e-12 -2.62943e-12 -2.66125e-12 -2.76783e-12 -2.81567e-12 -2.90772e-12 -2.96702e-12 -3.01653e-12 -3.15335e-12 -3.23855e-12 -3.24751e-12 -3.3031e-12 -3.33305e-12 -3.40871e-12 -3.45337e-12 -3.50447e-12 -3.61073e-12 -3.64742e-12 -3.72637e-12 -3.77371e-12 -3.84323e-12 -3.90057e-12 -3.97848e-12 -4.07818e-12 -4.12454e-12 -4.15046e-12 -4.23121e-12 -4.29324e-12 -4.37574e-12 -4.44523e-12 -4.54149e-12 -4.65499e-12 -4.76751e-12 -4.80435e-12 -4.88517e-12 -4.90975e-12 -5.01624e-12 -5.16949e-12 -5.22383e-12 -5.35253e-12 -5.36621e-12 -5.60203e-12 -5.66449e-12 -5.75557e-12 -5.87071e-12 -5.91888e-12 -5.98385e-12 -6.03217e-12 -6.2059e-12 -6.35759e-12 -6.49219e-12 -6.5615e-12 -6.6576e-12 -6.72985e-12 -6.80709e-12 -6.98579e-12 -7.06008e-12 -7.25475e-12 -7.41201e-12 -7.61434e-12 -7.68743e-12 -7.79264e-12 -7.90597e-12 -7.99185e-12 -8.1357e-12 -8.2733e-12 -8.52011e-12 -8.68321e-12 -8.72805e-12 -8.78545e-12 -9.14293e-12 -9.36166e-12 -9.41481e-12 -9.55606e-12 -9.68282e-12 -1.0036e-11 -1.01177e-11 -1.02118e-11 -1.02935e-11 -1.06209e-11 -1.0927e-11 -1.13614e-11 -1.14727e-11 -1.16499e-11 -1.21045e-11 -1.23256e-11 -1.28173e-11 -1.37283e-11 -1.45226e-11 -1.54851e-11 ]

LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:318) Wrote LDA transform to pretrained-model/lre17_train_30s/transform_100.mat
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 492731 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 1.53327 (should be close to zero), length divided by sqrt(dim) was 1.2004 (should probably be close to one)
ivector-mean ark:pretrained-model/lre17_train_30s/xvector_lda100.ark pretrained-model/lre17_train_30s/xvector_lda100.global.vec 
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_train_30s/xvector_lda100.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 492731 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.12033, standard deviation was 0.431066
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_dev_3s/xvector.scp ark:pretrained-model/lre17_dev_3s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 926 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 2.21882 (should be close to zero), length divided by sqrt(dim) was 1.5711 (should probably be close to one)
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_dev_3s/xvector_lda100.ark ark:pretrained-model/lre17_dev_3s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 926 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_dev_3s/xvector_lda100_submean.ark ark:pretrained-model/lre17_dev_3s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 926 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.46195, standard deviation was 0.647369
logistic-regression-train --max-steps=20 --mix-up=0 ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark ark:data/lre17_train_30s/utt2label pretrained-model/lre17_train_30s/lr/lr.raw.model 
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance ara-acm-ar-20031217-063700-0-b-40228-40378
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance ara-apc-arb-lev-20040721-231950-a-03109-03260
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance ara-apc-fla-0585-b-22127-22303
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance eng-usg-en-4902-b-50689-50953
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance spa-car-20050929-220820-154-fsp-a-65613-65764
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance spa-lac-20051213-185913-737-fsp-b-35030-35185
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance zho-cmn-TM-003970-b-52704-52901
LOG (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:90) Retrieved 492731 vectors with 7 missing. There were 14 class labels.
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -2.6281
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -1.09791
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.756456
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.666002
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.619344
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.608191
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.59609
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.593833
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.589134
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.585039
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.580517
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.58006
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.577648
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.577063
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.576048
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.575238
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.574579
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.574178
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.574065
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.573982
LOG (logistic-regression-train[5.5.883~1-3eea3]:Train():logistic-regression.cc:58) Finished training parameters without mixture components.
logistic-regression-copy pretrained-model/lre17_train_30s/lr/lr.raw.model pretrained-model/lre17_train_30s/lr/lr.scale.model 
LOG (logistic-regression-copy[5.5.883~1-3eea3]:main():logistic-regression-copy.cc:71) Wrote model to pretrained-model/lre17_train_30s/lr/lr.scale.model
logistic-regression-eval --apply-log=true --max-steps=20 --mix-up=0 pretrained-model/lre17_train_30s/lr/lr.scale.model ark:pretrained-model/lre17_train_30s/lr/trials.label ark:pretrained-model/lre17_dev_3s/xvector_lda100_submean_norm.ark - 
LOG (logistic-regression-eval[5.5.883~1-3eea3]:ComputeScores():logistic-regression-eval.cc:111) Calculated scores for 12964 vectors with 0 missing. 
compute-eer - 
LOG (compute-eer[5.5.671~1-e5a5a]:main():compute-eer.cc:136) Equal error rate is 17.0626%, at threshold -3.12746
EER% 17.06

[ lre17_dev_3s ]
17.06	pretrained-model/lre17_dev_3s/score/lr_lre17_train_30s_lre17_dev_3s_lda100_submean_norm.eer


############################
lre17_train_30s
lre17_train_30s
lre17_dev_3s
true
############################
#LOG::Steps 01 Check and generate config Done
[Auto find] Your vectortype is xvector

#LOG Steps02::Determine Your vectortype is xvector Done
[Notice] It will set the default config lre17_train_30s[lre17_train_30s lre17_train_30s lre17_dev_3s] for lda, submean and whiten, if used.
#LOG Steps03::Configuration information Done
steps4 start
Vectordir:pretrained-model
lre17_dev_3s
lre17_train_30s
#LOG Steps04::Check x-vector or I-vector generation error Done
##Step5 start
lre17_train_30s[lre17_train_30s lre17_train_30s lre17_dev_3s]
#LOG Steps05:: Done
#LOG Steps06::Build connection between score process and enrollset Done
#LOG Steps07::False to speed up and true to clear files with error   Done
#LOG Steps08::Get Trials Done
#LOG Steps09:: Done
[ lr ]
ivector-normalize-length scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.26335, standard deviation was 0.352726
ivector-compute-lda --dim=100 --total-covariance-factor=0.1 ark:pretrained-model/lre17_train_30s/xvector_norm.ark ark:data/lre17_train_30s/utt2spk pretrained-model/lre17_train_30s/transform_100.mat 
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:288) Read 492731 utterances, 0 with errors.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:294) Computing within-class covariance.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:299) 2-norm of iVector mean is 15.9953
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:136) Stats have 14 speakers, 492731 utterances. 
WARNING (ivector-compute-lda[5.5.883~1-3eea3]:ComputeNormalizingTransform():ivector-compute-lda.cc:98) Floored 129 eigenvalues of covariance to 1.98841e-05
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:171) Singular values of between-class covariance after projecting with interpolated [total/within] covariance with a weight of 0.1 on the total covariance, are:  [ 2.90588 2.40519 2.15112 1.57986 1.19346 1.04706 1.00489 0.626699 0.492955 0.430551 0.339654 0.262772 0.222067 3.39918e-10 1.50561e-11 1.41766e-11 1.3787e-11 1.29261e-11 1.23991e-11 1.20515e-11 1.17956e-11 1.15073e-11 1.10855e-11 1.08423e-11 1.06726e-11 1.043e-11 1.01446e-11 9.95466e-12 9.83686e-12 9.57723e-12 9.48959e-12 9.26464e-12 9.12586e-12 8.87946e-12 8.80499e-12 8.68645e-12 8.51801e-12 8.23287e-12 8.02002e-12 7.93907e-12 7.75004e-12 7.65597e-12 7.50592e-12 7.38407e-12 7.29823e-12 7.26841e-12 7.11195e-12 7.03757e-12 6.84603e-12 6.69938e-12 6.68551e-12 6.52209e-12 6.43416e-12 6.36334e-12 6.18289e-12 6.10256e-12 6.02968e-12 6.02539e-12 5.94713e-12 5.87727e-12 5.75655e-12 5.65056e-12 5.59136e-12 5.47039e-12 5.42325e-12 5.29381e-12 5.1476e-12 5.13813e-12 4.97784e-12 4.89734e-12 4.82425e-12 4.79652e-12 4.68154e-12 4.63167e-12 4.53169e-12 4.4588e-12 4.41556e-12 4.32868e-12 4.29275e-12 4.19057e-12 4.11548e-12 4.02008e-12 3.97968e-12 3.93841e-12 3.84898e-12 3.72921e-12 3.66285e-12 3.59908e-12 3.56063e-12 3.48e-12 3.37638e-12 3.33863e-12 3.26507e-12 3.18899e-12 3.10062e-12 3.06316e-12 3.00453e-12 2.97976e-12 2.94594e-12 2.83621e-12 2.77813e-12 2.72591e-12 2.67029e-12 2.61699e-12 2.5666e-12 2.49457e-12 2.47714e-12 2.4372e-12 2.37685e-12 2.33161e-12 2.30659e-12 2.24263e-12 2.21768e-12 2.1415e-12 2.1314e-12 1.9936e-12 1.98225e-12 1.93959e-12 1.89675e-12 1.84232e-12 1.8375e-12 1.75251e-12 1.6588e-12 1.61799e-12 1.58681e-12 1.56003e-12 1.52131e-12 1.50476e-12 1.46223e-12 1.43422e-12 1.40598e-12 1.37978e-12 1.32589e-12 1.3049e-12 1.24837e-12 1.20679e-12 1.20302e-12 1.16225e-12 1.15229e-12 1.11321e-12 1.08741e-12 1.07572e-12 1.03118e-12 9.95066e-13 9.6071e-13 9.44611e-13 9.38644e-13 9.01184e-13 8.89295e-13 8.61739e-13 8.18741e-13 7.93326e-13 7.64134e-13 7.58101e-13 7.31556e-13 6.96536e-13 6.84815e-13 6.76163e-13 6.65161e-13 6.44676e-13 6.0626e-13 5.88007e-13 5.65442e-13 5.43494e-13 5.38411e-13 5.12856e-13 4.99757e-13 4.87929e-13 4.61777e-13 4.41923e-13 4.31617e-13 4.22456e-13 4.08345e-13 3.9768e-13 3.77143e-13 3.70577e-13 3.58922e-13 3.45225e-13 3.31515e-13 3.10234e-13 2.98156e-13 2.9378e-13 2.80455e-13 2.74832e-13 2.62717e-13 2.545e-13 2.48787e-13 2.43613e-13 2.34035e-13 2.07882e-13 2.05728e-13 1.95553e-13 1.86808e-13 1.84096e-13 1.77587e-13 1.71563e-13 1.62316e-13 1.54437e-13 1.50053e-13 1.3997e-13 1.35626e-13 1.26337e-13 1.23558e-13 1.1968e-13 1.12383e-13 1.02613e-13 9.82605e-14 9.51325e-14 8.87849e-14 8.67413e-14 8.27222e-14 7.68338e-14 7.34747e-14 7.28316e-14 6.4683e-14 6.32607e-14 5.92533e-14 5.8259e-14 5.41779e-14 5.26459e-14 5.00974e-14 4.78823e-14 4.38248e-14 4.2568e-14 3.98685e-14 3.85568e-14 3.42532e-14 3.32714e-14 3.08674e-14 2.871e-14 2.64161e-14 2.59336e-14 2.47206e-14 2.27356e-14 2.05709e-14 2.03413e-14 1.90146e-14 1.80811e-14 1.6708e-14 1.44957e-14 1.36066e-14 1.29644e-14 1.24864e-14 1.06508e-14 1.00594e-14 9.57377e-15 9.0738e-15 7.92316e-15 7.28889e-15 6.41913e-15 6.12398e-15 5.39913e-15 4.97701e-15 3.98205e-15 3.49914e-15 3.1913e-15 2.56876e-15 2.0396e-15 1.4198e-15 1.02199e-15 6.15529e-16 3.15916e-16 -1.96798e-16 -3.17994e-16 -5.90426e-16 -7.74202e-16 -1.11434e-15 -1.5514e-15 -2.05356e-15 -3.02328e-15 -3.13438e-15 -3.45145e-15 -4.22404e-15 -4.74058e-15 -5.23778e-15 -6.017e-15 -6.94501e-15 -7.65427e-15 -7.84041e-15 -8.74914e-15 -9.13164e-15 -9.87051e-15 -1.17554e-14 -1.19892e-14 -1.34698e-14 -1.51032e-14 -1.55803e-14 -1.759e-14 -1.88156e-14 -1.99412e-14 -2.13197e-14 -2.15938e-14 -2.30491e-14 -2.3581e-14 -2.49873e-14 -2.6116e-14 -2.86055e-14 -3.22324e-14 -3.3155e-14 -3.35879e-14 -3.68105e-14 -3.87799e-14 -4.09096e-14 -4.43543e-14 -4.86832e-14 -5.08205e-14 -5.21878e-14 -5.67198e-14 -5.96175e-14 -6.54364e-14 -6.73705e-14 -7.05449e-14 -7.60507e-14 -7.97494e-14 -8.15429e-14 -8.55435e-14 -8.77699e-14 -9.61526e-14 -9.99249e-14 -1.02573e-13 -1.04567e-13 -1.13184e-13 -1.16864e-13 -1.27924e-13 -1.33393e-13 -1.36094e-13 -1.42171e-13 -1.52787e-13 -1.54833e-13 -1.64929e-13 -1.699e-13 -1.79654e-13 -1.83442e-13 -1.97142e-13 -2.11987e-13 -2.2212e-13 -2.24497e-13 -2.29748e-13 -2.32274e-13 -2.42866e-13 -2.52672e-13 -2.72472e-13 -2.80419e-13 -2.89329e-13 -3.01063e-13 -3.12078e-13 -3.31314e-13 -3.44217e-13 -3.53446e-13 -3.59518e-13 -3.73339e-13 -3.91661e-13 -3.99794e-13 -4.22261e-13 -4.36173e-13 -4.581e-13 -4.6864e-13 -5.03118e-13 -5.14217e-13 -5.20797e-13 -5.56986e-13 -5.64542e-13 -5.82459e-13 -6.05782e-13 -6.25377e-13 -6.30576e-13 -6.55691e-13 -6.80004e-13 -6.83333e-13 -7.21023e-13 -7.25088e-13 -7.50613e-13 -7.70865e-13 -8.13039e-13 -8.25481e-13 -8.44359e-13 -8.4773e-13 -9.02988e-13 -9.20939e-13 -9.4504e-13 -9.96507e-13 -1.01588e-12 -1.03602e-12 -1.07917e-12 -1.09231e-12 -1.11465e-12 -1.1499e-12 -1.16233e-12 -1.19555e-12 -1.23182e-12 -1.26457e-12 -1.31607e-12 -1.33455e-12 -1.34232e-12 -1.35921e-12 -1.44e-12 -1.45164e-12 -1.48693e-12 -1.52027e-12 -1.55585e-12 -1.60597e-12 -1.61634e-12 -1.65232e-12 -1.69198e-12 -1.7438e-12 -1.77951e-12 -1.8239e-12 -1.85721e-12 -1.89554e-12 -1.95747e-12 -2.02753e-12 -2.06826e-12 -2.13031e-12 -2.13196e-12 -2.15219e-12 -2.27799e-12 -2.31803e-12 -2.34879e-12 -2.40175e-12 -2.47899e-12 -2.545e-12 -2.59225e-12 -2.60742e-12 -2.62943e-12 -2.66125e-12 -2.76783e-12 -2.81567e-12 -2.90772e-12 -2.96702e-12 -3.01653e-12 -3.15335e-12 -3.23855e-12 -3.24751e-12 -3.3031e-12 -3.33305e-12 -3.40871e-12 -3.45337e-12 -3.50447e-12 -3.61073e-12 -3.64742e-12 -3.72637e-12 -3.77371e-12 -3.84323e-12 -3.90057e-12 -3.97848e-12 -4.07818e-12 -4.12454e-12 -4.15046e-12 -4.23121e-12 -4.29324e-12 -4.37574e-12 -4.44523e-12 -4.54149e-12 -4.65499e-12 -4.76751e-12 -4.80435e-12 -4.88517e-12 -4.90975e-12 -5.01624e-12 -5.16949e-12 -5.22383e-12 -5.35253e-12 -5.36621e-12 -5.60203e-12 -5.66449e-12 -5.75557e-12 -5.87071e-12 -5.91888e-12 -5.98385e-12 -6.03217e-12 -6.2059e-12 -6.35759e-12 -6.49219e-12 -6.5615e-12 -6.6576e-12 -6.72985e-12 -6.80709e-12 -6.98579e-12 -7.06008e-12 -7.25475e-12 -7.41201e-12 -7.61434e-12 -7.68743e-12 -7.79264e-12 -7.90597e-12 -7.99185e-12 -8.1357e-12 -8.2733e-12 -8.52011e-12 -8.68321e-12 -8.72805e-12 -8.78545e-12 -9.14293e-12 -9.36166e-12 -9.41481e-12 -9.55606e-12 -9.68282e-12 -1.0036e-11 -1.01177e-11 -1.02118e-11 -1.02935e-11 -1.06209e-11 -1.0927e-11 -1.13614e-11 -1.14727e-11 -1.16499e-11 -1.21045e-11 -1.23256e-11 -1.28173e-11 -1.37283e-11 -1.45226e-11 -1.54851e-11 ]

LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:318) Wrote LDA transform to pretrained-model/lre17_train_30s/transform_100.mat
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 492731 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 1.53327 (should be close to zero), length divided by sqrt(dim) was 1.2004 (should probably be close to one)
ivector-mean ark:pretrained-model/lre17_train_30s/xvector_lda100.ark pretrained-model/lre17_train_30s/xvector_lda100.global.vec 
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_train_30s/xvector_lda100.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 492731 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.12033, standard deviation was 0.431066
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_dev_3s/xvector.scp ark:pretrained-model/lre17_dev_3s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 926 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 2.21882 (should be close to zero), length divided by sqrt(dim) was 1.5711 (should probably be close to one)
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_dev_3s/xvector_lda100.ark ark:pretrained-model/lre17_dev_3s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 926 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_dev_3s/xvector_lda100_submean.ark ark:pretrained-model/lre17_dev_3s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 926 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.46195, standard deviation was 0.647369
logistic-regression-train --max-steps=20 --mix-up=0 ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark ark:data/lre17_train_30s/utt2label pretrained-model/lre17_train_30s/lr/lr.raw.model 
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance ara-acm-ar-20031217-063700-0-b-40228-40378
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance ara-apc-arb-lev-20040721-231950-a-03109-03260
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance ara-apc-fla-0585-b-22127-22303
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance eng-usg-en-4902-b-50689-50953
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance spa-car-20050929-220820-154-fsp-a-65613-65764
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance spa-lac-20051213-185913-737-fsp-b-35030-35185
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance zho-cmn-TM-003970-b-52704-52901
LOG (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:90) Retrieved 492731 vectors with 7 missing. There were 14 class labels.
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -2.6281
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -1.09791
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.756456
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.666002
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.619344
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.608191
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.59609
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.593833
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.589134
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.585039
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.580517
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.58006
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.577648
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.577063
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.576048
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.575238
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.574579
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.574178
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.574065
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.573982
LOG (logistic-regression-train[5.5.883~1-3eea3]:Train():logistic-regression.cc:58) Finished training parameters without mixture components.
logistic-regression-copy pretrained-model/lre17_train_30s/lr/lr.raw.model pretrained-model/lre17_train_30s/lr/lr.scale.model 
LOG (logistic-regression-copy[5.5.883~1-3eea3]:main():logistic-regression-copy.cc:71) Wrote model to pretrained-model/lre17_train_30s/lr/lr.scale.model
logistic-regression-eval --apply-log=true --max-steps=20 --mix-up=0 pretrained-model/lre17_train_30s/lr/lr.scale.model ark:pretrained-model/lre17_train_30s/lr/trials.label ark:pretrained-model/lre17_dev_3s/xvector_lda100_submean_norm.ark - 
LOG (logistic-regression-eval[5.5.883~1-3eea3]:ComputeScores():logistic-regression-eval.cc:111) Calculated scores for 12964 vectors with 0 missing. 
Cavg: 0.163

[ lre17_dev_3s ]
Cavg: 0.163	pretrained-model/lre17_dev_3s/score/lr_lre17_train_30s_lre17_dev_3s_lda100_submean_norm.Cavg


#LOG:: getScore use lr Done!
############################
lre17_train_30s
lre17_train_30s
lre17_dev_3s
true
############################
#LOG::Steps 01 Check and generate config Done
[Auto find] Your vectortype is xvector

#LOG Steps02::Determine Your vectortype is xvector Done
[Notice] It will set the default config lre17_train_30s[lre17_train_30s lre17_train_30s lre17_dev_3s] for lda, submean and whiten, if used.
#LOG Steps03::Configuration information Done
steps4 start
Vectordir:pretrained-model
lre17_dev_3s
lre17_train_30s
#LOG Steps04::Check x-vector or I-vector generation error Done
##Step5 start
lre17_train_30s[lre17_train_30s lre17_train_30s lre17_dev_3s]
#LOG Steps05:: Done
#LOG Steps06::Build connection between score process and enrollset Done
#LOG Steps07::False to speed up and true to clear files with error   Done
#LOG Steps08::Get Trials Done
#LOG Steps09:: Done
[ plda ]
ivector-mean ark:data/lre17_train_30s/spk2utt scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/spk_xvector_mean.ark ark,t:pretrained-model/lre17_train_30s/num_utts.ark 
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance ara-acm-ar-20031217-063700-0-b-40228-40378
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance ara-apc-arb-lev-20040721-231950-a-03109-03260
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance ara-apc-fla-0585-b-22127-22303
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance eng-usg-en-4902-b-50689-50953
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance spa-car-20050929-220820-154-fsp-a-65613-65764
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance spa-lac-20051213-185913-737-fsp-b-35030-35185
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance zho-cmn-TM-003970-b-52704-52901
LOG (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:134) Computed mean of 14 speakers (0 with no utterances), consisting of 492731 utterances (7 absent from input).
LOG (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:145) Norm of mean of speakers is 17.7422, root-mean-square speaker-iVector length divided by sqrt(dim) is 1.29274
ivector-normalize-length scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.26335, standard deviation was 0.352726
ivector-compute-lda --dim=100 --total-covariance-factor=0.1 ark:pretrained-model/lre17_train_30s/xvector_norm.ark ark:data/lre17_train_30s/utt2spk pretrained-model/lre17_train_30s/transform_100.mat 
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:288) Read 492731 utterances, 0 with errors.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:294) Computing within-class covariance.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:299) 2-norm of iVector mean is 15.9953
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:136) Stats have 14 speakers, 492731 utterances. 
WARNING (ivector-compute-lda[5.5.883~1-3eea3]:ComputeNormalizingTransform():ivector-compute-lda.cc:98) Floored 129 eigenvalues of covariance to 1.98841e-05
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:171) Singular values of between-class covariance after projecting with interpolated [total/within] covariance with a weight of 0.1 on the total covariance, are:  [ 2.90588 2.40519 2.15112 1.57986 1.19346 1.04706 1.00489 0.626699 0.492955 0.430551 0.339654 0.262772 0.222067 3.39918e-10 1.50561e-11 1.41766e-11 1.3787e-11 1.29261e-11 1.23991e-11 1.20515e-11 1.17956e-11 1.15073e-11 1.10855e-11 1.08423e-11 1.06726e-11 1.043e-11 1.01446e-11 9.95466e-12 9.83686e-12 9.57723e-12 9.48959e-12 9.26464e-12 9.12586e-12 8.87946e-12 8.80499e-12 8.68645e-12 8.51801e-12 8.23287e-12 8.02002e-12 7.93907e-12 7.75004e-12 7.65597e-12 7.50592e-12 7.38407e-12 7.29823e-12 7.26841e-12 7.11195e-12 7.03757e-12 6.84603e-12 6.69938e-12 6.68551e-12 6.52209e-12 6.43416e-12 6.36334e-12 6.18289e-12 6.10256e-12 6.02968e-12 6.02539e-12 5.94713e-12 5.87727e-12 5.75655e-12 5.65056e-12 5.59136e-12 5.47039e-12 5.42325e-12 5.29381e-12 5.1476e-12 5.13813e-12 4.97784e-12 4.89734e-12 4.82425e-12 4.79652e-12 4.68154e-12 4.63167e-12 4.53169e-12 4.4588e-12 4.41556e-12 4.32868e-12 4.29275e-12 4.19057e-12 4.11548e-12 4.02008e-12 3.97968e-12 3.93841e-12 3.84898e-12 3.72921e-12 3.66285e-12 3.59908e-12 3.56063e-12 3.48e-12 3.37638e-12 3.33863e-12 3.26507e-12 3.18899e-12 3.10062e-12 3.06316e-12 3.00453e-12 2.97976e-12 2.94594e-12 2.83621e-12 2.77813e-12 2.72591e-12 2.67029e-12 2.61699e-12 2.5666e-12 2.49457e-12 2.47714e-12 2.4372e-12 2.37685e-12 2.33161e-12 2.30659e-12 2.24263e-12 2.21768e-12 2.1415e-12 2.1314e-12 1.9936e-12 1.98225e-12 1.93959e-12 1.89675e-12 1.84232e-12 1.8375e-12 1.75251e-12 1.6588e-12 1.61799e-12 1.58681e-12 1.56003e-12 1.52131e-12 1.50476e-12 1.46223e-12 1.43422e-12 1.40598e-12 1.37978e-12 1.32589e-12 1.3049e-12 1.24837e-12 1.20679e-12 1.20302e-12 1.16225e-12 1.15229e-12 1.11321e-12 1.08741e-12 1.07572e-12 1.03118e-12 9.95066e-13 9.6071e-13 9.44611e-13 9.38644e-13 9.01184e-13 8.89295e-13 8.61739e-13 8.18741e-13 7.93326e-13 7.64134e-13 7.58101e-13 7.31556e-13 6.96536e-13 6.84815e-13 6.76163e-13 6.65161e-13 6.44676e-13 6.0626e-13 5.88007e-13 5.65442e-13 5.43494e-13 5.38411e-13 5.12856e-13 4.99757e-13 4.87929e-13 4.61777e-13 4.41923e-13 4.31617e-13 4.22456e-13 4.08345e-13 3.9768e-13 3.77143e-13 3.70577e-13 3.58922e-13 3.45225e-13 3.31515e-13 3.10234e-13 2.98156e-13 2.9378e-13 2.80455e-13 2.74832e-13 2.62717e-13 2.545e-13 2.48787e-13 2.43613e-13 2.34035e-13 2.07882e-13 2.05728e-13 1.95553e-13 1.86808e-13 1.84096e-13 1.77587e-13 1.71563e-13 1.62316e-13 1.54437e-13 1.50053e-13 1.3997e-13 1.35626e-13 1.26337e-13 1.23558e-13 1.1968e-13 1.12383e-13 1.02613e-13 9.82605e-14 9.51325e-14 8.87849e-14 8.67413e-14 8.27222e-14 7.68338e-14 7.34747e-14 7.28316e-14 6.4683e-14 6.32607e-14 5.92533e-14 5.8259e-14 5.41779e-14 5.26459e-14 5.00974e-14 4.78823e-14 4.38248e-14 4.2568e-14 3.98685e-14 3.85568e-14 3.42532e-14 3.32714e-14 3.08674e-14 2.871e-14 2.64161e-14 2.59336e-14 2.47206e-14 2.27356e-14 2.05709e-14 2.03413e-14 1.90146e-14 1.80811e-14 1.6708e-14 1.44957e-14 1.36066e-14 1.29644e-14 1.24864e-14 1.06508e-14 1.00594e-14 9.57377e-15 9.0738e-15 7.92316e-15 7.28889e-15 6.41913e-15 6.12398e-15 5.39913e-15 4.97701e-15 3.98205e-15 3.49914e-15 3.1913e-15 2.56876e-15 2.0396e-15 1.4198e-15 1.02199e-15 6.15529e-16 3.15916e-16 -1.96798e-16 -3.17994e-16 -5.90426e-16 -7.74202e-16 -1.11434e-15 -1.5514e-15 -2.05356e-15 -3.02328e-15 -3.13438e-15 -3.45145e-15 -4.22404e-15 -4.74058e-15 -5.23778e-15 -6.017e-15 -6.94501e-15 -7.65427e-15 -7.84041e-15 -8.74914e-15 -9.13164e-15 -9.87051e-15 -1.17554e-14 -1.19892e-14 -1.34698e-14 -1.51032e-14 -1.55803e-14 -1.759e-14 -1.88156e-14 -1.99412e-14 -2.13197e-14 -2.15938e-14 -2.30491e-14 -2.3581e-14 -2.49873e-14 -2.6116e-14 -2.86055e-14 -3.22324e-14 -3.3155e-14 -3.35879e-14 -3.68105e-14 -3.87799e-14 -4.09096e-14 -4.43543e-14 -4.86832e-14 -5.08205e-14 -5.21878e-14 -5.67198e-14 -5.96175e-14 -6.54364e-14 -6.73705e-14 -7.05449e-14 -7.60507e-14 -7.97494e-14 -8.15429e-14 -8.55435e-14 -8.77699e-14 -9.61526e-14 -9.99249e-14 -1.02573e-13 -1.04567e-13 -1.13184e-13 -1.16864e-13 -1.27924e-13 -1.33393e-13 -1.36094e-13 -1.42171e-13 -1.52787e-13 -1.54833e-13 -1.64929e-13 -1.699e-13 -1.79654e-13 -1.83442e-13 -1.97142e-13 -2.11987e-13 -2.2212e-13 -2.24497e-13 -2.29748e-13 -2.32274e-13 -2.42866e-13 -2.52672e-13 -2.72472e-13 -2.80419e-13 -2.89329e-13 -3.01063e-13 -3.12078e-13 -3.31314e-13 -3.44217e-13 -3.53446e-13 -3.59518e-13 -3.73339e-13 -3.91661e-13 -3.99794e-13 -4.22261e-13 -4.36173e-13 -4.581e-13 -4.6864e-13 -5.03118e-13 -5.14217e-13 -5.20797e-13 -5.56986e-13 -5.64542e-13 -5.82459e-13 -6.05782e-13 -6.25377e-13 -6.30576e-13 -6.55691e-13 -6.80004e-13 -6.83333e-13 -7.21023e-13 -7.25088e-13 -7.50613e-13 -7.70865e-13 -8.13039e-13 -8.25481e-13 -8.44359e-13 -8.4773e-13 -9.02988e-13 -9.20939e-13 -9.4504e-13 -9.96507e-13 -1.01588e-12 -1.03602e-12 -1.07917e-12 -1.09231e-12 -1.11465e-12 -1.1499e-12 -1.16233e-12 -1.19555e-12 -1.23182e-12 -1.26457e-12 -1.31607e-12 -1.33455e-12 -1.34232e-12 -1.35921e-12 -1.44e-12 -1.45164e-12 -1.48693e-12 -1.52027e-12 -1.55585e-12 -1.60597e-12 -1.61634e-12 -1.65232e-12 -1.69198e-12 -1.7438e-12 -1.77951e-12 -1.8239e-12 -1.85721e-12 -1.89554e-12 -1.95747e-12 -2.02753e-12 -2.06826e-12 -2.13031e-12 -2.13196e-12 -2.15219e-12 -2.27799e-12 -2.31803e-12 -2.34879e-12 -2.40175e-12 -2.47899e-12 -2.545e-12 -2.59225e-12 -2.60742e-12 -2.62943e-12 -2.66125e-12 -2.76783e-12 -2.81567e-12 -2.90772e-12 -2.96702e-12 -3.01653e-12 -3.15335e-12 -3.23855e-12 -3.24751e-12 -3.3031e-12 -3.33305e-12 -3.40871e-12 -3.45337e-12 -3.50447e-12 -3.61073e-12 -3.64742e-12 -3.72637e-12 -3.77371e-12 -3.84323e-12 -3.90057e-12 -3.97848e-12 -4.07818e-12 -4.12454e-12 -4.15046e-12 -4.23121e-12 -4.29324e-12 -4.37574e-12 -4.44523e-12 -4.54149e-12 -4.65499e-12 -4.76751e-12 -4.80435e-12 -4.88517e-12 -4.90975e-12 -5.01624e-12 -5.16949e-12 -5.22383e-12 -5.35253e-12 -5.36621e-12 -5.60203e-12 -5.66449e-12 -5.75557e-12 -5.87071e-12 -5.91888e-12 -5.98385e-12 -6.03217e-12 -6.2059e-12 -6.35759e-12 -6.49219e-12 -6.5615e-12 -6.6576e-12 -6.72985e-12 -6.80709e-12 -6.98579e-12 -7.06008e-12 -7.25475e-12 -7.41201e-12 -7.61434e-12 -7.68743e-12 -7.79264e-12 -7.90597e-12 -7.99185e-12 -8.1357e-12 -8.2733e-12 -8.52011e-12 -8.68321e-12 -8.72805e-12 -8.78545e-12 -9.14293e-12 -9.36166e-12 -9.41481e-12 -9.55606e-12 -9.68282e-12 -1.0036e-11 -1.01177e-11 -1.02118e-11 -1.02935e-11 -1.06209e-11 -1.0927e-11 -1.13614e-11 -1.14727e-11 -1.16499e-11 -1.21045e-11 -1.23256e-11 -1.28173e-11 -1.37283e-11 -1.45226e-11 -1.54851e-11 ]

LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:318) Wrote LDA transform to pretrained-model/lre17_train_30s/transform_100.mat
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat ark:pretrained-model/lre17_train_30s/spk_xvector_mean.ark ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 14 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 2.75929 (should be close to zero), length divided by sqrt(dim) was 1.40767 (should probably be close to one)
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 492731 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 1.53327 (should be close to zero), length divided by sqrt(dim) was 1.2004 (should probably be close to one)
ivector-mean ark:pretrained-model/lre17_train_30s/xvector_lda100.ark pretrained-model/lre17_train_30s/xvector_lda100.global.vec 
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100.ark ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 14 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean.ark ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 14 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.00249, standard deviation was 1.05391
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_dev_3s/xvector.scp ark:pretrained-model/lre17_dev_3s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 926 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 2.21882 (should be close to zero), length divided by sqrt(dim) was 1.5711 (should probably be close to one)
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_dev_3s/xvector_lda100.ark ark:pretrained-model/lre17_dev_3s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 926 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_dev_3s/xvector_lda100_submean.ark ark:pretrained-model/lre17_dev_3s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 926 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.46195, standard deviation was 0.647369
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_train_30s/xvector_lda100.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 492731 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.12033, standard deviation was 0.431066
ivector-compute-plda ark:data/lre17_train_30s/spk2utt ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark pretrained-model/lre17_train_30s/plda 
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance ara-acm-ar-20031217-063700-0-b-40228-40378
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance ara-apc-arb-lev-20040721-231950-a-03109-03260
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance ara-apc-fla-0585-b-22127-22303
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance eng-usg-en-4902-b-50689-50953
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance spa-car-20050929-220820-154-fsp-a-65613-65764
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance spa-lac-20051213-185913-737-fsp-b-35030-35185
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance zho-cmn-TM-003970-b-52704-52901
LOG (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:109) Accumulated stats from 14 speakers (0 with no utterances), consisting of 492731 utterances (7 absent from input).
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 0 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0484
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9578
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 1 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0474
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9649
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 2 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.047
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.962
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 3 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0468
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9611
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 4 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0466
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9607
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 5 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0465
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9604
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 6 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0464
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9603
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 7 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0463
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9602
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 8 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0463
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9601
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 9 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0462
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9601
LOG (ivector-compute-plda[5.5.883~1-3eea3]:GetOutput():plda.cc:540) Norm of mean of iVector distribution is 1.36617
LOG (ivector-compute-plda[5.5.883~1-3eea3]:GetOutput():plda.cc:577) Diagonal of between-class variance in normalized space is  [ 6.80719 5.07642 4.21652 3.83125 3.16111 2.9845 1.87321 1.66839 1.41967 1.1445 1.04717 0.410991 0.353115 4.81664e-06 4.80518e-06 4.79952e-06 4.78659e-06 4.78533e-06 4.76916e-06 4.76006e-06 4.75583e-06 4.74886e-06 4.74146e-06 4.73521e-06 4.7278e-06 4.72638e-06 4.72055e-06 4.71931e-06 4.71476e-06 4.71429e-06 4.70611e-06 4.70559e-06 4.70065e-06 4.69463e-06 4.68495e-06 4.68266e-06 4.67751e-06 4.67639e-06 4.67314e-06 4.66825e-06 4.66532e-06 4.6579e-06 4.65547e-06 4.65337e-06 4.64883e-06 4.64267e-06 4.63883e-06 4.63503e-06 4.63125e-06 4.62836e-06 4.62584e-06 4.62273e-06 4.61574e-06 4.61444e-06 4.61382e-06 4.61116e-06 4.60593e-06 4.6043e-06 4.60154e-06 4.59664e-06 4.5945e-06 4.59077e-06 4.5904e-06 4.58557e-06 4.58489e-06 4.58178e-06 4.57969e-06 4.57774e-06 4.5726e-06 4.56954e-06 4.56696e-06 4.56051e-06 4.55849e-06 4.55698e-06 4.55087e-06 4.54951e-06 4.54885e-06 4.54784e-06 4.54052e-06 4.53963e-06 4.53844e-06 4.53773e-06 4.53246e-06 4.52768e-06 4.52645e-06 4.52513e-06 4.52103e-06 4.51922e-06 4.51895e-06 4.51649e-06 4.51228e-06 4.51026e-06 4.50859e-06 4.5083e-06 4.5071e-06 4.5048e-06 4.50242e-06 4.49714e-06 4.48988e-06 4.43171e-06 ]

ivector-plda-scoring --normalize-length=true --num-utts=ark:pretrained-model/lre17_train_30s/num_utts.ark 'ivector-copy-plda --smoothing=0.0 pretrained-model/lre17_train_30s/plda - |' ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean_norm.ark ark:pretrained-model/lre17_dev_3s/xvector_lda100_submean_norm.ark - pretrained-model/lre17_dev_3s/score/plda_lre17_train_30s_lre17_dev_3s_lda100_submean_norm.score 
ivector-copy-plda --smoothing=0.0 pretrained-model/lre17_train_30s/plda - 
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:96) Reading train iVectors
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:122) Read 14 training iVectors, errors on 0
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:126) Average renormalization scale on training iVectors was 0.375309
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:129) Reading test iVectors
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:147) Read 926 test iVectors.
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:150) Average renormalization scale on test iVectors was 1.05697
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:214) Mean score was -22.2453, standard deviation was 31.8945
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:217) Processed 12964 trials, 0 had errors.
compute-eer - 
LOG (compute-eer[5.5.671~1-e5a5a]:main():compute-eer.cc:136) Equal error rate is 24.514%, at threshold -1.61723
EER% 24.51

[ lre17_dev_3s ]
24.51	pretrained-model/lre17_dev_3s/score/plda_lre17_train_30s_lre17_dev_3s_lda100_submean_norm.eer


############################
lre17_train_30s
lre17_train_30s
lre17_dev_3s
true
############################
#LOG::Steps 01 Check and generate config Done
[Auto find] Your vectortype is xvector

#LOG Steps02::Determine Your vectortype is xvector Done
[Notice] It will set the default config lre17_train_30s[lre17_train_30s lre17_train_30s lre17_dev_3s] for lda, submean and whiten, if used.
#LOG Steps03::Configuration information Done
steps4 start
Vectordir:pretrained-model
lre17_dev_3s
lre17_train_30s
#LOG Steps04::Check x-vector or I-vector generation error Done
##Step5 start
lre17_train_30s[lre17_train_30s lre17_train_30s lre17_dev_3s]
#LOG Steps05:: Done
#LOG Steps06::Build connection between score process and enrollset Done
#LOG Steps07::False to speed up and true to clear files with error   Done
#LOG Steps08::Get Trials Done
#LOG Steps09:: Done
[ plda ]
ivector-mean ark:data/lre17_train_30s/spk2utt scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/spk_xvector_mean.ark ark,t:pretrained-model/lre17_train_30s/num_utts.ark 
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance ara-acm-ar-20031217-063700-0-b-40228-40378
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance ara-apc-arb-lev-20040721-231950-a-03109-03260
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance ara-apc-fla-0585-b-22127-22303
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance eng-usg-en-4902-b-50689-50953
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance spa-car-20050929-220820-154-fsp-a-65613-65764
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance spa-lac-20051213-185913-737-fsp-b-35030-35185
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance zho-cmn-TM-003970-b-52704-52901
LOG (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:134) Computed mean of 14 speakers (0 with no utterances), consisting of 492731 utterances (7 absent from input).
LOG (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:145) Norm of mean of speakers is 17.7422, root-mean-square speaker-iVector length divided by sqrt(dim) is 1.29274
ivector-normalize-length scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.26335, standard deviation was 0.352726
ivector-compute-lda --dim=100 --total-covariance-factor=0.1 ark:pretrained-model/lre17_train_30s/xvector_norm.ark ark:data/lre17_train_30s/utt2spk pretrained-model/lre17_train_30s/transform_100.mat 
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:288) Read 492731 utterances, 0 with errors.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:294) Computing within-class covariance.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:299) 2-norm of iVector mean is 15.9953
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:136) Stats have 14 speakers, 492731 utterances. 
WARNING (ivector-compute-lda[5.5.883~1-3eea3]:ComputeNormalizingTransform():ivector-compute-lda.cc:98) Floored 129 eigenvalues of covariance to 1.98841e-05
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:171) Singular values of between-class covariance after projecting with interpolated [total/within] covariance with a weight of 0.1 on the total covariance, are:  [ 2.90588 2.40519 2.15112 1.57986 1.19346 1.04706 1.00489 0.626699 0.492955 0.430551 0.339654 0.262772 0.222067 3.39918e-10 1.50561e-11 1.41766e-11 1.3787e-11 1.29261e-11 1.23991e-11 1.20515e-11 1.17956e-11 1.15073e-11 1.10855e-11 1.08423e-11 1.06726e-11 1.043e-11 1.01446e-11 9.95466e-12 9.83686e-12 9.57723e-12 9.48959e-12 9.26464e-12 9.12586e-12 8.87946e-12 8.80499e-12 8.68645e-12 8.51801e-12 8.23287e-12 8.02002e-12 7.93907e-12 7.75004e-12 7.65597e-12 7.50592e-12 7.38407e-12 7.29823e-12 7.26841e-12 7.11195e-12 7.03757e-12 6.84603e-12 6.69938e-12 6.68551e-12 6.52209e-12 6.43416e-12 6.36334e-12 6.18289e-12 6.10256e-12 6.02968e-12 6.02539e-12 5.94713e-12 5.87727e-12 5.75655e-12 5.65056e-12 5.59136e-12 5.47039e-12 5.42325e-12 5.29381e-12 5.1476e-12 5.13813e-12 4.97784e-12 4.89734e-12 4.82425e-12 4.79652e-12 4.68154e-12 4.63167e-12 4.53169e-12 4.4588e-12 4.41556e-12 4.32868e-12 4.29275e-12 4.19057e-12 4.11548e-12 4.02008e-12 3.97968e-12 3.93841e-12 3.84898e-12 3.72921e-12 3.66285e-12 3.59908e-12 3.56063e-12 3.48e-12 3.37638e-12 3.33863e-12 3.26507e-12 3.18899e-12 3.10062e-12 3.06316e-12 3.00453e-12 2.97976e-12 2.94594e-12 2.83621e-12 2.77813e-12 2.72591e-12 2.67029e-12 2.61699e-12 2.5666e-12 2.49457e-12 2.47714e-12 2.4372e-12 2.37685e-12 2.33161e-12 2.30659e-12 2.24263e-12 2.21768e-12 2.1415e-12 2.1314e-12 1.9936e-12 1.98225e-12 1.93959e-12 1.89675e-12 1.84232e-12 1.8375e-12 1.75251e-12 1.6588e-12 1.61799e-12 1.58681e-12 1.56003e-12 1.52131e-12 1.50476e-12 1.46223e-12 1.43422e-12 1.40598e-12 1.37978e-12 1.32589e-12 1.3049e-12 1.24837e-12 1.20679e-12 1.20302e-12 1.16225e-12 1.15229e-12 1.11321e-12 1.08741e-12 1.07572e-12 1.03118e-12 9.95066e-13 9.6071e-13 9.44611e-13 9.38644e-13 9.01184e-13 8.89295e-13 8.61739e-13 8.18741e-13 7.93326e-13 7.64134e-13 7.58101e-13 7.31556e-13 6.96536e-13 6.84815e-13 6.76163e-13 6.65161e-13 6.44676e-13 6.0626e-13 5.88007e-13 5.65442e-13 5.43494e-13 5.38411e-13 5.12856e-13 4.99757e-13 4.87929e-13 4.61777e-13 4.41923e-13 4.31617e-13 4.22456e-13 4.08345e-13 3.9768e-13 3.77143e-13 3.70577e-13 3.58922e-13 3.45225e-13 3.31515e-13 3.10234e-13 2.98156e-13 2.9378e-13 2.80455e-13 2.74832e-13 2.62717e-13 2.545e-13 2.48787e-13 2.43613e-13 2.34035e-13 2.07882e-13 2.05728e-13 1.95553e-13 1.86808e-13 1.84096e-13 1.77587e-13 1.71563e-13 1.62316e-13 1.54437e-13 1.50053e-13 1.3997e-13 1.35626e-13 1.26337e-13 1.23558e-13 1.1968e-13 1.12383e-13 1.02613e-13 9.82605e-14 9.51325e-14 8.87849e-14 8.67413e-14 8.27222e-14 7.68338e-14 7.34747e-14 7.28316e-14 6.4683e-14 6.32607e-14 5.92533e-14 5.8259e-14 5.41779e-14 5.26459e-14 5.00974e-14 4.78823e-14 4.38248e-14 4.2568e-14 3.98685e-14 3.85568e-14 3.42532e-14 3.32714e-14 3.08674e-14 2.871e-14 2.64161e-14 2.59336e-14 2.47206e-14 2.27356e-14 2.05709e-14 2.03413e-14 1.90146e-14 1.80811e-14 1.6708e-14 1.44957e-14 1.36066e-14 1.29644e-14 1.24864e-14 1.06508e-14 1.00594e-14 9.57377e-15 9.0738e-15 7.92316e-15 7.28889e-15 6.41913e-15 6.12398e-15 5.39913e-15 4.97701e-15 3.98205e-15 3.49914e-15 3.1913e-15 2.56876e-15 2.0396e-15 1.4198e-15 1.02199e-15 6.15529e-16 3.15916e-16 -1.96798e-16 -3.17994e-16 -5.90426e-16 -7.74202e-16 -1.11434e-15 -1.5514e-15 -2.05356e-15 -3.02328e-15 -3.13438e-15 -3.45145e-15 -4.22404e-15 -4.74058e-15 -5.23778e-15 -6.017e-15 -6.94501e-15 -7.65427e-15 -7.84041e-15 -8.74914e-15 -9.13164e-15 -9.87051e-15 -1.17554e-14 -1.19892e-14 -1.34698e-14 -1.51032e-14 -1.55803e-14 -1.759e-14 -1.88156e-14 -1.99412e-14 -2.13197e-14 -2.15938e-14 -2.30491e-14 -2.3581e-14 -2.49873e-14 -2.6116e-14 -2.86055e-14 -3.22324e-14 -3.3155e-14 -3.35879e-14 -3.68105e-14 -3.87799e-14 -4.09096e-14 -4.43543e-14 -4.86832e-14 -5.08205e-14 -5.21878e-14 -5.67198e-14 -5.96175e-14 -6.54364e-14 -6.73705e-14 -7.05449e-14 -7.60507e-14 -7.97494e-14 -8.15429e-14 -8.55435e-14 -8.77699e-14 -9.61526e-14 -9.99249e-14 -1.02573e-13 -1.04567e-13 -1.13184e-13 -1.16864e-13 -1.27924e-13 -1.33393e-13 -1.36094e-13 -1.42171e-13 -1.52787e-13 -1.54833e-13 -1.64929e-13 -1.699e-13 -1.79654e-13 -1.83442e-13 -1.97142e-13 -2.11987e-13 -2.2212e-13 -2.24497e-13 -2.29748e-13 -2.32274e-13 -2.42866e-13 -2.52672e-13 -2.72472e-13 -2.80419e-13 -2.89329e-13 -3.01063e-13 -3.12078e-13 -3.31314e-13 -3.44217e-13 -3.53446e-13 -3.59518e-13 -3.73339e-13 -3.91661e-13 -3.99794e-13 -4.22261e-13 -4.36173e-13 -4.581e-13 -4.6864e-13 -5.03118e-13 -5.14217e-13 -5.20797e-13 -5.56986e-13 -5.64542e-13 -5.82459e-13 -6.05782e-13 -6.25377e-13 -6.30576e-13 -6.55691e-13 -6.80004e-13 -6.83333e-13 -7.21023e-13 -7.25088e-13 -7.50613e-13 -7.70865e-13 -8.13039e-13 -8.25481e-13 -8.44359e-13 -8.4773e-13 -9.02988e-13 -9.20939e-13 -9.4504e-13 -9.96507e-13 -1.01588e-12 -1.03602e-12 -1.07917e-12 -1.09231e-12 -1.11465e-12 -1.1499e-12 -1.16233e-12 -1.19555e-12 -1.23182e-12 -1.26457e-12 -1.31607e-12 -1.33455e-12 -1.34232e-12 -1.35921e-12 -1.44e-12 -1.45164e-12 -1.48693e-12 -1.52027e-12 -1.55585e-12 -1.60597e-12 -1.61634e-12 -1.65232e-12 -1.69198e-12 -1.7438e-12 -1.77951e-12 -1.8239e-12 -1.85721e-12 -1.89554e-12 -1.95747e-12 -2.02753e-12 -2.06826e-12 -2.13031e-12 -2.13196e-12 -2.15219e-12 -2.27799e-12 -2.31803e-12 -2.34879e-12 -2.40175e-12 -2.47899e-12 -2.545e-12 -2.59225e-12 -2.60742e-12 -2.62943e-12 -2.66125e-12 -2.76783e-12 -2.81567e-12 -2.90772e-12 -2.96702e-12 -3.01653e-12 -3.15335e-12 -3.23855e-12 -3.24751e-12 -3.3031e-12 -3.33305e-12 -3.40871e-12 -3.45337e-12 -3.50447e-12 -3.61073e-12 -3.64742e-12 -3.72637e-12 -3.77371e-12 -3.84323e-12 -3.90057e-12 -3.97848e-12 -4.07818e-12 -4.12454e-12 -4.15046e-12 -4.23121e-12 -4.29324e-12 -4.37574e-12 -4.44523e-12 -4.54149e-12 -4.65499e-12 -4.76751e-12 -4.80435e-12 -4.88517e-12 -4.90975e-12 -5.01624e-12 -5.16949e-12 -5.22383e-12 -5.35253e-12 -5.36621e-12 -5.60203e-12 -5.66449e-12 -5.75557e-12 -5.87071e-12 -5.91888e-12 -5.98385e-12 -6.03217e-12 -6.2059e-12 -6.35759e-12 -6.49219e-12 -6.5615e-12 -6.6576e-12 -6.72985e-12 -6.80709e-12 -6.98579e-12 -7.06008e-12 -7.25475e-12 -7.41201e-12 -7.61434e-12 -7.68743e-12 -7.79264e-12 -7.90597e-12 -7.99185e-12 -8.1357e-12 -8.2733e-12 -8.52011e-12 -8.68321e-12 -8.72805e-12 -8.78545e-12 -9.14293e-12 -9.36166e-12 -9.41481e-12 -9.55606e-12 -9.68282e-12 -1.0036e-11 -1.01177e-11 -1.02118e-11 -1.02935e-11 -1.06209e-11 -1.0927e-11 -1.13614e-11 -1.14727e-11 -1.16499e-11 -1.21045e-11 -1.23256e-11 -1.28173e-11 -1.37283e-11 -1.45226e-11 -1.54851e-11 ]

LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:318) Wrote LDA transform to pretrained-model/lre17_train_30s/transform_100.mat
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat ark:pretrained-model/lre17_train_30s/spk_xvector_mean.ark ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 14 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 2.75929 (should be close to zero), length divided by sqrt(dim) was 1.40767 (should probably be close to one)
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 492731 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 1.53327 (should be close to zero), length divided by sqrt(dim) was 1.2004 (should probably be close to one)
ivector-mean ark:pretrained-model/lre17_train_30s/xvector_lda100.ark pretrained-model/lre17_train_30s/xvector_lda100.global.vec 
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100.ark ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 14 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean.ark ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 14 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.00249, standard deviation was 1.05391
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_dev_3s/xvector.scp ark:pretrained-model/lre17_dev_3s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 926 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 2.21882 (should be close to zero), length divided by sqrt(dim) was 1.5711 (should probably be close to one)
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_dev_3s/xvector_lda100.ark ark:pretrained-model/lre17_dev_3s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 926 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_dev_3s/xvector_lda100_submean.ark ark:pretrained-model/lre17_dev_3s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 926 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.46195, standard deviation was 0.647369
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_train_30s/xvector_lda100.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 492731 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.12033, standard deviation was 0.431066
ivector-compute-plda ark:data/lre17_train_30s/spk2utt ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark pretrained-model/lre17_train_30s/plda 
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance ara-acm-ar-20031217-063700-0-b-40228-40378
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance ara-apc-arb-lev-20040721-231950-a-03109-03260
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance ara-apc-fla-0585-b-22127-22303
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance eng-usg-en-4902-b-50689-50953
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance spa-car-20050929-220820-154-fsp-a-65613-65764
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance spa-lac-20051213-185913-737-fsp-b-35030-35185
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance zho-cmn-TM-003970-b-52704-52901
LOG (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:109) Accumulated stats from 14 speakers (0 with no utterances), consisting of 492731 utterances (7 absent from input).
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 0 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0484
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9578
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 1 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0474
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9649
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 2 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.047
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.962
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 3 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0468
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9611
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 4 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0466
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9607
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 5 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0465
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9604
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 6 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0464
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9603
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 7 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0463
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9602
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 8 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0463
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9601
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 9 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0462
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9601
LOG (ivector-compute-plda[5.5.883~1-3eea3]:GetOutput():plda.cc:540) Norm of mean of iVector distribution is 1.36617
LOG (ivector-compute-plda[5.5.883~1-3eea3]:GetOutput():plda.cc:577) Diagonal of between-class variance in normalized space is  [ 6.80719 5.07642 4.21652 3.83125 3.16111 2.9845 1.87321 1.66839 1.41967 1.1445 1.04717 0.410991 0.353115 4.81664e-06 4.80518e-06 4.79952e-06 4.78659e-06 4.78533e-06 4.76916e-06 4.76006e-06 4.75583e-06 4.74886e-06 4.74146e-06 4.73521e-06 4.7278e-06 4.72638e-06 4.72055e-06 4.71931e-06 4.71476e-06 4.71429e-06 4.70611e-06 4.70559e-06 4.70065e-06 4.69463e-06 4.68495e-06 4.68266e-06 4.67751e-06 4.67639e-06 4.67314e-06 4.66825e-06 4.66532e-06 4.6579e-06 4.65547e-06 4.65337e-06 4.64883e-06 4.64267e-06 4.63883e-06 4.63503e-06 4.63125e-06 4.62836e-06 4.62584e-06 4.62273e-06 4.61574e-06 4.61444e-06 4.61382e-06 4.61116e-06 4.60593e-06 4.6043e-06 4.60154e-06 4.59664e-06 4.5945e-06 4.59077e-06 4.5904e-06 4.58557e-06 4.58489e-06 4.58178e-06 4.57969e-06 4.57774e-06 4.5726e-06 4.56954e-06 4.56696e-06 4.56051e-06 4.55849e-06 4.55698e-06 4.55087e-06 4.54951e-06 4.54885e-06 4.54784e-06 4.54052e-06 4.53963e-06 4.53844e-06 4.53773e-06 4.53246e-06 4.52768e-06 4.52645e-06 4.52513e-06 4.52103e-06 4.51922e-06 4.51895e-06 4.51649e-06 4.51228e-06 4.51026e-06 4.50859e-06 4.5083e-06 4.5071e-06 4.5048e-06 4.50242e-06 4.49714e-06 4.48988e-06 4.43171e-06 ]

ivector-plda-scoring --normalize-length=true --num-utts=ark:pretrained-model/lre17_train_30s/num_utts.ark 'ivector-copy-plda --smoothing=0.0 pretrained-model/lre17_train_30s/plda - |' ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean_norm.ark ark:pretrained-model/lre17_dev_3s/xvector_lda100_submean_norm.ark - pretrained-model/lre17_dev_3s/score/plda_lre17_train_30s_lre17_dev_3s_lda100_submean_norm.score 
ivector-copy-plda --smoothing=0.0 pretrained-model/lre17_train_30s/plda - 
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:96) Reading train iVectors
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:122) Read 14 training iVectors, errors on 0
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:126) Average renormalization scale on training iVectors was 0.375309
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:129) Reading test iVectors
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:147) Read 926 test iVectors.
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:150) Average renormalization scale on test iVectors was 1.05697
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:214) Mean score was -22.2453, standard deviation was 31.8945
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:217) Processed 12964 trials, 0 had errors.
Cavg: 0.2621

[ lre17_dev_3s ]
Cavg: 0.2621	pretrained-model/lre17_dev_3s/score/plda_lre17_train_30s_lre17_dev_3s_lda100_submean_norm.Cavg


#LOG:: getScore use plda Done!
############################
lre17_train_30s
lre17_train_30s
lre17_dev_10s
true
############################
#LOG::Steps 01 Check and generate config Done
[Auto find] Your vectortype is xvector

#LOG Steps02::Determine Your vectortype is xvector Done
[Notice] It will set the default config lre17_train_30s[lre17_train_30s lre17_train_30s lre17_dev_10s] for lda, submean and whiten, if used.
#LOG Steps03::Configuration information Done
steps4 start
Vectordir:pretrained-model
lre17_dev_10s
lre17_train_30s
#LOG Steps04::Check x-vector or I-vector generation error Done
##Step5 start
lre17_train_30s[lre17_train_30s lre17_train_30s lre17_dev_10s]
#LOG Steps05:: Done
#LOG Steps06::Build connection between score process and enrollset Done
#LOG Steps07::False to speed up and true to clear files with error   Done
Get raw trials...
[Note] You should check it by yourself to avoid spks-lable-discrepancy in two set.
test spks are [ ara-acm ara-apc ara-ary ara-arz eng-gbr eng-usg por-brz qsl-pol qsl-rus spa-car spa-eur spa-lac zho-cmn zho-nan ]
register spks are [ ara-acm ara-apc ara-ary ara-arz eng-gbr eng-usg por-brz qsl-pol qsl-rus spa-car spa-eur spa-lac zho-cmn zho-nan ]
Write to pretrained-model/lre17_dev_10s/trials.raw done.
Remove invalid trials...
0 utt's trials have been removed and the invalid utts is in pretrained-model/lre17_dev_10s/trials.invalid.utt.list
Write to pretrained-model/lre17_dev_10s/trials done.
Make lre17_dev_10s trials done.
#LOG Steps08::Get Trials Done
#LOG Steps09:: Done
[ lr ]
ivector-normalize-length scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.26335, standard deviation was 0.352726
ivector-compute-lda --dim=100 --total-covariance-factor=0.1 ark:pretrained-model/lre17_train_30s/xvector_norm.ark ark:data/lre17_train_30s/utt2spk pretrained-model/lre17_train_30s/transform_100.mat 
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:288) Read 492731 utterances, 0 with errors.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:294) Computing within-class covariance.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:299) 2-norm of iVector mean is 15.9953
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:136) Stats have 14 speakers, 492731 utterances. 
WARNING (ivector-compute-lda[5.5.883~1-3eea3]:ComputeNormalizingTransform():ivector-compute-lda.cc:98) Floored 129 eigenvalues of covariance to 1.98841e-05
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:171) Singular values of between-class covariance after projecting with interpolated [total/within] covariance with a weight of 0.1 on the total covariance, are:  [ 2.90588 2.40519 2.15112 1.57986 1.19346 1.04706 1.00489 0.626699 0.492955 0.430551 0.339654 0.262772 0.222067 3.39918e-10 1.50561e-11 1.41766e-11 1.3787e-11 1.29261e-11 1.23991e-11 1.20515e-11 1.17956e-11 1.15073e-11 1.10855e-11 1.08423e-11 1.06726e-11 1.043e-11 1.01446e-11 9.95466e-12 9.83686e-12 9.57723e-12 9.48959e-12 9.26464e-12 9.12586e-12 8.87946e-12 8.80499e-12 8.68645e-12 8.51801e-12 8.23287e-12 8.02002e-12 7.93907e-12 7.75004e-12 7.65597e-12 7.50592e-12 7.38407e-12 7.29823e-12 7.26841e-12 7.11195e-12 7.03757e-12 6.84603e-12 6.69938e-12 6.68551e-12 6.52209e-12 6.43416e-12 6.36334e-12 6.18289e-12 6.10256e-12 6.02968e-12 6.02539e-12 5.94713e-12 5.87727e-12 5.75655e-12 5.65056e-12 5.59136e-12 5.47039e-12 5.42325e-12 5.29381e-12 5.1476e-12 5.13813e-12 4.97784e-12 4.89734e-12 4.82425e-12 4.79652e-12 4.68154e-12 4.63167e-12 4.53169e-12 4.4588e-12 4.41556e-12 4.32868e-12 4.29275e-12 4.19057e-12 4.11548e-12 4.02008e-12 3.97968e-12 3.93841e-12 3.84898e-12 3.72921e-12 3.66285e-12 3.59908e-12 3.56063e-12 3.48e-12 3.37638e-12 3.33863e-12 3.26507e-12 3.18899e-12 3.10062e-12 3.06316e-12 3.00453e-12 2.97976e-12 2.94594e-12 2.83621e-12 2.77813e-12 2.72591e-12 2.67029e-12 2.61699e-12 2.5666e-12 2.49457e-12 2.47714e-12 2.4372e-12 2.37685e-12 2.33161e-12 2.30659e-12 2.24263e-12 2.21768e-12 2.1415e-12 2.1314e-12 1.9936e-12 1.98225e-12 1.93959e-12 1.89675e-12 1.84232e-12 1.8375e-12 1.75251e-12 1.6588e-12 1.61799e-12 1.58681e-12 1.56003e-12 1.52131e-12 1.50476e-12 1.46223e-12 1.43422e-12 1.40598e-12 1.37978e-12 1.32589e-12 1.3049e-12 1.24837e-12 1.20679e-12 1.20302e-12 1.16225e-12 1.15229e-12 1.11321e-12 1.08741e-12 1.07572e-12 1.03118e-12 9.95066e-13 9.6071e-13 9.44611e-13 9.38644e-13 9.01184e-13 8.89295e-13 8.61739e-13 8.18741e-13 7.93326e-13 7.64134e-13 7.58101e-13 7.31556e-13 6.96536e-13 6.84815e-13 6.76163e-13 6.65161e-13 6.44676e-13 6.0626e-13 5.88007e-13 5.65442e-13 5.43494e-13 5.38411e-13 5.12856e-13 4.99757e-13 4.87929e-13 4.61777e-13 4.41923e-13 4.31617e-13 4.22456e-13 4.08345e-13 3.9768e-13 3.77143e-13 3.70577e-13 3.58922e-13 3.45225e-13 3.31515e-13 3.10234e-13 2.98156e-13 2.9378e-13 2.80455e-13 2.74832e-13 2.62717e-13 2.545e-13 2.48787e-13 2.43613e-13 2.34035e-13 2.07882e-13 2.05728e-13 1.95553e-13 1.86808e-13 1.84096e-13 1.77587e-13 1.71563e-13 1.62316e-13 1.54437e-13 1.50053e-13 1.3997e-13 1.35626e-13 1.26337e-13 1.23558e-13 1.1968e-13 1.12383e-13 1.02613e-13 9.82605e-14 9.51325e-14 8.87849e-14 8.67413e-14 8.27222e-14 7.68338e-14 7.34747e-14 7.28316e-14 6.4683e-14 6.32607e-14 5.92533e-14 5.8259e-14 5.41779e-14 5.26459e-14 5.00974e-14 4.78823e-14 4.38248e-14 4.2568e-14 3.98685e-14 3.85568e-14 3.42532e-14 3.32714e-14 3.08674e-14 2.871e-14 2.64161e-14 2.59336e-14 2.47206e-14 2.27356e-14 2.05709e-14 2.03413e-14 1.90146e-14 1.80811e-14 1.6708e-14 1.44957e-14 1.36066e-14 1.29644e-14 1.24864e-14 1.06508e-14 1.00594e-14 9.57377e-15 9.0738e-15 7.92316e-15 7.28889e-15 6.41913e-15 6.12398e-15 5.39913e-15 4.97701e-15 3.98205e-15 3.49914e-15 3.1913e-15 2.56876e-15 2.0396e-15 1.4198e-15 1.02199e-15 6.15529e-16 3.15916e-16 -1.96798e-16 -3.17994e-16 -5.90426e-16 -7.74202e-16 -1.11434e-15 -1.5514e-15 -2.05356e-15 -3.02328e-15 -3.13438e-15 -3.45145e-15 -4.22404e-15 -4.74058e-15 -5.23778e-15 -6.017e-15 -6.94501e-15 -7.65427e-15 -7.84041e-15 -8.74914e-15 -9.13164e-15 -9.87051e-15 -1.17554e-14 -1.19892e-14 -1.34698e-14 -1.51032e-14 -1.55803e-14 -1.759e-14 -1.88156e-14 -1.99412e-14 -2.13197e-14 -2.15938e-14 -2.30491e-14 -2.3581e-14 -2.49873e-14 -2.6116e-14 -2.86055e-14 -3.22324e-14 -3.3155e-14 -3.35879e-14 -3.68105e-14 -3.87799e-14 -4.09096e-14 -4.43543e-14 -4.86832e-14 -5.08205e-14 -5.21878e-14 -5.67198e-14 -5.96175e-14 -6.54364e-14 -6.73705e-14 -7.05449e-14 -7.60507e-14 -7.97494e-14 -8.15429e-14 -8.55435e-14 -8.77699e-14 -9.61526e-14 -9.99249e-14 -1.02573e-13 -1.04567e-13 -1.13184e-13 -1.16864e-13 -1.27924e-13 -1.33393e-13 -1.36094e-13 -1.42171e-13 -1.52787e-13 -1.54833e-13 -1.64929e-13 -1.699e-13 -1.79654e-13 -1.83442e-13 -1.97142e-13 -2.11987e-13 -2.2212e-13 -2.24497e-13 -2.29748e-13 -2.32274e-13 -2.42866e-13 -2.52672e-13 -2.72472e-13 -2.80419e-13 -2.89329e-13 -3.01063e-13 -3.12078e-13 -3.31314e-13 -3.44217e-13 -3.53446e-13 -3.59518e-13 -3.73339e-13 -3.91661e-13 -3.99794e-13 -4.22261e-13 -4.36173e-13 -4.581e-13 -4.6864e-13 -5.03118e-13 -5.14217e-13 -5.20797e-13 -5.56986e-13 -5.64542e-13 -5.82459e-13 -6.05782e-13 -6.25377e-13 -6.30576e-13 -6.55691e-13 -6.80004e-13 -6.83333e-13 -7.21023e-13 -7.25088e-13 -7.50613e-13 -7.70865e-13 -8.13039e-13 -8.25481e-13 -8.44359e-13 -8.4773e-13 -9.02988e-13 -9.20939e-13 -9.4504e-13 -9.96507e-13 -1.01588e-12 -1.03602e-12 -1.07917e-12 -1.09231e-12 -1.11465e-12 -1.1499e-12 -1.16233e-12 -1.19555e-12 -1.23182e-12 -1.26457e-12 -1.31607e-12 -1.33455e-12 -1.34232e-12 -1.35921e-12 -1.44e-12 -1.45164e-12 -1.48693e-12 -1.52027e-12 -1.55585e-12 -1.60597e-12 -1.61634e-12 -1.65232e-12 -1.69198e-12 -1.7438e-12 -1.77951e-12 -1.8239e-12 -1.85721e-12 -1.89554e-12 -1.95747e-12 -2.02753e-12 -2.06826e-12 -2.13031e-12 -2.13196e-12 -2.15219e-12 -2.27799e-12 -2.31803e-12 -2.34879e-12 -2.40175e-12 -2.47899e-12 -2.545e-12 -2.59225e-12 -2.60742e-12 -2.62943e-12 -2.66125e-12 -2.76783e-12 -2.81567e-12 -2.90772e-12 -2.96702e-12 -3.01653e-12 -3.15335e-12 -3.23855e-12 -3.24751e-12 -3.3031e-12 -3.33305e-12 -3.40871e-12 -3.45337e-12 -3.50447e-12 -3.61073e-12 -3.64742e-12 -3.72637e-12 -3.77371e-12 -3.84323e-12 -3.90057e-12 -3.97848e-12 -4.07818e-12 -4.12454e-12 -4.15046e-12 -4.23121e-12 -4.29324e-12 -4.37574e-12 -4.44523e-12 -4.54149e-12 -4.65499e-12 -4.76751e-12 -4.80435e-12 -4.88517e-12 -4.90975e-12 -5.01624e-12 -5.16949e-12 -5.22383e-12 -5.35253e-12 -5.36621e-12 -5.60203e-12 -5.66449e-12 -5.75557e-12 -5.87071e-12 -5.91888e-12 -5.98385e-12 -6.03217e-12 -6.2059e-12 -6.35759e-12 -6.49219e-12 -6.5615e-12 -6.6576e-12 -6.72985e-12 -6.80709e-12 -6.98579e-12 -7.06008e-12 -7.25475e-12 -7.41201e-12 -7.61434e-12 -7.68743e-12 -7.79264e-12 -7.90597e-12 -7.99185e-12 -8.1357e-12 -8.2733e-12 -8.52011e-12 -8.68321e-12 -8.72805e-12 -8.78545e-12 -9.14293e-12 -9.36166e-12 -9.41481e-12 -9.55606e-12 -9.68282e-12 -1.0036e-11 -1.01177e-11 -1.02118e-11 -1.02935e-11 -1.06209e-11 -1.0927e-11 -1.13614e-11 -1.14727e-11 -1.16499e-11 -1.21045e-11 -1.23256e-11 -1.28173e-11 -1.37283e-11 -1.45226e-11 -1.54851e-11 ]

LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:318) Wrote LDA transform to pretrained-model/lre17_train_30s/transform_100.mat
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 492731 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 1.53327 (should be close to zero), length divided by sqrt(dim) was 1.2004 (should probably be close to one)
ivector-mean ark:pretrained-model/lre17_train_30s/xvector_lda100.ark pretrained-model/lre17_train_30s/xvector_lda100.global.vec 
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_train_30s/xvector_lda100.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 492731 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.12033, standard deviation was 0.431066
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_dev_10s/xvector.scp ark:pretrained-model/lre17_dev_10s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 928 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 1.83826 (should be close to zero), length divided by sqrt(dim) was 1.15397 (should probably be close to one)
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_dev_10s/xvector_lda100.ark ark:pretrained-model/lre17_dev_10s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 928 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_dev_10s/xvector_lda100_submean.ark ark:pretrained-model/lre17_dev_10s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 928 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.0497, standard deviation was 0.54643
logistic-regression-train --max-steps=20 --mix-up=0 ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark ark:data/lre17_train_30s/utt2label pretrained-model/lre17_train_30s/lr/lr.raw.model 
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance ara-acm-ar-20031217-063700-0-b-40228-40378
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance ara-apc-arb-lev-20040721-231950-a-03109-03260
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance ara-apc-fla-0585-b-22127-22303
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance eng-usg-en-4902-b-50689-50953
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance spa-car-20050929-220820-154-fsp-a-65613-65764
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance spa-lac-20051213-185913-737-fsp-b-35030-35185
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance zho-cmn-TM-003970-b-52704-52901
LOG (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:90) Retrieved 492731 vectors with 7 missing. There were 14 class labels.
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -2.6281
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -1.09791
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.756456
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.666002
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.619344
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.608191
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.59609
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.593833
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.589134
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.585039
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.580517
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.58006
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.577648
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.577063
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.576048
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.575238
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.574579
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.574178
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.574065
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.573982
LOG (logistic-regression-train[5.5.883~1-3eea3]:Train():logistic-regression.cc:58) Finished training parameters without mixture components.
logistic-regression-copy pretrained-model/lre17_train_30s/lr/lr.raw.model pretrained-model/lre17_train_30s/lr/lr.scale.model 
LOG (logistic-regression-copy[5.5.883~1-3eea3]:main():logistic-regression-copy.cc:71) Wrote model to pretrained-model/lre17_train_30s/lr/lr.scale.model
logistic-regression-eval --apply-log=true --max-steps=20 --mix-up=0 pretrained-model/lre17_train_30s/lr/lr.scale.model ark:pretrained-model/lre17_train_30s/lr/trials.label ark:pretrained-model/lre17_dev_10s/xvector_lda100_submean_norm.ark - 
LOG (logistic-regression-eval[5.5.883~1-3eea3]:ComputeScores():logistic-regression-eval.cc:111) Calculated scores for 12992 vectors with 0 missing. 
compute-eer - 
LOG (compute-eer[5.5.671~1-e5a5a]:main():compute-eer.cc:136) Equal error rate is 10.7759%, at threshold -3.11385
EER% 10.78

[ lre17_dev_10s ]
10.78	pretrained-model/lre17_dev_10s/score/lr_lre17_train_30s_lre17_dev_10s_lda100_submean_norm.eer


############################
lre17_train_30s
lre17_train_30s
lre17_dev_10s
true
############################
#LOG::Steps 01 Check and generate config Done
[Auto find] Your vectortype is xvector

#LOG Steps02::Determine Your vectortype is xvector Done
[Notice] It will set the default config lre17_train_30s[lre17_train_30s lre17_train_30s lre17_dev_10s] for lda, submean and whiten, if used.
#LOG Steps03::Configuration information Done
steps4 start
Vectordir:pretrained-model
lre17_dev_10s
lre17_train_30s
#LOG Steps04::Check x-vector or I-vector generation error Done
##Step5 start
lre17_train_30s[lre17_train_30s lre17_train_30s lre17_dev_10s]
#LOG Steps05:: Done
#LOG Steps06::Build connection between score process and enrollset Done
#LOG Steps07::False to speed up and true to clear files with error   Done
#LOG Steps08::Get Trials Done
#LOG Steps09:: Done
[ lr ]
ivector-normalize-length scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.26335, standard deviation was 0.352726
ivector-compute-lda --dim=100 --total-covariance-factor=0.1 ark:pretrained-model/lre17_train_30s/xvector_norm.ark ark:data/lre17_train_30s/utt2spk pretrained-model/lre17_train_30s/transform_100.mat 
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:288) Read 492731 utterances, 0 with errors.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:294) Computing within-class covariance.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:299) 2-norm of iVector mean is 15.9953
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:136) Stats have 14 speakers, 492731 utterances. 
WARNING (ivector-compute-lda[5.5.883~1-3eea3]:ComputeNormalizingTransform():ivector-compute-lda.cc:98) Floored 129 eigenvalues of covariance to 1.98841e-05
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:171) Singular values of between-class covariance after projecting with interpolated [total/within] covariance with a weight of 0.1 on the total covariance, are:  [ 2.90588 2.40519 2.15112 1.57986 1.19346 1.04706 1.00489 0.626699 0.492955 0.430551 0.339654 0.262772 0.222067 3.39918e-10 1.50561e-11 1.41766e-11 1.3787e-11 1.29261e-11 1.23991e-11 1.20515e-11 1.17956e-11 1.15073e-11 1.10855e-11 1.08423e-11 1.06726e-11 1.043e-11 1.01446e-11 9.95466e-12 9.83686e-12 9.57723e-12 9.48959e-12 9.26464e-12 9.12586e-12 8.87946e-12 8.80499e-12 8.68645e-12 8.51801e-12 8.23287e-12 8.02002e-12 7.93907e-12 7.75004e-12 7.65597e-12 7.50592e-12 7.38407e-12 7.29823e-12 7.26841e-12 7.11195e-12 7.03757e-12 6.84603e-12 6.69938e-12 6.68551e-12 6.52209e-12 6.43416e-12 6.36334e-12 6.18289e-12 6.10256e-12 6.02968e-12 6.02539e-12 5.94713e-12 5.87727e-12 5.75655e-12 5.65056e-12 5.59136e-12 5.47039e-12 5.42325e-12 5.29381e-12 5.1476e-12 5.13813e-12 4.97784e-12 4.89734e-12 4.82425e-12 4.79652e-12 4.68154e-12 4.63167e-12 4.53169e-12 4.4588e-12 4.41556e-12 4.32868e-12 4.29275e-12 4.19057e-12 4.11548e-12 4.02008e-12 3.97968e-12 3.93841e-12 3.84898e-12 3.72921e-12 3.66285e-12 3.59908e-12 3.56063e-12 3.48e-12 3.37638e-12 3.33863e-12 3.26507e-12 3.18899e-12 3.10062e-12 3.06316e-12 3.00453e-12 2.97976e-12 2.94594e-12 2.83621e-12 2.77813e-12 2.72591e-12 2.67029e-12 2.61699e-12 2.5666e-12 2.49457e-12 2.47714e-12 2.4372e-12 2.37685e-12 2.33161e-12 2.30659e-12 2.24263e-12 2.21768e-12 2.1415e-12 2.1314e-12 1.9936e-12 1.98225e-12 1.93959e-12 1.89675e-12 1.84232e-12 1.8375e-12 1.75251e-12 1.6588e-12 1.61799e-12 1.58681e-12 1.56003e-12 1.52131e-12 1.50476e-12 1.46223e-12 1.43422e-12 1.40598e-12 1.37978e-12 1.32589e-12 1.3049e-12 1.24837e-12 1.20679e-12 1.20302e-12 1.16225e-12 1.15229e-12 1.11321e-12 1.08741e-12 1.07572e-12 1.03118e-12 9.95066e-13 9.6071e-13 9.44611e-13 9.38644e-13 9.01184e-13 8.89295e-13 8.61739e-13 8.18741e-13 7.93326e-13 7.64134e-13 7.58101e-13 7.31556e-13 6.96536e-13 6.84815e-13 6.76163e-13 6.65161e-13 6.44676e-13 6.0626e-13 5.88007e-13 5.65442e-13 5.43494e-13 5.38411e-13 5.12856e-13 4.99757e-13 4.87929e-13 4.61777e-13 4.41923e-13 4.31617e-13 4.22456e-13 4.08345e-13 3.9768e-13 3.77143e-13 3.70577e-13 3.58922e-13 3.45225e-13 3.31515e-13 3.10234e-13 2.98156e-13 2.9378e-13 2.80455e-13 2.74832e-13 2.62717e-13 2.545e-13 2.48787e-13 2.43613e-13 2.34035e-13 2.07882e-13 2.05728e-13 1.95553e-13 1.86808e-13 1.84096e-13 1.77587e-13 1.71563e-13 1.62316e-13 1.54437e-13 1.50053e-13 1.3997e-13 1.35626e-13 1.26337e-13 1.23558e-13 1.1968e-13 1.12383e-13 1.02613e-13 9.82605e-14 9.51325e-14 8.87849e-14 8.67413e-14 8.27222e-14 7.68338e-14 7.34747e-14 7.28316e-14 6.4683e-14 6.32607e-14 5.92533e-14 5.8259e-14 5.41779e-14 5.26459e-14 5.00974e-14 4.78823e-14 4.38248e-14 4.2568e-14 3.98685e-14 3.85568e-14 3.42532e-14 3.32714e-14 3.08674e-14 2.871e-14 2.64161e-14 2.59336e-14 2.47206e-14 2.27356e-14 2.05709e-14 2.03413e-14 1.90146e-14 1.80811e-14 1.6708e-14 1.44957e-14 1.36066e-14 1.29644e-14 1.24864e-14 1.06508e-14 1.00594e-14 9.57377e-15 9.0738e-15 7.92316e-15 7.28889e-15 6.41913e-15 6.12398e-15 5.39913e-15 4.97701e-15 3.98205e-15 3.49914e-15 3.1913e-15 2.56876e-15 2.0396e-15 1.4198e-15 1.02199e-15 6.15529e-16 3.15916e-16 -1.96798e-16 -3.17994e-16 -5.90426e-16 -7.74202e-16 -1.11434e-15 -1.5514e-15 -2.05356e-15 -3.02328e-15 -3.13438e-15 -3.45145e-15 -4.22404e-15 -4.74058e-15 -5.23778e-15 -6.017e-15 -6.94501e-15 -7.65427e-15 -7.84041e-15 -8.74914e-15 -9.13164e-15 -9.87051e-15 -1.17554e-14 -1.19892e-14 -1.34698e-14 -1.51032e-14 -1.55803e-14 -1.759e-14 -1.88156e-14 -1.99412e-14 -2.13197e-14 -2.15938e-14 -2.30491e-14 -2.3581e-14 -2.49873e-14 -2.6116e-14 -2.86055e-14 -3.22324e-14 -3.3155e-14 -3.35879e-14 -3.68105e-14 -3.87799e-14 -4.09096e-14 -4.43543e-14 -4.86832e-14 -5.08205e-14 -5.21878e-14 -5.67198e-14 -5.96175e-14 -6.54364e-14 -6.73705e-14 -7.05449e-14 -7.60507e-14 -7.97494e-14 -8.15429e-14 -8.55435e-14 -8.77699e-14 -9.61526e-14 -9.99249e-14 -1.02573e-13 -1.04567e-13 -1.13184e-13 -1.16864e-13 -1.27924e-13 -1.33393e-13 -1.36094e-13 -1.42171e-13 -1.52787e-13 -1.54833e-13 -1.64929e-13 -1.699e-13 -1.79654e-13 -1.83442e-13 -1.97142e-13 -2.11987e-13 -2.2212e-13 -2.24497e-13 -2.29748e-13 -2.32274e-13 -2.42866e-13 -2.52672e-13 -2.72472e-13 -2.80419e-13 -2.89329e-13 -3.01063e-13 -3.12078e-13 -3.31314e-13 -3.44217e-13 -3.53446e-13 -3.59518e-13 -3.73339e-13 -3.91661e-13 -3.99794e-13 -4.22261e-13 -4.36173e-13 -4.581e-13 -4.6864e-13 -5.03118e-13 -5.14217e-13 -5.20797e-13 -5.56986e-13 -5.64542e-13 -5.82459e-13 -6.05782e-13 -6.25377e-13 -6.30576e-13 -6.55691e-13 -6.80004e-13 -6.83333e-13 -7.21023e-13 -7.25088e-13 -7.50613e-13 -7.70865e-13 -8.13039e-13 -8.25481e-13 -8.44359e-13 -8.4773e-13 -9.02988e-13 -9.20939e-13 -9.4504e-13 -9.96507e-13 -1.01588e-12 -1.03602e-12 -1.07917e-12 -1.09231e-12 -1.11465e-12 -1.1499e-12 -1.16233e-12 -1.19555e-12 -1.23182e-12 -1.26457e-12 -1.31607e-12 -1.33455e-12 -1.34232e-12 -1.35921e-12 -1.44e-12 -1.45164e-12 -1.48693e-12 -1.52027e-12 -1.55585e-12 -1.60597e-12 -1.61634e-12 -1.65232e-12 -1.69198e-12 -1.7438e-12 -1.77951e-12 -1.8239e-12 -1.85721e-12 -1.89554e-12 -1.95747e-12 -2.02753e-12 -2.06826e-12 -2.13031e-12 -2.13196e-12 -2.15219e-12 -2.27799e-12 -2.31803e-12 -2.34879e-12 -2.40175e-12 -2.47899e-12 -2.545e-12 -2.59225e-12 -2.60742e-12 -2.62943e-12 -2.66125e-12 -2.76783e-12 -2.81567e-12 -2.90772e-12 -2.96702e-12 -3.01653e-12 -3.15335e-12 -3.23855e-12 -3.24751e-12 -3.3031e-12 -3.33305e-12 -3.40871e-12 -3.45337e-12 -3.50447e-12 -3.61073e-12 -3.64742e-12 -3.72637e-12 -3.77371e-12 -3.84323e-12 -3.90057e-12 -3.97848e-12 -4.07818e-12 -4.12454e-12 -4.15046e-12 -4.23121e-12 -4.29324e-12 -4.37574e-12 -4.44523e-12 -4.54149e-12 -4.65499e-12 -4.76751e-12 -4.80435e-12 -4.88517e-12 -4.90975e-12 -5.01624e-12 -5.16949e-12 -5.22383e-12 -5.35253e-12 -5.36621e-12 -5.60203e-12 -5.66449e-12 -5.75557e-12 -5.87071e-12 -5.91888e-12 -5.98385e-12 -6.03217e-12 -6.2059e-12 -6.35759e-12 -6.49219e-12 -6.5615e-12 -6.6576e-12 -6.72985e-12 -6.80709e-12 -6.98579e-12 -7.06008e-12 -7.25475e-12 -7.41201e-12 -7.61434e-12 -7.68743e-12 -7.79264e-12 -7.90597e-12 -7.99185e-12 -8.1357e-12 -8.2733e-12 -8.52011e-12 -8.68321e-12 -8.72805e-12 -8.78545e-12 -9.14293e-12 -9.36166e-12 -9.41481e-12 -9.55606e-12 -9.68282e-12 -1.0036e-11 -1.01177e-11 -1.02118e-11 -1.02935e-11 -1.06209e-11 -1.0927e-11 -1.13614e-11 -1.14727e-11 -1.16499e-11 -1.21045e-11 -1.23256e-11 -1.28173e-11 -1.37283e-11 -1.45226e-11 -1.54851e-11 ]

LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:318) Wrote LDA transform to pretrained-model/lre17_train_30s/transform_100.mat
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 492731 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 1.53327 (should be close to zero), length divided by sqrt(dim) was 1.2004 (should probably be close to one)
ivector-mean ark:pretrained-model/lre17_train_30s/xvector_lda100.ark pretrained-model/lre17_train_30s/xvector_lda100.global.vec 
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_train_30s/xvector_lda100.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 492731 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.12033, standard deviation was 0.431066
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_dev_10s/xvector.scp ark:pretrained-model/lre17_dev_10s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 928 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 1.83826 (should be close to zero), length divided by sqrt(dim) was 1.15397 (should probably be close to one)
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_dev_10s/xvector_lda100.ark ark:pretrained-model/lre17_dev_10s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 928 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_dev_10s/xvector_lda100_submean.ark ark:pretrained-model/lre17_dev_10s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 928 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.0497, standard deviation was 0.54643
logistic-regression-train --max-steps=20 --mix-up=0 ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark ark:data/lre17_train_30s/utt2label pretrained-model/lre17_train_30s/lr/lr.raw.model 
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance ara-acm-ar-20031217-063700-0-b-40228-40378
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance ara-apc-arb-lev-20040721-231950-a-03109-03260
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance ara-apc-fla-0585-b-22127-22303
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance eng-usg-en-4902-b-50689-50953
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance spa-car-20050929-220820-154-fsp-a-65613-65764
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance spa-lac-20051213-185913-737-fsp-b-35030-35185
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance zho-cmn-TM-003970-b-52704-52901
LOG (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:90) Retrieved 492731 vectors with 7 missing. There were 14 class labels.
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -2.6281
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -1.09791
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.756456
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.666002
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.619344
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.608191
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.59609
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.593833
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.589134
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.585039
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.580517
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.58006
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.577648
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.577063
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.576048
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.575238
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.574579
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.574178
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.574065
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.573982
LOG (logistic-regression-train[5.5.883~1-3eea3]:Train():logistic-regression.cc:58) Finished training parameters without mixture components.
logistic-regression-copy pretrained-model/lre17_train_30s/lr/lr.raw.model pretrained-model/lre17_train_30s/lr/lr.scale.model 
LOG (logistic-regression-copy[5.5.883~1-3eea3]:main():logistic-regression-copy.cc:71) Wrote model to pretrained-model/lre17_train_30s/lr/lr.scale.model
logistic-regression-eval --apply-log=true --max-steps=20 --mix-up=0 pretrained-model/lre17_train_30s/lr/lr.scale.model ark:pretrained-model/lre17_train_30s/lr/trials.label ark:pretrained-model/lre17_dev_10s/xvector_lda100_submean_norm.ark - 
LOG (logistic-regression-eval[5.5.883~1-3eea3]:ComputeScores():logistic-regression-eval.cc:111) Calculated scores for 12992 vectors with 0 missing. 
Cavg: 0.1025

[ lre17_dev_10s ]
Cavg: 0.1025	pretrained-model/lre17_dev_10s/score/lr_lre17_train_30s_lre17_dev_10s_lda100_submean_norm.Cavg


#LOG:: getScore use lr Done!
############################
lre17_train_30s
lre17_train_30s
lre17_dev_10s
true
############################
#LOG::Steps 01 Check and generate config Done
[Auto find] Your vectortype is xvector

#LOG Steps02::Determine Your vectortype is xvector Done
[Notice] It will set the default config lre17_train_30s[lre17_train_30s lre17_train_30s lre17_dev_10s] for lda, submean and whiten, if used.
#LOG Steps03::Configuration information Done
steps4 start
Vectordir:pretrained-model
lre17_dev_10s
lre17_train_30s
#LOG Steps04::Check x-vector or I-vector generation error Done
##Step5 start
lre17_train_30s[lre17_train_30s lre17_train_30s lre17_dev_10s]
#LOG Steps05:: Done
#LOG Steps06::Build connection between score process and enrollset Done
#LOG Steps07::False to speed up and true to clear files with error   Done
#LOG Steps08::Get Trials Done
#LOG Steps09:: Done
[ plda ]
ivector-mean ark:data/lre17_train_30s/spk2utt scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/spk_xvector_mean.ark ark,t:pretrained-model/lre17_train_30s/num_utts.ark 
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance ara-acm-ar-20031217-063700-0-b-40228-40378
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance ara-apc-arb-lev-20040721-231950-a-03109-03260
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance ara-apc-fla-0585-b-22127-22303
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance eng-usg-en-4902-b-50689-50953
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance spa-car-20050929-220820-154-fsp-a-65613-65764
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance spa-lac-20051213-185913-737-fsp-b-35030-35185
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance zho-cmn-TM-003970-b-52704-52901
LOG (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:134) Computed mean of 14 speakers (0 with no utterances), consisting of 492731 utterances (7 absent from input).
LOG (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:145) Norm of mean of speakers is 17.7422, root-mean-square speaker-iVector length divided by sqrt(dim) is 1.29274
ivector-normalize-length scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.26335, standard deviation was 0.352726
ivector-compute-lda --dim=100 --total-covariance-factor=0.1 ark:pretrained-model/lre17_train_30s/xvector_norm.ark ark:data/lre17_train_30s/utt2spk pretrained-model/lre17_train_30s/transform_100.mat 
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:288) Read 492731 utterances, 0 with errors.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:294) Computing within-class covariance.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:299) 2-norm of iVector mean is 15.9953
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:136) Stats have 14 speakers, 492731 utterances. 
WARNING (ivector-compute-lda[5.5.883~1-3eea3]:ComputeNormalizingTransform():ivector-compute-lda.cc:98) Floored 129 eigenvalues of covariance to 1.98841e-05
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:171) Singular values of between-class covariance after projecting with interpolated [total/within] covariance with a weight of 0.1 on the total covariance, are:  [ 2.90588 2.40519 2.15112 1.57986 1.19346 1.04706 1.00489 0.626699 0.492955 0.430551 0.339654 0.262772 0.222067 3.39918e-10 1.50561e-11 1.41766e-11 1.3787e-11 1.29261e-11 1.23991e-11 1.20515e-11 1.17956e-11 1.15073e-11 1.10855e-11 1.08423e-11 1.06726e-11 1.043e-11 1.01446e-11 9.95466e-12 9.83686e-12 9.57723e-12 9.48959e-12 9.26464e-12 9.12586e-12 8.87946e-12 8.80499e-12 8.68645e-12 8.51801e-12 8.23287e-12 8.02002e-12 7.93907e-12 7.75004e-12 7.65597e-12 7.50592e-12 7.38407e-12 7.29823e-12 7.26841e-12 7.11195e-12 7.03757e-12 6.84603e-12 6.69938e-12 6.68551e-12 6.52209e-12 6.43416e-12 6.36334e-12 6.18289e-12 6.10256e-12 6.02968e-12 6.02539e-12 5.94713e-12 5.87727e-12 5.75655e-12 5.65056e-12 5.59136e-12 5.47039e-12 5.42325e-12 5.29381e-12 5.1476e-12 5.13813e-12 4.97784e-12 4.89734e-12 4.82425e-12 4.79652e-12 4.68154e-12 4.63167e-12 4.53169e-12 4.4588e-12 4.41556e-12 4.32868e-12 4.29275e-12 4.19057e-12 4.11548e-12 4.02008e-12 3.97968e-12 3.93841e-12 3.84898e-12 3.72921e-12 3.66285e-12 3.59908e-12 3.56063e-12 3.48e-12 3.37638e-12 3.33863e-12 3.26507e-12 3.18899e-12 3.10062e-12 3.06316e-12 3.00453e-12 2.97976e-12 2.94594e-12 2.83621e-12 2.77813e-12 2.72591e-12 2.67029e-12 2.61699e-12 2.5666e-12 2.49457e-12 2.47714e-12 2.4372e-12 2.37685e-12 2.33161e-12 2.30659e-12 2.24263e-12 2.21768e-12 2.1415e-12 2.1314e-12 1.9936e-12 1.98225e-12 1.93959e-12 1.89675e-12 1.84232e-12 1.8375e-12 1.75251e-12 1.6588e-12 1.61799e-12 1.58681e-12 1.56003e-12 1.52131e-12 1.50476e-12 1.46223e-12 1.43422e-12 1.40598e-12 1.37978e-12 1.32589e-12 1.3049e-12 1.24837e-12 1.20679e-12 1.20302e-12 1.16225e-12 1.15229e-12 1.11321e-12 1.08741e-12 1.07572e-12 1.03118e-12 9.95066e-13 9.6071e-13 9.44611e-13 9.38644e-13 9.01184e-13 8.89295e-13 8.61739e-13 8.18741e-13 7.93326e-13 7.64134e-13 7.58101e-13 7.31556e-13 6.96536e-13 6.84815e-13 6.76163e-13 6.65161e-13 6.44676e-13 6.0626e-13 5.88007e-13 5.65442e-13 5.43494e-13 5.38411e-13 5.12856e-13 4.99757e-13 4.87929e-13 4.61777e-13 4.41923e-13 4.31617e-13 4.22456e-13 4.08345e-13 3.9768e-13 3.77143e-13 3.70577e-13 3.58922e-13 3.45225e-13 3.31515e-13 3.10234e-13 2.98156e-13 2.9378e-13 2.80455e-13 2.74832e-13 2.62717e-13 2.545e-13 2.48787e-13 2.43613e-13 2.34035e-13 2.07882e-13 2.05728e-13 1.95553e-13 1.86808e-13 1.84096e-13 1.77587e-13 1.71563e-13 1.62316e-13 1.54437e-13 1.50053e-13 1.3997e-13 1.35626e-13 1.26337e-13 1.23558e-13 1.1968e-13 1.12383e-13 1.02613e-13 9.82605e-14 9.51325e-14 8.87849e-14 8.67413e-14 8.27222e-14 7.68338e-14 7.34747e-14 7.28316e-14 6.4683e-14 6.32607e-14 5.92533e-14 5.8259e-14 5.41779e-14 5.26459e-14 5.00974e-14 4.78823e-14 4.38248e-14 4.2568e-14 3.98685e-14 3.85568e-14 3.42532e-14 3.32714e-14 3.08674e-14 2.871e-14 2.64161e-14 2.59336e-14 2.47206e-14 2.27356e-14 2.05709e-14 2.03413e-14 1.90146e-14 1.80811e-14 1.6708e-14 1.44957e-14 1.36066e-14 1.29644e-14 1.24864e-14 1.06508e-14 1.00594e-14 9.57377e-15 9.0738e-15 7.92316e-15 7.28889e-15 6.41913e-15 6.12398e-15 5.39913e-15 4.97701e-15 3.98205e-15 3.49914e-15 3.1913e-15 2.56876e-15 2.0396e-15 1.4198e-15 1.02199e-15 6.15529e-16 3.15916e-16 -1.96798e-16 -3.17994e-16 -5.90426e-16 -7.74202e-16 -1.11434e-15 -1.5514e-15 -2.05356e-15 -3.02328e-15 -3.13438e-15 -3.45145e-15 -4.22404e-15 -4.74058e-15 -5.23778e-15 -6.017e-15 -6.94501e-15 -7.65427e-15 -7.84041e-15 -8.74914e-15 -9.13164e-15 -9.87051e-15 -1.17554e-14 -1.19892e-14 -1.34698e-14 -1.51032e-14 -1.55803e-14 -1.759e-14 -1.88156e-14 -1.99412e-14 -2.13197e-14 -2.15938e-14 -2.30491e-14 -2.3581e-14 -2.49873e-14 -2.6116e-14 -2.86055e-14 -3.22324e-14 -3.3155e-14 -3.35879e-14 -3.68105e-14 -3.87799e-14 -4.09096e-14 -4.43543e-14 -4.86832e-14 -5.08205e-14 -5.21878e-14 -5.67198e-14 -5.96175e-14 -6.54364e-14 -6.73705e-14 -7.05449e-14 -7.60507e-14 -7.97494e-14 -8.15429e-14 -8.55435e-14 -8.77699e-14 -9.61526e-14 -9.99249e-14 -1.02573e-13 -1.04567e-13 -1.13184e-13 -1.16864e-13 -1.27924e-13 -1.33393e-13 -1.36094e-13 -1.42171e-13 -1.52787e-13 -1.54833e-13 -1.64929e-13 -1.699e-13 -1.79654e-13 -1.83442e-13 -1.97142e-13 -2.11987e-13 -2.2212e-13 -2.24497e-13 -2.29748e-13 -2.32274e-13 -2.42866e-13 -2.52672e-13 -2.72472e-13 -2.80419e-13 -2.89329e-13 -3.01063e-13 -3.12078e-13 -3.31314e-13 -3.44217e-13 -3.53446e-13 -3.59518e-13 -3.73339e-13 -3.91661e-13 -3.99794e-13 -4.22261e-13 -4.36173e-13 -4.581e-13 -4.6864e-13 -5.03118e-13 -5.14217e-13 -5.20797e-13 -5.56986e-13 -5.64542e-13 -5.82459e-13 -6.05782e-13 -6.25377e-13 -6.30576e-13 -6.55691e-13 -6.80004e-13 -6.83333e-13 -7.21023e-13 -7.25088e-13 -7.50613e-13 -7.70865e-13 -8.13039e-13 -8.25481e-13 -8.44359e-13 -8.4773e-13 -9.02988e-13 -9.20939e-13 -9.4504e-13 -9.96507e-13 -1.01588e-12 -1.03602e-12 -1.07917e-12 -1.09231e-12 -1.11465e-12 -1.1499e-12 -1.16233e-12 -1.19555e-12 -1.23182e-12 -1.26457e-12 -1.31607e-12 -1.33455e-12 -1.34232e-12 -1.35921e-12 -1.44e-12 -1.45164e-12 -1.48693e-12 -1.52027e-12 -1.55585e-12 -1.60597e-12 -1.61634e-12 -1.65232e-12 -1.69198e-12 -1.7438e-12 -1.77951e-12 -1.8239e-12 -1.85721e-12 -1.89554e-12 -1.95747e-12 -2.02753e-12 -2.06826e-12 -2.13031e-12 -2.13196e-12 -2.15219e-12 -2.27799e-12 -2.31803e-12 -2.34879e-12 -2.40175e-12 -2.47899e-12 -2.545e-12 -2.59225e-12 -2.60742e-12 -2.62943e-12 -2.66125e-12 -2.76783e-12 -2.81567e-12 -2.90772e-12 -2.96702e-12 -3.01653e-12 -3.15335e-12 -3.23855e-12 -3.24751e-12 -3.3031e-12 -3.33305e-12 -3.40871e-12 -3.45337e-12 -3.50447e-12 -3.61073e-12 -3.64742e-12 -3.72637e-12 -3.77371e-12 -3.84323e-12 -3.90057e-12 -3.97848e-12 -4.07818e-12 -4.12454e-12 -4.15046e-12 -4.23121e-12 -4.29324e-12 -4.37574e-12 -4.44523e-12 -4.54149e-12 -4.65499e-12 -4.76751e-12 -4.80435e-12 -4.88517e-12 -4.90975e-12 -5.01624e-12 -5.16949e-12 -5.22383e-12 -5.35253e-12 -5.36621e-12 -5.60203e-12 -5.66449e-12 -5.75557e-12 -5.87071e-12 -5.91888e-12 -5.98385e-12 -6.03217e-12 -6.2059e-12 -6.35759e-12 -6.49219e-12 -6.5615e-12 -6.6576e-12 -6.72985e-12 -6.80709e-12 -6.98579e-12 -7.06008e-12 -7.25475e-12 -7.41201e-12 -7.61434e-12 -7.68743e-12 -7.79264e-12 -7.90597e-12 -7.99185e-12 -8.1357e-12 -8.2733e-12 -8.52011e-12 -8.68321e-12 -8.72805e-12 -8.78545e-12 -9.14293e-12 -9.36166e-12 -9.41481e-12 -9.55606e-12 -9.68282e-12 -1.0036e-11 -1.01177e-11 -1.02118e-11 -1.02935e-11 -1.06209e-11 -1.0927e-11 -1.13614e-11 -1.14727e-11 -1.16499e-11 -1.21045e-11 -1.23256e-11 -1.28173e-11 -1.37283e-11 -1.45226e-11 -1.54851e-11 ]

LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:318) Wrote LDA transform to pretrained-model/lre17_train_30s/transform_100.mat
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat ark:pretrained-model/lre17_train_30s/spk_xvector_mean.ark ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 14 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 2.75929 (should be close to zero), length divided by sqrt(dim) was 1.40767 (should probably be close to one)
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 492731 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 1.53327 (should be close to zero), length divided by sqrt(dim) was 1.2004 (should probably be close to one)
ivector-mean ark:pretrained-model/lre17_train_30s/xvector_lda100.ark pretrained-model/lre17_train_30s/xvector_lda100.global.vec 
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100.ark ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 14 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean.ark ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 14 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.00249, standard deviation was 1.05391
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_dev_10s/xvector.scp ark:pretrained-model/lre17_dev_10s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 928 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 1.83826 (should be close to zero), length divided by sqrt(dim) was 1.15397 (should probably be close to one)
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_dev_10s/xvector_lda100.ark ark:pretrained-model/lre17_dev_10s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 928 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_dev_10s/xvector_lda100_submean.ark ark:pretrained-model/lre17_dev_10s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 928 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.0497, standard deviation was 0.54643
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_train_30s/xvector_lda100.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 492731 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.12033, standard deviation was 0.431066
ivector-compute-plda ark:data/lre17_train_30s/spk2utt ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark pretrained-model/lre17_train_30s/plda 
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance ara-acm-ar-20031217-063700-0-b-40228-40378
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance ara-apc-arb-lev-20040721-231950-a-03109-03260
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance ara-apc-fla-0585-b-22127-22303
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance eng-usg-en-4902-b-50689-50953
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance spa-car-20050929-220820-154-fsp-a-65613-65764
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance spa-lac-20051213-185913-737-fsp-b-35030-35185
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance zho-cmn-TM-003970-b-52704-52901
LOG (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:109) Accumulated stats from 14 speakers (0 with no utterances), consisting of 492731 utterances (7 absent from input).
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 0 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0484
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9578
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 1 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0474
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9649
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 2 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.047
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.962
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 3 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0468
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9611
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 4 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0466
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9607
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 5 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0465
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9604
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 6 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0464
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9603
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 7 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0463
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9602
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 8 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0463
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9601
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 9 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0462
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9601
LOG (ivector-compute-plda[5.5.883~1-3eea3]:GetOutput():plda.cc:540) Norm of mean of iVector distribution is 1.36617
LOG (ivector-compute-plda[5.5.883~1-3eea3]:GetOutput():plda.cc:577) Diagonal of between-class variance in normalized space is  [ 6.80719 5.07642 4.21652 3.83125 3.16111 2.9845 1.87321 1.66839 1.41967 1.1445 1.04717 0.410991 0.353115 4.81664e-06 4.80518e-06 4.79952e-06 4.78659e-06 4.78533e-06 4.76916e-06 4.76006e-06 4.75583e-06 4.74886e-06 4.74146e-06 4.73521e-06 4.7278e-06 4.72638e-06 4.72055e-06 4.71931e-06 4.71476e-06 4.71429e-06 4.70611e-06 4.70559e-06 4.70065e-06 4.69463e-06 4.68495e-06 4.68266e-06 4.67751e-06 4.67639e-06 4.67314e-06 4.66825e-06 4.66532e-06 4.6579e-06 4.65547e-06 4.65337e-06 4.64883e-06 4.64267e-06 4.63883e-06 4.63503e-06 4.63125e-06 4.62836e-06 4.62584e-06 4.62273e-06 4.61574e-06 4.61444e-06 4.61382e-06 4.61116e-06 4.60593e-06 4.6043e-06 4.60154e-06 4.59664e-06 4.5945e-06 4.59077e-06 4.5904e-06 4.58557e-06 4.58489e-06 4.58178e-06 4.57969e-06 4.57774e-06 4.5726e-06 4.56954e-06 4.56696e-06 4.56051e-06 4.55849e-06 4.55698e-06 4.55087e-06 4.54951e-06 4.54885e-06 4.54784e-06 4.54052e-06 4.53963e-06 4.53844e-06 4.53773e-06 4.53246e-06 4.52768e-06 4.52645e-06 4.52513e-06 4.52103e-06 4.51922e-06 4.51895e-06 4.51649e-06 4.51228e-06 4.51026e-06 4.50859e-06 4.5083e-06 4.5071e-06 4.5048e-06 4.50242e-06 4.49714e-06 4.48988e-06 4.43171e-06 ]

ivector-plda-scoring --normalize-length=true --num-utts=ark:pretrained-model/lre17_train_30s/num_utts.ark 'ivector-copy-plda --smoothing=0.0 pretrained-model/lre17_train_30s/plda - |' ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean_norm.ark ark:pretrained-model/lre17_dev_10s/xvector_lda100_submean_norm.ark - pretrained-model/lre17_dev_10s/score/plda_lre17_train_30s_lre17_dev_10s_lda100_submean_norm.score 
ivector-copy-plda --smoothing=0.0 pretrained-model/lre17_train_30s/plda - 
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:96) Reading train iVectors
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:122) Read 14 training iVectors, errors on 0
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:126) Average renormalization scale on training iVectors was 0.375309
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:129) Reading test iVectors
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:147) Read 928 test iVectors.
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:150) Average renormalization scale on test iVectors was 1.17976
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:214) Mean score was -34.6846, standard deviation was 42.3223
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:217) Processed 12992 trials, 0 had errors.
compute-eer - 
LOG (compute-eer[5.5.671~1-e5a5a]:main():compute-eer.cc:136) Equal error rate is 19.8276%, at threshold -5.57219
EER% 19.83

[ lre17_dev_10s ]
19.83	pretrained-model/lre17_dev_10s/score/plda_lre17_train_30s_lre17_dev_10s_lda100_submean_norm.eer


############################
lre17_train_30s
lre17_train_30s
lre17_dev_10s
true
############################
#LOG::Steps 01 Check and generate config Done
[Auto find] Your vectortype is xvector

#LOG Steps02::Determine Your vectortype is xvector Done
[Notice] It will set the default config lre17_train_30s[lre17_train_30s lre17_train_30s lre17_dev_10s] for lda, submean and whiten, if used.
#LOG Steps03::Configuration information Done
steps4 start
Vectordir:pretrained-model
lre17_dev_10s
lre17_train_30s
#LOG Steps04::Check x-vector or I-vector generation error Done
##Step5 start
lre17_train_30s[lre17_train_30s lre17_train_30s lre17_dev_10s]
#LOG Steps05:: Done
#LOG Steps06::Build connection between score process and enrollset Done
#LOG Steps07::False to speed up and true to clear files with error   Done
#LOG Steps08::Get Trials Done
#LOG Steps09:: Done
[ plda ]
ivector-mean ark:data/lre17_train_30s/spk2utt scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/spk_xvector_mean.ark ark,t:pretrained-model/lre17_train_30s/num_utts.ark 
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance ara-acm-ar-20031217-063700-0-b-40228-40378
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance ara-apc-arb-lev-20040721-231950-a-03109-03260
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance ara-apc-fla-0585-b-22127-22303
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance eng-usg-en-4902-b-50689-50953
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance spa-car-20050929-220820-154-fsp-a-65613-65764
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance spa-lac-20051213-185913-737-fsp-b-35030-35185
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance zho-cmn-TM-003970-b-52704-52901
LOG (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:134) Computed mean of 14 speakers (0 with no utterances), consisting of 492731 utterances (7 absent from input).
LOG (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:145) Norm of mean of speakers is 17.7422, root-mean-square speaker-iVector length divided by sqrt(dim) is 1.29274
ivector-normalize-length scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.26335, standard deviation was 0.352726
ivector-compute-lda --dim=100 --total-covariance-factor=0.1 ark:pretrained-model/lre17_train_30s/xvector_norm.ark ark:data/lre17_train_30s/utt2spk pretrained-model/lre17_train_30s/transform_100.mat 
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:288) Read 492731 utterances, 0 with errors.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:294) Computing within-class covariance.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:299) 2-norm of iVector mean is 15.9953
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:136) Stats have 14 speakers, 492731 utterances. 
WARNING (ivector-compute-lda[5.5.883~1-3eea3]:ComputeNormalizingTransform():ivector-compute-lda.cc:98) Floored 129 eigenvalues of covariance to 1.98841e-05
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:171) Singular values of between-class covariance after projecting with interpolated [total/within] covariance with a weight of 0.1 on the total covariance, are:  [ 2.90588 2.40519 2.15112 1.57986 1.19346 1.04706 1.00489 0.626699 0.492955 0.430551 0.339654 0.262772 0.222067 3.39918e-10 1.50561e-11 1.41766e-11 1.3787e-11 1.29261e-11 1.23991e-11 1.20515e-11 1.17956e-11 1.15073e-11 1.10855e-11 1.08423e-11 1.06726e-11 1.043e-11 1.01446e-11 9.95466e-12 9.83686e-12 9.57723e-12 9.48959e-12 9.26464e-12 9.12586e-12 8.87946e-12 8.80499e-12 8.68645e-12 8.51801e-12 8.23287e-12 8.02002e-12 7.93907e-12 7.75004e-12 7.65597e-12 7.50592e-12 7.38407e-12 7.29823e-12 7.26841e-12 7.11195e-12 7.03757e-12 6.84603e-12 6.69938e-12 6.68551e-12 6.52209e-12 6.43416e-12 6.36334e-12 6.18289e-12 6.10256e-12 6.02968e-12 6.02539e-12 5.94713e-12 5.87727e-12 5.75655e-12 5.65056e-12 5.59136e-12 5.47039e-12 5.42325e-12 5.29381e-12 5.1476e-12 5.13813e-12 4.97784e-12 4.89734e-12 4.82425e-12 4.79652e-12 4.68154e-12 4.63167e-12 4.53169e-12 4.4588e-12 4.41556e-12 4.32868e-12 4.29275e-12 4.19057e-12 4.11548e-12 4.02008e-12 3.97968e-12 3.93841e-12 3.84898e-12 3.72921e-12 3.66285e-12 3.59908e-12 3.56063e-12 3.48e-12 3.37638e-12 3.33863e-12 3.26507e-12 3.18899e-12 3.10062e-12 3.06316e-12 3.00453e-12 2.97976e-12 2.94594e-12 2.83621e-12 2.77813e-12 2.72591e-12 2.67029e-12 2.61699e-12 2.5666e-12 2.49457e-12 2.47714e-12 2.4372e-12 2.37685e-12 2.33161e-12 2.30659e-12 2.24263e-12 2.21768e-12 2.1415e-12 2.1314e-12 1.9936e-12 1.98225e-12 1.93959e-12 1.89675e-12 1.84232e-12 1.8375e-12 1.75251e-12 1.6588e-12 1.61799e-12 1.58681e-12 1.56003e-12 1.52131e-12 1.50476e-12 1.46223e-12 1.43422e-12 1.40598e-12 1.37978e-12 1.32589e-12 1.3049e-12 1.24837e-12 1.20679e-12 1.20302e-12 1.16225e-12 1.15229e-12 1.11321e-12 1.08741e-12 1.07572e-12 1.03118e-12 9.95066e-13 9.6071e-13 9.44611e-13 9.38644e-13 9.01184e-13 8.89295e-13 8.61739e-13 8.18741e-13 7.93326e-13 7.64134e-13 7.58101e-13 7.31556e-13 6.96536e-13 6.84815e-13 6.76163e-13 6.65161e-13 6.44676e-13 6.0626e-13 5.88007e-13 5.65442e-13 5.43494e-13 5.38411e-13 5.12856e-13 4.99757e-13 4.87929e-13 4.61777e-13 4.41923e-13 4.31617e-13 4.22456e-13 4.08345e-13 3.9768e-13 3.77143e-13 3.70577e-13 3.58922e-13 3.45225e-13 3.31515e-13 3.10234e-13 2.98156e-13 2.9378e-13 2.80455e-13 2.74832e-13 2.62717e-13 2.545e-13 2.48787e-13 2.43613e-13 2.34035e-13 2.07882e-13 2.05728e-13 1.95553e-13 1.86808e-13 1.84096e-13 1.77587e-13 1.71563e-13 1.62316e-13 1.54437e-13 1.50053e-13 1.3997e-13 1.35626e-13 1.26337e-13 1.23558e-13 1.1968e-13 1.12383e-13 1.02613e-13 9.82605e-14 9.51325e-14 8.87849e-14 8.67413e-14 8.27222e-14 7.68338e-14 7.34747e-14 7.28316e-14 6.4683e-14 6.32607e-14 5.92533e-14 5.8259e-14 5.41779e-14 5.26459e-14 5.00974e-14 4.78823e-14 4.38248e-14 4.2568e-14 3.98685e-14 3.85568e-14 3.42532e-14 3.32714e-14 3.08674e-14 2.871e-14 2.64161e-14 2.59336e-14 2.47206e-14 2.27356e-14 2.05709e-14 2.03413e-14 1.90146e-14 1.80811e-14 1.6708e-14 1.44957e-14 1.36066e-14 1.29644e-14 1.24864e-14 1.06508e-14 1.00594e-14 9.57377e-15 9.0738e-15 7.92316e-15 7.28889e-15 6.41913e-15 6.12398e-15 5.39913e-15 4.97701e-15 3.98205e-15 3.49914e-15 3.1913e-15 2.56876e-15 2.0396e-15 1.4198e-15 1.02199e-15 6.15529e-16 3.15916e-16 -1.96798e-16 -3.17994e-16 -5.90426e-16 -7.74202e-16 -1.11434e-15 -1.5514e-15 -2.05356e-15 -3.02328e-15 -3.13438e-15 -3.45145e-15 -4.22404e-15 -4.74058e-15 -5.23778e-15 -6.017e-15 -6.94501e-15 -7.65427e-15 -7.84041e-15 -8.74914e-15 -9.13164e-15 -9.87051e-15 -1.17554e-14 -1.19892e-14 -1.34698e-14 -1.51032e-14 -1.55803e-14 -1.759e-14 -1.88156e-14 -1.99412e-14 -2.13197e-14 -2.15938e-14 -2.30491e-14 -2.3581e-14 -2.49873e-14 -2.6116e-14 -2.86055e-14 -3.22324e-14 -3.3155e-14 -3.35879e-14 -3.68105e-14 -3.87799e-14 -4.09096e-14 -4.43543e-14 -4.86832e-14 -5.08205e-14 -5.21878e-14 -5.67198e-14 -5.96175e-14 -6.54364e-14 -6.73705e-14 -7.05449e-14 -7.60507e-14 -7.97494e-14 -8.15429e-14 -8.55435e-14 -8.77699e-14 -9.61526e-14 -9.99249e-14 -1.02573e-13 -1.04567e-13 -1.13184e-13 -1.16864e-13 -1.27924e-13 -1.33393e-13 -1.36094e-13 -1.42171e-13 -1.52787e-13 -1.54833e-13 -1.64929e-13 -1.699e-13 -1.79654e-13 -1.83442e-13 -1.97142e-13 -2.11987e-13 -2.2212e-13 -2.24497e-13 -2.29748e-13 -2.32274e-13 -2.42866e-13 -2.52672e-13 -2.72472e-13 -2.80419e-13 -2.89329e-13 -3.01063e-13 -3.12078e-13 -3.31314e-13 -3.44217e-13 -3.53446e-13 -3.59518e-13 -3.73339e-13 -3.91661e-13 -3.99794e-13 -4.22261e-13 -4.36173e-13 -4.581e-13 -4.6864e-13 -5.03118e-13 -5.14217e-13 -5.20797e-13 -5.56986e-13 -5.64542e-13 -5.82459e-13 -6.05782e-13 -6.25377e-13 -6.30576e-13 -6.55691e-13 -6.80004e-13 -6.83333e-13 -7.21023e-13 -7.25088e-13 -7.50613e-13 -7.70865e-13 -8.13039e-13 -8.25481e-13 -8.44359e-13 -8.4773e-13 -9.02988e-13 -9.20939e-13 -9.4504e-13 -9.96507e-13 -1.01588e-12 -1.03602e-12 -1.07917e-12 -1.09231e-12 -1.11465e-12 -1.1499e-12 -1.16233e-12 -1.19555e-12 -1.23182e-12 -1.26457e-12 -1.31607e-12 -1.33455e-12 -1.34232e-12 -1.35921e-12 -1.44e-12 -1.45164e-12 -1.48693e-12 -1.52027e-12 -1.55585e-12 -1.60597e-12 -1.61634e-12 -1.65232e-12 -1.69198e-12 -1.7438e-12 -1.77951e-12 -1.8239e-12 -1.85721e-12 -1.89554e-12 -1.95747e-12 -2.02753e-12 -2.06826e-12 -2.13031e-12 -2.13196e-12 -2.15219e-12 -2.27799e-12 -2.31803e-12 -2.34879e-12 -2.40175e-12 -2.47899e-12 -2.545e-12 -2.59225e-12 -2.60742e-12 -2.62943e-12 -2.66125e-12 -2.76783e-12 -2.81567e-12 -2.90772e-12 -2.96702e-12 -3.01653e-12 -3.15335e-12 -3.23855e-12 -3.24751e-12 -3.3031e-12 -3.33305e-12 -3.40871e-12 -3.45337e-12 -3.50447e-12 -3.61073e-12 -3.64742e-12 -3.72637e-12 -3.77371e-12 -3.84323e-12 -3.90057e-12 -3.97848e-12 -4.07818e-12 -4.12454e-12 -4.15046e-12 -4.23121e-12 -4.29324e-12 -4.37574e-12 -4.44523e-12 -4.54149e-12 -4.65499e-12 -4.76751e-12 -4.80435e-12 -4.88517e-12 -4.90975e-12 -5.01624e-12 -5.16949e-12 -5.22383e-12 -5.35253e-12 -5.36621e-12 -5.60203e-12 -5.66449e-12 -5.75557e-12 -5.87071e-12 -5.91888e-12 -5.98385e-12 -6.03217e-12 -6.2059e-12 -6.35759e-12 -6.49219e-12 -6.5615e-12 -6.6576e-12 -6.72985e-12 -6.80709e-12 -6.98579e-12 -7.06008e-12 -7.25475e-12 -7.41201e-12 -7.61434e-12 -7.68743e-12 -7.79264e-12 -7.90597e-12 -7.99185e-12 -8.1357e-12 -8.2733e-12 -8.52011e-12 -8.68321e-12 -8.72805e-12 -8.78545e-12 -9.14293e-12 -9.36166e-12 -9.41481e-12 -9.55606e-12 -9.68282e-12 -1.0036e-11 -1.01177e-11 -1.02118e-11 -1.02935e-11 -1.06209e-11 -1.0927e-11 -1.13614e-11 -1.14727e-11 -1.16499e-11 -1.21045e-11 -1.23256e-11 -1.28173e-11 -1.37283e-11 -1.45226e-11 -1.54851e-11 ]

LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:318) Wrote LDA transform to pretrained-model/lre17_train_30s/transform_100.mat
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat ark:pretrained-model/lre17_train_30s/spk_xvector_mean.ark ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 14 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 2.75929 (should be close to zero), length divided by sqrt(dim) was 1.40767 (should probably be close to one)
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 492731 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 1.53327 (should be close to zero), length divided by sqrt(dim) was 1.2004 (should probably be close to one)
ivector-mean ark:pretrained-model/lre17_train_30s/xvector_lda100.ark pretrained-model/lre17_train_30s/xvector_lda100.global.vec 
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100.ark ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 14 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean.ark ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 14 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.00249, standard deviation was 1.05391
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_dev_10s/xvector.scp ark:pretrained-model/lre17_dev_10s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 928 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 1.83826 (should be close to zero), length divided by sqrt(dim) was 1.15397 (should probably be close to one)
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_dev_10s/xvector_lda100.ark ark:pretrained-model/lre17_dev_10s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 928 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_dev_10s/xvector_lda100_submean.ark ark:pretrained-model/lre17_dev_10s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 928 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.0497, standard deviation was 0.54643
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_train_30s/xvector_lda100.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 492731 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.12033, standard deviation was 0.431066
ivector-compute-plda ark:data/lre17_train_30s/spk2utt ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark pretrained-model/lre17_train_30s/plda 
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance ara-acm-ar-20031217-063700-0-b-40228-40378
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance ara-apc-arb-lev-20040721-231950-a-03109-03260
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance ara-apc-fla-0585-b-22127-22303
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance eng-usg-en-4902-b-50689-50953
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance spa-car-20050929-220820-154-fsp-a-65613-65764
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance spa-lac-20051213-185913-737-fsp-b-35030-35185
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance zho-cmn-TM-003970-b-52704-52901
LOG (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:109) Accumulated stats from 14 speakers (0 with no utterances), consisting of 492731 utterances (7 absent from input).
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 0 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0484
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9578
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 1 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0474
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9649
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 2 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.047
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.962
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 3 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0468
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9611
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 4 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0466
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9607
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 5 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0465
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9604
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 6 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0464
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9603
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 7 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0463
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9602
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 8 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0463
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9601
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 9 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0462
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9601
LOG (ivector-compute-plda[5.5.883~1-3eea3]:GetOutput():plda.cc:540) Norm of mean of iVector distribution is 1.36617
LOG (ivector-compute-plda[5.5.883~1-3eea3]:GetOutput():plda.cc:577) Diagonal of between-class variance in normalized space is  [ 6.80719 5.07642 4.21652 3.83125 3.16111 2.9845 1.87321 1.66839 1.41967 1.1445 1.04717 0.410991 0.353115 4.81664e-06 4.80518e-06 4.79952e-06 4.78659e-06 4.78533e-06 4.76916e-06 4.76006e-06 4.75583e-06 4.74886e-06 4.74146e-06 4.73521e-06 4.7278e-06 4.72638e-06 4.72055e-06 4.71931e-06 4.71476e-06 4.71429e-06 4.70611e-06 4.70559e-06 4.70065e-06 4.69463e-06 4.68495e-06 4.68266e-06 4.67751e-06 4.67639e-06 4.67314e-06 4.66825e-06 4.66532e-06 4.6579e-06 4.65547e-06 4.65337e-06 4.64883e-06 4.64267e-06 4.63883e-06 4.63503e-06 4.63125e-06 4.62836e-06 4.62584e-06 4.62273e-06 4.61574e-06 4.61444e-06 4.61382e-06 4.61116e-06 4.60593e-06 4.6043e-06 4.60154e-06 4.59664e-06 4.5945e-06 4.59077e-06 4.5904e-06 4.58557e-06 4.58489e-06 4.58178e-06 4.57969e-06 4.57774e-06 4.5726e-06 4.56954e-06 4.56696e-06 4.56051e-06 4.55849e-06 4.55698e-06 4.55087e-06 4.54951e-06 4.54885e-06 4.54784e-06 4.54052e-06 4.53963e-06 4.53844e-06 4.53773e-06 4.53246e-06 4.52768e-06 4.52645e-06 4.52513e-06 4.52103e-06 4.51922e-06 4.51895e-06 4.51649e-06 4.51228e-06 4.51026e-06 4.50859e-06 4.5083e-06 4.5071e-06 4.5048e-06 4.50242e-06 4.49714e-06 4.48988e-06 4.43171e-06 ]

ivector-plda-scoring --normalize-length=true --num-utts=ark:pretrained-model/lre17_train_30s/num_utts.ark 'ivector-copy-plda --smoothing=0.0 pretrained-model/lre17_train_30s/plda - |' ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean_norm.ark ark:pretrained-model/lre17_dev_10s/xvector_lda100_submean_norm.ark - pretrained-model/lre17_dev_10s/score/plda_lre17_train_30s_lre17_dev_10s_lda100_submean_norm.score 
ivector-copy-plda --smoothing=0.0 pretrained-model/lre17_train_30s/plda - 
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:96) Reading train iVectors
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:122) Read 14 training iVectors, errors on 0
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:126) Average renormalization scale on training iVectors was 0.375309
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:129) Reading test iVectors
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:147) Read 928 test iVectors.
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:150) Average renormalization scale on test iVectors was 1.17976
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:214) Mean score was -34.6846, standard deviation was 42.3223
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:217) Processed 12992 trials, 0 had errors.
Cavg: 0.2298

[ lre17_dev_10s ]
Cavg: 0.2298	pretrained-model/lre17_dev_10s/score/plda_lre17_train_30s_lre17_dev_10s_lda100_submean_norm.Cavg


#LOG:: getScore use plda Done!
############################
lre17_train_30s
lre17_train_30s
lre17_dev_30s
true
############################
#LOG::Steps 01 Check and generate config Done
[Auto find] Your vectortype is xvector

#LOG Steps02::Determine Your vectortype is xvector Done
[Notice] It will set the default config lre17_train_30s[lre17_train_30s lre17_train_30s lre17_dev_30s] for lda, submean and whiten, if used.
#LOG Steps03::Configuration information Done
steps4 start
Vectordir:pretrained-model
lre17_dev_30s
lre17_train_30s
#LOG Steps04::Check x-vector or I-vector generation error Done
##Step5 start
lre17_train_30s[lre17_train_30s lre17_train_30s lre17_dev_30s]
#LOG Steps05:: Done
#LOG Steps06::Build connection between score process and enrollset Done
#LOG Steps07::False to speed up and true to clear files with error   Done
Get raw trials...
[Note] You should check it by yourself to avoid spks-lable-discrepancy in two set.
test spks are [ ara-acm ara-apc ara-ary ara-arz eng-gbr eng-usg por-brz qsl-pol qsl-rus spa-car spa-eur spa-lac zho-cmn zho-nan ]
register spks are [ ara-acm ara-apc ara-ary ara-arz eng-gbr eng-usg por-brz qsl-pol qsl-rus spa-car spa-eur spa-lac zho-cmn zho-nan ]
Write to pretrained-model/lre17_dev_30s/trials.raw done.
Remove invalid trials...
0 utt's trials have been removed and the invalid utts is in pretrained-model/lre17_dev_30s/trials.invalid.utt.list
Write to pretrained-model/lre17_dev_30s/trials done.
Make lre17_dev_30s trials done.
#LOG Steps08::Get Trials Done
#LOG Steps09:: Done
[ lr ]
ivector-normalize-length scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.26335, standard deviation was 0.352726
ivector-compute-lda --dim=100 --total-covariance-factor=0.1 ark:pretrained-model/lre17_train_30s/xvector_norm.ark ark:data/lre17_train_30s/utt2spk pretrained-model/lre17_train_30s/transform_100.mat 
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:288) Read 492731 utterances, 0 with errors.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:294) Computing within-class covariance.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:299) 2-norm of iVector mean is 15.9953
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:136) Stats have 14 speakers, 492731 utterances. 
WARNING (ivector-compute-lda[5.5.883~1-3eea3]:ComputeNormalizingTransform():ivector-compute-lda.cc:98) Floored 129 eigenvalues of covariance to 1.98841e-05
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:171) Singular values of between-class covariance after projecting with interpolated [total/within] covariance with a weight of 0.1 on the total covariance, are:  [ 2.90588 2.40519 2.15112 1.57986 1.19346 1.04706 1.00489 0.626699 0.492955 0.430551 0.339654 0.262772 0.222067 3.39918e-10 1.50561e-11 1.41766e-11 1.3787e-11 1.29261e-11 1.23991e-11 1.20515e-11 1.17956e-11 1.15073e-11 1.10855e-11 1.08423e-11 1.06726e-11 1.043e-11 1.01446e-11 9.95466e-12 9.83686e-12 9.57723e-12 9.48959e-12 9.26464e-12 9.12586e-12 8.87946e-12 8.80499e-12 8.68645e-12 8.51801e-12 8.23287e-12 8.02002e-12 7.93907e-12 7.75004e-12 7.65597e-12 7.50592e-12 7.38407e-12 7.29823e-12 7.26841e-12 7.11195e-12 7.03757e-12 6.84603e-12 6.69938e-12 6.68551e-12 6.52209e-12 6.43416e-12 6.36334e-12 6.18289e-12 6.10256e-12 6.02968e-12 6.02539e-12 5.94713e-12 5.87727e-12 5.75655e-12 5.65056e-12 5.59136e-12 5.47039e-12 5.42325e-12 5.29381e-12 5.1476e-12 5.13813e-12 4.97784e-12 4.89734e-12 4.82425e-12 4.79652e-12 4.68154e-12 4.63167e-12 4.53169e-12 4.4588e-12 4.41556e-12 4.32868e-12 4.29275e-12 4.19057e-12 4.11548e-12 4.02008e-12 3.97968e-12 3.93841e-12 3.84898e-12 3.72921e-12 3.66285e-12 3.59908e-12 3.56063e-12 3.48e-12 3.37638e-12 3.33863e-12 3.26507e-12 3.18899e-12 3.10062e-12 3.06316e-12 3.00453e-12 2.97976e-12 2.94594e-12 2.83621e-12 2.77813e-12 2.72591e-12 2.67029e-12 2.61699e-12 2.5666e-12 2.49457e-12 2.47714e-12 2.4372e-12 2.37685e-12 2.33161e-12 2.30659e-12 2.24263e-12 2.21768e-12 2.1415e-12 2.1314e-12 1.9936e-12 1.98225e-12 1.93959e-12 1.89675e-12 1.84232e-12 1.8375e-12 1.75251e-12 1.6588e-12 1.61799e-12 1.58681e-12 1.56003e-12 1.52131e-12 1.50476e-12 1.46223e-12 1.43422e-12 1.40598e-12 1.37978e-12 1.32589e-12 1.3049e-12 1.24837e-12 1.20679e-12 1.20302e-12 1.16225e-12 1.15229e-12 1.11321e-12 1.08741e-12 1.07572e-12 1.03118e-12 9.95066e-13 9.6071e-13 9.44611e-13 9.38644e-13 9.01184e-13 8.89295e-13 8.61739e-13 8.18741e-13 7.93326e-13 7.64134e-13 7.58101e-13 7.31556e-13 6.96536e-13 6.84815e-13 6.76163e-13 6.65161e-13 6.44676e-13 6.0626e-13 5.88007e-13 5.65442e-13 5.43494e-13 5.38411e-13 5.12856e-13 4.99757e-13 4.87929e-13 4.61777e-13 4.41923e-13 4.31617e-13 4.22456e-13 4.08345e-13 3.9768e-13 3.77143e-13 3.70577e-13 3.58922e-13 3.45225e-13 3.31515e-13 3.10234e-13 2.98156e-13 2.9378e-13 2.80455e-13 2.74832e-13 2.62717e-13 2.545e-13 2.48787e-13 2.43613e-13 2.34035e-13 2.07882e-13 2.05728e-13 1.95553e-13 1.86808e-13 1.84096e-13 1.77587e-13 1.71563e-13 1.62316e-13 1.54437e-13 1.50053e-13 1.3997e-13 1.35626e-13 1.26337e-13 1.23558e-13 1.1968e-13 1.12383e-13 1.02613e-13 9.82605e-14 9.51325e-14 8.87849e-14 8.67413e-14 8.27222e-14 7.68338e-14 7.34747e-14 7.28316e-14 6.4683e-14 6.32607e-14 5.92533e-14 5.8259e-14 5.41779e-14 5.26459e-14 5.00974e-14 4.78823e-14 4.38248e-14 4.2568e-14 3.98685e-14 3.85568e-14 3.42532e-14 3.32714e-14 3.08674e-14 2.871e-14 2.64161e-14 2.59336e-14 2.47206e-14 2.27356e-14 2.05709e-14 2.03413e-14 1.90146e-14 1.80811e-14 1.6708e-14 1.44957e-14 1.36066e-14 1.29644e-14 1.24864e-14 1.06508e-14 1.00594e-14 9.57377e-15 9.0738e-15 7.92316e-15 7.28889e-15 6.41913e-15 6.12398e-15 5.39913e-15 4.97701e-15 3.98205e-15 3.49914e-15 3.1913e-15 2.56876e-15 2.0396e-15 1.4198e-15 1.02199e-15 6.15529e-16 3.15916e-16 -1.96798e-16 -3.17994e-16 -5.90426e-16 -7.74202e-16 -1.11434e-15 -1.5514e-15 -2.05356e-15 -3.02328e-15 -3.13438e-15 -3.45145e-15 -4.22404e-15 -4.74058e-15 -5.23778e-15 -6.017e-15 -6.94501e-15 -7.65427e-15 -7.84041e-15 -8.74914e-15 -9.13164e-15 -9.87051e-15 -1.17554e-14 -1.19892e-14 -1.34698e-14 -1.51032e-14 -1.55803e-14 -1.759e-14 -1.88156e-14 -1.99412e-14 -2.13197e-14 -2.15938e-14 -2.30491e-14 -2.3581e-14 -2.49873e-14 -2.6116e-14 -2.86055e-14 -3.22324e-14 -3.3155e-14 -3.35879e-14 -3.68105e-14 -3.87799e-14 -4.09096e-14 -4.43543e-14 -4.86832e-14 -5.08205e-14 -5.21878e-14 -5.67198e-14 -5.96175e-14 -6.54364e-14 -6.73705e-14 -7.05449e-14 -7.60507e-14 -7.97494e-14 -8.15429e-14 -8.55435e-14 -8.77699e-14 -9.61526e-14 -9.99249e-14 -1.02573e-13 -1.04567e-13 -1.13184e-13 -1.16864e-13 -1.27924e-13 -1.33393e-13 -1.36094e-13 -1.42171e-13 -1.52787e-13 -1.54833e-13 -1.64929e-13 -1.699e-13 -1.79654e-13 -1.83442e-13 -1.97142e-13 -2.11987e-13 -2.2212e-13 -2.24497e-13 -2.29748e-13 -2.32274e-13 -2.42866e-13 -2.52672e-13 -2.72472e-13 -2.80419e-13 -2.89329e-13 -3.01063e-13 -3.12078e-13 -3.31314e-13 -3.44217e-13 -3.53446e-13 -3.59518e-13 -3.73339e-13 -3.91661e-13 -3.99794e-13 -4.22261e-13 -4.36173e-13 -4.581e-13 -4.6864e-13 -5.03118e-13 -5.14217e-13 -5.20797e-13 -5.56986e-13 -5.64542e-13 -5.82459e-13 -6.05782e-13 -6.25377e-13 -6.30576e-13 -6.55691e-13 -6.80004e-13 -6.83333e-13 -7.21023e-13 -7.25088e-13 -7.50613e-13 -7.70865e-13 -8.13039e-13 -8.25481e-13 -8.44359e-13 -8.4773e-13 -9.02988e-13 -9.20939e-13 -9.4504e-13 -9.96507e-13 -1.01588e-12 -1.03602e-12 -1.07917e-12 -1.09231e-12 -1.11465e-12 -1.1499e-12 -1.16233e-12 -1.19555e-12 -1.23182e-12 -1.26457e-12 -1.31607e-12 -1.33455e-12 -1.34232e-12 -1.35921e-12 -1.44e-12 -1.45164e-12 -1.48693e-12 -1.52027e-12 -1.55585e-12 -1.60597e-12 -1.61634e-12 -1.65232e-12 -1.69198e-12 -1.7438e-12 -1.77951e-12 -1.8239e-12 -1.85721e-12 -1.89554e-12 -1.95747e-12 -2.02753e-12 -2.06826e-12 -2.13031e-12 -2.13196e-12 -2.15219e-12 -2.27799e-12 -2.31803e-12 -2.34879e-12 -2.40175e-12 -2.47899e-12 -2.545e-12 -2.59225e-12 -2.60742e-12 -2.62943e-12 -2.66125e-12 -2.76783e-12 -2.81567e-12 -2.90772e-12 -2.96702e-12 -3.01653e-12 -3.15335e-12 -3.23855e-12 -3.24751e-12 -3.3031e-12 -3.33305e-12 -3.40871e-12 -3.45337e-12 -3.50447e-12 -3.61073e-12 -3.64742e-12 -3.72637e-12 -3.77371e-12 -3.84323e-12 -3.90057e-12 -3.97848e-12 -4.07818e-12 -4.12454e-12 -4.15046e-12 -4.23121e-12 -4.29324e-12 -4.37574e-12 -4.44523e-12 -4.54149e-12 -4.65499e-12 -4.76751e-12 -4.80435e-12 -4.88517e-12 -4.90975e-12 -5.01624e-12 -5.16949e-12 -5.22383e-12 -5.35253e-12 -5.36621e-12 -5.60203e-12 -5.66449e-12 -5.75557e-12 -5.87071e-12 -5.91888e-12 -5.98385e-12 -6.03217e-12 -6.2059e-12 -6.35759e-12 -6.49219e-12 -6.5615e-12 -6.6576e-12 -6.72985e-12 -6.80709e-12 -6.98579e-12 -7.06008e-12 -7.25475e-12 -7.41201e-12 -7.61434e-12 -7.68743e-12 -7.79264e-12 -7.90597e-12 -7.99185e-12 -8.1357e-12 -8.2733e-12 -8.52011e-12 -8.68321e-12 -8.72805e-12 -8.78545e-12 -9.14293e-12 -9.36166e-12 -9.41481e-12 -9.55606e-12 -9.68282e-12 -1.0036e-11 -1.01177e-11 -1.02118e-11 -1.02935e-11 -1.06209e-11 -1.0927e-11 -1.13614e-11 -1.14727e-11 -1.16499e-11 -1.21045e-11 -1.23256e-11 -1.28173e-11 -1.37283e-11 -1.45226e-11 -1.54851e-11 ]

LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:318) Wrote LDA transform to pretrained-model/lre17_train_30s/transform_100.mat
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 492731 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 1.53327 (should be close to zero), length divided by sqrt(dim) was 1.2004 (should probably be close to one)
ivector-mean ark:pretrained-model/lre17_train_30s/xvector_lda100.ark pretrained-model/lre17_train_30s/xvector_lda100.global.vec 
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_train_30s/xvector_lda100.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 492731 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.12033, standard deviation was 0.431066
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_dev_30s/xvector.scp ark:pretrained-model/lre17_dev_30s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 933 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 1.70772 (should be close to zero), length divided by sqrt(dim) was 0.975826 (should probably be close to one)
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_dev_30s/xvector_lda100.ark ark:pretrained-model/lre17_dev_30s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 933 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_dev_30s/xvector_lda100_submean.ark ark:pretrained-model/lre17_dev_30s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 933 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 0.869818, standard deviation was 0.508233
logistic-regression-train --max-steps=20 --mix-up=0 ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark ark:data/lre17_train_30s/utt2label pretrained-model/lre17_train_30s/lr/lr.raw.model 
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance ara-acm-ar-20031217-063700-0-b-40228-40378
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance ara-apc-arb-lev-20040721-231950-a-03109-03260
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance ara-apc-fla-0585-b-22127-22303
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance eng-usg-en-4902-b-50689-50953
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance spa-car-20050929-220820-154-fsp-a-65613-65764
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance spa-lac-20051213-185913-737-fsp-b-35030-35185
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance zho-cmn-TM-003970-b-52704-52901
LOG (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:90) Retrieved 492731 vectors with 7 missing. There were 14 class labels.
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -2.6281
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -1.09791
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.756456
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.666002
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.619344
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.608191
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.59609
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.593833
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.589134
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.585039
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.580517
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.58006
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.577648
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.577063
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.576048
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.575238
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.574579
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.574178
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.574065
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.573982
LOG (logistic-regression-train[5.5.883~1-3eea3]:Train():logistic-regression.cc:58) Finished training parameters without mixture components.
logistic-regression-copy pretrained-model/lre17_train_30s/lr/lr.raw.model pretrained-model/lre17_train_30s/lr/lr.scale.model 
LOG (logistic-regression-copy[5.5.883~1-3eea3]:main():logistic-regression-copy.cc:71) Wrote model to pretrained-model/lre17_train_30s/lr/lr.scale.model
logistic-regression-eval --apply-log=true --max-steps=20 --mix-up=0 pretrained-model/lre17_train_30s/lr/lr.scale.model ark:pretrained-model/lre17_train_30s/lr/trials.label ark:pretrained-model/lre17_dev_30s/xvector_lda100_submean_norm.ark - 
LOG (logistic-regression-eval[5.5.883~1-3eea3]:ComputeScores():logistic-regression-eval.cc:111) Calculated scores for 13062 vectors with 0 missing. 
compute-eer - 
LOG (compute-eer[5.5.671~1-e5a5a]:main():compute-eer.cc:136) Equal error rate is 9.75348%, at threshold -3.26044
EER% 9.753

[ lre17_dev_30s ]
9.753	pretrained-model/lre17_dev_30s/score/lr_lre17_train_30s_lre17_dev_30s_lda100_submean_norm.eer


############################
lre17_train_30s
lre17_train_30s
lre17_dev_30s
true
############################
#LOG::Steps 01 Check and generate config Done
[Auto find] Your vectortype is xvector

#LOG Steps02::Determine Your vectortype is xvector Done
[Notice] It will set the default config lre17_train_30s[lre17_train_30s lre17_train_30s lre17_dev_30s] for lda, submean and whiten, if used.
#LOG Steps03::Configuration information Done
steps4 start
Vectordir:pretrained-model
lre17_dev_30s
lre17_train_30s
#LOG Steps04::Check x-vector or I-vector generation error Done
##Step5 start
lre17_train_30s[lre17_train_30s lre17_train_30s lre17_dev_30s]
#LOG Steps05:: Done
#LOG Steps06::Build connection between score process and enrollset Done
#LOG Steps07::False to speed up and true to clear files with error   Done
#LOG Steps08::Get Trials Done
#LOG Steps09:: Done
[ lr ]
ivector-normalize-length scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.26335, standard deviation was 0.352726
ivector-compute-lda --dim=100 --total-covariance-factor=0.1 ark:pretrained-model/lre17_train_30s/xvector_norm.ark ark:data/lre17_train_30s/utt2spk pretrained-model/lre17_train_30s/transform_100.mat 
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:288) Read 492731 utterances, 0 with errors.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:294) Computing within-class covariance.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:299) 2-norm of iVector mean is 15.9953
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:136) Stats have 14 speakers, 492731 utterances. 
WARNING (ivector-compute-lda[5.5.883~1-3eea3]:ComputeNormalizingTransform():ivector-compute-lda.cc:98) Floored 129 eigenvalues of covariance to 1.98841e-05
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:171) Singular values of between-class covariance after projecting with interpolated [total/within] covariance with a weight of 0.1 on the total covariance, are:  [ 2.90588 2.40519 2.15112 1.57986 1.19346 1.04706 1.00489 0.626699 0.492955 0.430551 0.339654 0.262772 0.222067 3.39918e-10 1.50561e-11 1.41766e-11 1.3787e-11 1.29261e-11 1.23991e-11 1.20515e-11 1.17956e-11 1.15073e-11 1.10855e-11 1.08423e-11 1.06726e-11 1.043e-11 1.01446e-11 9.95466e-12 9.83686e-12 9.57723e-12 9.48959e-12 9.26464e-12 9.12586e-12 8.87946e-12 8.80499e-12 8.68645e-12 8.51801e-12 8.23287e-12 8.02002e-12 7.93907e-12 7.75004e-12 7.65597e-12 7.50592e-12 7.38407e-12 7.29823e-12 7.26841e-12 7.11195e-12 7.03757e-12 6.84603e-12 6.69938e-12 6.68551e-12 6.52209e-12 6.43416e-12 6.36334e-12 6.18289e-12 6.10256e-12 6.02968e-12 6.02539e-12 5.94713e-12 5.87727e-12 5.75655e-12 5.65056e-12 5.59136e-12 5.47039e-12 5.42325e-12 5.29381e-12 5.1476e-12 5.13813e-12 4.97784e-12 4.89734e-12 4.82425e-12 4.79652e-12 4.68154e-12 4.63167e-12 4.53169e-12 4.4588e-12 4.41556e-12 4.32868e-12 4.29275e-12 4.19057e-12 4.11548e-12 4.02008e-12 3.97968e-12 3.93841e-12 3.84898e-12 3.72921e-12 3.66285e-12 3.59908e-12 3.56063e-12 3.48e-12 3.37638e-12 3.33863e-12 3.26507e-12 3.18899e-12 3.10062e-12 3.06316e-12 3.00453e-12 2.97976e-12 2.94594e-12 2.83621e-12 2.77813e-12 2.72591e-12 2.67029e-12 2.61699e-12 2.5666e-12 2.49457e-12 2.47714e-12 2.4372e-12 2.37685e-12 2.33161e-12 2.30659e-12 2.24263e-12 2.21768e-12 2.1415e-12 2.1314e-12 1.9936e-12 1.98225e-12 1.93959e-12 1.89675e-12 1.84232e-12 1.8375e-12 1.75251e-12 1.6588e-12 1.61799e-12 1.58681e-12 1.56003e-12 1.52131e-12 1.50476e-12 1.46223e-12 1.43422e-12 1.40598e-12 1.37978e-12 1.32589e-12 1.3049e-12 1.24837e-12 1.20679e-12 1.20302e-12 1.16225e-12 1.15229e-12 1.11321e-12 1.08741e-12 1.07572e-12 1.03118e-12 9.95066e-13 9.6071e-13 9.44611e-13 9.38644e-13 9.01184e-13 8.89295e-13 8.61739e-13 8.18741e-13 7.93326e-13 7.64134e-13 7.58101e-13 7.31556e-13 6.96536e-13 6.84815e-13 6.76163e-13 6.65161e-13 6.44676e-13 6.0626e-13 5.88007e-13 5.65442e-13 5.43494e-13 5.38411e-13 5.12856e-13 4.99757e-13 4.87929e-13 4.61777e-13 4.41923e-13 4.31617e-13 4.22456e-13 4.08345e-13 3.9768e-13 3.77143e-13 3.70577e-13 3.58922e-13 3.45225e-13 3.31515e-13 3.10234e-13 2.98156e-13 2.9378e-13 2.80455e-13 2.74832e-13 2.62717e-13 2.545e-13 2.48787e-13 2.43613e-13 2.34035e-13 2.07882e-13 2.05728e-13 1.95553e-13 1.86808e-13 1.84096e-13 1.77587e-13 1.71563e-13 1.62316e-13 1.54437e-13 1.50053e-13 1.3997e-13 1.35626e-13 1.26337e-13 1.23558e-13 1.1968e-13 1.12383e-13 1.02613e-13 9.82605e-14 9.51325e-14 8.87849e-14 8.67413e-14 8.27222e-14 7.68338e-14 7.34747e-14 7.28316e-14 6.4683e-14 6.32607e-14 5.92533e-14 5.8259e-14 5.41779e-14 5.26459e-14 5.00974e-14 4.78823e-14 4.38248e-14 4.2568e-14 3.98685e-14 3.85568e-14 3.42532e-14 3.32714e-14 3.08674e-14 2.871e-14 2.64161e-14 2.59336e-14 2.47206e-14 2.27356e-14 2.05709e-14 2.03413e-14 1.90146e-14 1.80811e-14 1.6708e-14 1.44957e-14 1.36066e-14 1.29644e-14 1.24864e-14 1.06508e-14 1.00594e-14 9.57377e-15 9.0738e-15 7.92316e-15 7.28889e-15 6.41913e-15 6.12398e-15 5.39913e-15 4.97701e-15 3.98205e-15 3.49914e-15 3.1913e-15 2.56876e-15 2.0396e-15 1.4198e-15 1.02199e-15 6.15529e-16 3.15916e-16 -1.96798e-16 -3.17994e-16 -5.90426e-16 -7.74202e-16 -1.11434e-15 -1.5514e-15 -2.05356e-15 -3.02328e-15 -3.13438e-15 -3.45145e-15 -4.22404e-15 -4.74058e-15 -5.23778e-15 -6.017e-15 -6.94501e-15 -7.65427e-15 -7.84041e-15 -8.74914e-15 -9.13164e-15 -9.87051e-15 -1.17554e-14 -1.19892e-14 -1.34698e-14 -1.51032e-14 -1.55803e-14 -1.759e-14 -1.88156e-14 -1.99412e-14 -2.13197e-14 -2.15938e-14 -2.30491e-14 -2.3581e-14 -2.49873e-14 -2.6116e-14 -2.86055e-14 -3.22324e-14 -3.3155e-14 -3.35879e-14 -3.68105e-14 -3.87799e-14 -4.09096e-14 -4.43543e-14 -4.86832e-14 -5.08205e-14 -5.21878e-14 -5.67198e-14 -5.96175e-14 -6.54364e-14 -6.73705e-14 -7.05449e-14 -7.60507e-14 -7.97494e-14 -8.15429e-14 -8.55435e-14 -8.77699e-14 -9.61526e-14 -9.99249e-14 -1.02573e-13 -1.04567e-13 -1.13184e-13 -1.16864e-13 -1.27924e-13 -1.33393e-13 -1.36094e-13 -1.42171e-13 -1.52787e-13 -1.54833e-13 -1.64929e-13 -1.699e-13 -1.79654e-13 -1.83442e-13 -1.97142e-13 -2.11987e-13 -2.2212e-13 -2.24497e-13 -2.29748e-13 -2.32274e-13 -2.42866e-13 -2.52672e-13 -2.72472e-13 -2.80419e-13 -2.89329e-13 -3.01063e-13 -3.12078e-13 -3.31314e-13 -3.44217e-13 -3.53446e-13 -3.59518e-13 -3.73339e-13 -3.91661e-13 -3.99794e-13 -4.22261e-13 -4.36173e-13 -4.581e-13 -4.6864e-13 -5.03118e-13 -5.14217e-13 -5.20797e-13 -5.56986e-13 -5.64542e-13 -5.82459e-13 -6.05782e-13 -6.25377e-13 -6.30576e-13 -6.55691e-13 -6.80004e-13 -6.83333e-13 -7.21023e-13 -7.25088e-13 -7.50613e-13 -7.70865e-13 -8.13039e-13 -8.25481e-13 -8.44359e-13 -8.4773e-13 -9.02988e-13 -9.20939e-13 -9.4504e-13 -9.96507e-13 -1.01588e-12 -1.03602e-12 -1.07917e-12 -1.09231e-12 -1.11465e-12 -1.1499e-12 -1.16233e-12 -1.19555e-12 -1.23182e-12 -1.26457e-12 -1.31607e-12 -1.33455e-12 -1.34232e-12 -1.35921e-12 -1.44e-12 -1.45164e-12 -1.48693e-12 -1.52027e-12 -1.55585e-12 -1.60597e-12 -1.61634e-12 -1.65232e-12 -1.69198e-12 -1.7438e-12 -1.77951e-12 -1.8239e-12 -1.85721e-12 -1.89554e-12 -1.95747e-12 -2.02753e-12 -2.06826e-12 -2.13031e-12 -2.13196e-12 -2.15219e-12 -2.27799e-12 -2.31803e-12 -2.34879e-12 -2.40175e-12 -2.47899e-12 -2.545e-12 -2.59225e-12 -2.60742e-12 -2.62943e-12 -2.66125e-12 -2.76783e-12 -2.81567e-12 -2.90772e-12 -2.96702e-12 -3.01653e-12 -3.15335e-12 -3.23855e-12 -3.24751e-12 -3.3031e-12 -3.33305e-12 -3.40871e-12 -3.45337e-12 -3.50447e-12 -3.61073e-12 -3.64742e-12 -3.72637e-12 -3.77371e-12 -3.84323e-12 -3.90057e-12 -3.97848e-12 -4.07818e-12 -4.12454e-12 -4.15046e-12 -4.23121e-12 -4.29324e-12 -4.37574e-12 -4.44523e-12 -4.54149e-12 -4.65499e-12 -4.76751e-12 -4.80435e-12 -4.88517e-12 -4.90975e-12 -5.01624e-12 -5.16949e-12 -5.22383e-12 -5.35253e-12 -5.36621e-12 -5.60203e-12 -5.66449e-12 -5.75557e-12 -5.87071e-12 -5.91888e-12 -5.98385e-12 -6.03217e-12 -6.2059e-12 -6.35759e-12 -6.49219e-12 -6.5615e-12 -6.6576e-12 -6.72985e-12 -6.80709e-12 -6.98579e-12 -7.06008e-12 -7.25475e-12 -7.41201e-12 -7.61434e-12 -7.68743e-12 -7.79264e-12 -7.90597e-12 -7.99185e-12 -8.1357e-12 -8.2733e-12 -8.52011e-12 -8.68321e-12 -8.72805e-12 -8.78545e-12 -9.14293e-12 -9.36166e-12 -9.41481e-12 -9.55606e-12 -9.68282e-12 -1.0036e-11 -1.01177e-11 -1.02118e-11 -1.02935e-11 -1.06209e-11 -1.0927e-11 -1.13614e-11 -1.14727e-11 -1.16499e-11 -1.21045e-11 -1.23256e-11 -1.28173e-11 -1.37283e-11 -1.45226e-11 -1.54851e-11 ]

LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:318) Wrote LDA transform to pretrained-model/lre17_train_30s/transform_100.mat
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 492731 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 1.53327 (should be close to zero), length divided by sqrt(dim) was 1.2004 (should probably be close to one)
ivector-mean ark:pretrained-model/lre17_train_30s/xvector_lda100.ark pretrained-model/lre17_train_30s/xvector_lda100.global.vec 
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_train_30s/xvector_lda100.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 492731 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.12033, standard deviation was 0.431066
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_dev_30s/xvector.scp ark:pretrained-model/lre17_dev_30s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 933 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 1.70772 (should be close to zero), length divided by sqrt(dim) was 0.975826 (should probably be close to one)
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_dev_30s/xvector_lda100.ark ark:pretrained-model/lre17_dev_30s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 933 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_dev_30s/xvector_lda100_submean.ark ark:pretrained-model/lre17_dev_30s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 933 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 0.869818, standard deviation was 0.508233
logistic-regression-train --max-steps=20 --mix-up=0 ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark ark:data/lre17_train_30s/utt2label pretrained-model/lre17_train_30s/lr/lr.raw.model 
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance ara-acm-ar-20031217-063700-0-b-40228-40378
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance ara-apc-arb-lev-20040721-231950-a-03109-03260
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance ara-apc-fla-0585-b-22127-22303
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance eng-usg-en-4902-b-50689-50953
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance spa-car-20050929-220820-154-fsp-a-65613-65764
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance spa-lac-20051213-185913-737-fsp-b-35030-35185
WARNING (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:69) No vector for utterance zho-cmn-TM-003970-b-52704-52901
LOG (logistic-regression-train[5.5.883~1-3eea3]:main():logistic-regression-train.cc:90) Retrieved 492731 vectors with 7 missing. There were 14 class labels.
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -2.6281
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -1.09791
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.756456
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.666002
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.619344
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.608191
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.59609
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.593833
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.589134
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.585039
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.580517
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.58006
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.577648
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.577063
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.576048
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.575238
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.574579
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.574178
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.574065
LOG (logistic-regression-train[5.5.883~1-3eea3]:DoStep():logistic-regression.cc:221) Objective function is -0.573982
LOG (logistic-regression-train[5.5.883~1-3eea3]:Train():logistic-regression.cc:58) Finished training parameters without mixture components.
logistic-regression-copy pretrained-model/lre17_train_30s/lr/lr.raw.model pretrained-model/lre17_train_30s/lr/lr.scale.model 
LOG (logistic-regression-copy[5.5.883~1-3eea3]:main():logistic-regression-copy.cc:71) Wrote model to pretrained-model/lre17_train_30s/lr/lr.scale.model
logistic-regression-eval --apply-log=true --max-steps=20 --mix-up=0 pretrained-model/lre17_train_30s/lr/lr.scale.model ark:pretrained-model/lre17_train_30s/lr/trials.label ark:pretrained-model/lre17_dev_30s/xvector_lda100_submean_norm.ark - 
LOG (logistic-regression-eval[5.5.883~1-3eea3]:ComputeScores():logistic-regression-eval.cc:111) Calculated scores for 13062 vectors with 0 missing. 
Cavg: 0.0925

[ lre17_dev_30s ]
Cavg: 0.0925	pretrained-model/lre17_dev_30s/score/lr_lre17_train_30s_lre17_dev_30s_lda100_submean_norm.Cavg


#LOG:: getScore use lr Done!
############################
lre17_train_30s
lre17_train_30s
lre17_dev_30s
true
############################
#LOG::Steps 01 Check and generate config Done
[Auto find] Your vectortype is xvector

#LOG Steps02::Determine Your vectortype is xvector Done
[Notice] It will set the default config lre17_train_30s[lre17_train_30s lre17_train_30s lre17_dev_30s] for lda, submean and whiten, if used.
#LOG Steps03::Configuration information Done
steps4 start
Vectordir:pretrained-model
lre17_dev_30s
lre17_train_30s
#LOG Steps04::Check x-vector or I-vector generation error Done
##Step5 start
lre17_train_30s[lre17_train_30s lre17_train_30s lre17_dev_30s]
#LOG Steps05:: Done
#LOG Steps06::Build connection between score process and enrollset Done
#LOG Steps07::False to speed up and true to clear files with error   Done
#LOG Steps08::Get Trials Done
#LOG Steps09:: Done
[ plda ]
ivector-mean ark:data/lre17_train_30s/spk2utt scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/spk_xvector_mean.ark ark,t:pretrained-model/lre17_train_30s/num_utts.ark 
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance ara-acm-ar-20031217-063700-0-b-40228-40378
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance ara-apc-arb-lev-20040721-231950-a-03109-03260
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance ara-apc-fla-0585-b-22127-22303
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance eng-usg-en-4902-b-50689-50953
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance spa-car-20050929-220820-154-fsp-a-65613-65764
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance spa-lac-20051213-185913-737-fsp-b-35030-35185
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance zho-cmn-TM-003970-b-52704-52901
LOG (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:134) Computed mean of 14 speakers (0 with no utterances), consisting of 492731 utterances (7 absent from input).
LOG (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:145) Norm of mean of speakers is 17.7422, root-mean-square speaker-iVector length divided by sqrt(dim) is 1.29274
ivector-normalize-length scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.26335, standard deviation was 0.352726
ivector-compute-lda --dim=100 --total-covariance-factor=0.1 ark:pretrained-model/lre17_train_30s/xvector_norm.ark ark:data/lre17_train_30s/utt2spk pretrained-model/lre17_train_30s/transform_100.mat 
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:288) Read 492731 utterances, 0 with errors.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:294) Computing within-class covariance.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:299) 2-norm of iVector mean is 15.9953
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:136) Stats have 14 speakers, 492731 utterances. 
WARNING (ivector-compute-lda[5.5.883~1-3eea3]:ComputeNormalizingTransform():ivector-compute-lda.cc:98) Floored 129 eigenvalues of covariance to 1.98841e-05
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:171) Singular values of between-class covariance after projecting with interpolated [total/within] covariance with a weight of 0.1 on the total covariance, are:  [ 2.90588 2.40519 2.15112 1.57986 1.19346 1.04706 1.00489 0.626699 0.492955 0.430551 0.339654 0.262772 0.222067 3.39918e-10 1.50561e-11 1.41766e-11 1.3787e-11 1.29261e-11 1.23991e-11 1.20515e-11 1.17956e-11 1.15073e-11 1.10855e-11 1.08423e-11 1.06726e-11 1.043e-11 1.01446e-11 9.95466e-12 9.83686e-12 9.57723e-12 9.48959e-12 9.26464e-12 9.12586e-12 8.87946e-12 8.80499e-12 8.68645e-12 8.51801e-12 8.23287e-12 8.02002e-12 7.93907e-12 7.75004e-12 7.65597e-12 7.50592e-12 7.38407e-12 7.29823e-12 7.26841e-12 7.11195e-12 7.03757e-12 6.84603e-12 6.69938e-12 6.68551e-12 6.52209e-12 6.43416e-12 6.36334e-12 6.18289e-12 6.10256e-12 6.02968e-12 6.02539e-12 5.94713e-12 5.87727e-12 5.75655e-12 5.65056e-12 5.59136e-12 5.47039e-12 5.42325e-12 5.29381e-12 5.1476e-12 5.13813e-12 4.97784e-12 4.89734e-12 4.82425e-12 4.79652e-12 4.68154e-12 4.63167e-12 4.53169e-12 4.4588e-12 4.41556e-12 4.32868e-12 4.29275e-12 4.19057e-12 4.11548e-12 4.02008e-12 3.97968e-12 3.93841e-12 3.84898e-12 3.72921e-12 3.66285e-12 3.59908e-12 3.56063e-12 3.48e-12 3.37638e-12 3.33863e-12 3.26507e-12 3.18899e-12 3.10062e-12 3.06316e-12 3.00453e-12 2.97976e-12 2.94594e-12 2.83621e-12 2.77813e-12 2.72591e-12 2.67029e-12 2.61699e-12 2.5666e-12 2.49457e-12 2.47714e-12 2.4372e-12 2.37685e-12 2.33161e-12 2.30659e-12 2.24263e-12 2.21768e-12 2.1415e-12 2.1314e-12 1.9936e-12 1.98225e-12 1.93959e-12 1.89675e-12 1.84232e-12 1.8375e-12 1.75251e-12 1.6588e-12 1.61799e-12 1.58681e-12 1.56003e-12 1.52131e-12 1.50476e-12 1.46223e-12 1.43422e-12 1.40598e-12 1.37978e-12 1.32589e-12 1.3049e-12 1.24837e-12 1.20679e-12 1.20302e-12 1.16225e-12 1.15229e-12 1.11321e-12 1.08741e-12 1.07572e-12 1.03118e-12 9.95066e-13 9.6071e-13 9.44611e-13 9.38644e-13 9.01184e-13 8.89295e-13 8.61739e-13 8.18741e-13 7.93326e-13 7.64134e-13 7.58101e-13 7.31556e-13 6.96536e-13 6.84815e-13 6.76163e-13 6.65161e-13 6.44676e-13 6.0626e-13 5.88007e-13 5.65442e-13 5.43494e-13 5.38411e-13 5.12856e-13 4.99757e-13 4.87929e-13 4.61777e-13 4.41923e-13 4.31617e-13 4.22456e-13 4.08345e-13 3.9768e-13 3.77143e-13 3.70577e-13 3.58922e-13 3.45225e-13 3.31515e-13 3.10234e-13 2.98156e-13 2.9378e-13 2.80455e-13 2.74832e-13 2.62717e-13 2.545e-13 2.48787e-13 2.43613e-13 2.34035e-13 2.07882e-13 2.05728e-13 1.95553e-13 1.86808e-13 1.84096e-13 1.77587e-13 1.71563e-13 1.62316e-13 1.54437e-13 1.50053e-13 1.3997e-13 1.35626e-13 1.26337e-13 1.23558e-13 1.1968e-13 1.12383e-13 1.02613e-13 9.82605e-14 9.51325e-14 8.87849e-14 8.67413e-14 8.27222e-14 7.68338e-14 7.34747e-14 7.28316e-14 6.4683e-14 6.32607e-14 5.92533e-14 5.8259e-14 5.41779e-14 5.26459e-14 5.00974e-14 4.78823e-14 4.38248e-14 4.2568e-14 3.98685e-14 3.85568e-14 3.42532e-14 3.32714e-14 3.08674e-14 2.871e-14 2.64161e-14 2.59336e-14 2.47206e-14 2.27356e-14 2.05709e-14 2.03413e-14 1.90146e-14 1.80811e-14 1.6708e-14 1.44957e-14 1.36066e-14 1.29644e-14 1.24864e-14 1.06508e-14 1.00594e-14 9.57377e-15 9.0738e-15 7.92316e-15 7.28889e-15 6.41913e-15 6.12398e-15 5.39913e-15 4.97701e-15 3.98205e-15 3.49914e-15 3.1913e-15 2.56876e-15 2.0396e-15 1.4198e-15 1.02199e-15 6.15529e-16 3.15916e-16 -1.96798e-16 -3.17994e-16 -5.90426e-16 -7.74202e-16 -1.11434e-15 -1.5514e-15 -2.05356e-15 -3.02328e-15 -3.13438e-15 -3.45145e-15 -4.22404e-15 -4.74058e-15 -5.23778e-15 -6.017e-15 -6.94501e-15 -7.65427e-15 -7.84041e-15 -8.74914e-15 -9.13164e-15 -9.87051e-15 -1.17554e-14 -1.19892e-14 -1.34698e-14 -1.51032e-14 -1.55803e-14 -1.759e-14 -1.88156e-14 -1.99412e-14 -2.13197e-14 -2.15938e-14 -2.30491e-14 -2.3581e-14 -2.49873e-14 -2.6116e-14 -2.86055e-14 -3.22324e-14 -3.3155e-14 -3.35879e-14 -3.68105e-14 -3.87799e-14 -4.09096e-14 -4.43543e-14 -4.86832e-14 -5.08205e-14 -5.21878e-14 -5.67198e-14 -5.96175e-14 -6.54364e-14 -6.73705e-14 -7.05449e-14 -7.60507e-14 -7.97494e-14 -8.15429e-14 -8.55435e-14 -8.77699e-14 -9.61526e-14 -9.99249e-14 -1.02573e-13 -1.04567e-13 -1.13184e-13 -1.16864e-13 -1.27924e-13 -1.33393e-13 -1.36094e-13 -1.42171e-13 -1.52787e-13 -1.54833e-13 -1.64929e-13 -1.699e-13 -1.79654e-13 -1.83442e-13 -1.97142e-13 -2.11987e-13 -2.2212e-13 -2.24497e-13 -2.29748e-13 -2.32274e-13 -2.42866e-13 -2.52672e-13 -2.72472e-13 -2.80419e-13 -2.89329e-13 -3.01063e-13 -3.12078e-13 -3.31314e-13 -3.44217e-13 -3.53446e-13 -3.59518e-13 -3.73339e-13 -3.91661e-13 -3.99794e-13 -4.22261e-13 -4.36173e-13 -4.581e-13 -4.6864e-13 -5.03118e-13 -5.14217e-13 -5.20797e-13 -5.56986e-13 -5.64542e-13 -5.82459e-13 -6.05782e-13 -6.25377e-13 -6.30576e-13 -6.55691e-13 -6.80004e-13 -6.83333e-13 -7.21023e-13 -7.25088e-13 -7.50613e-13 -7.70865e-13 -8.13039e-13 -8.25481e-13 -8.44359e-13 -8.4773e-13 -9.02988e-13 -9.20939e-13 -9.4504e-13 -9.96507e-13 -1.01588e-12 -1.03602e-12 -1.07917e-12 -1.09231e-12 -1.11465e-12 -1.1499e-12 -1.16233e-12 -1.19555e-12 -1.23182e-12 -1.26457e-12 -1.31607e-12 -1.33455e-12 -1.34232e-12 -1.35921e-12 -1.44e-12 -1.45164e-12 -1.48693e-12 -1.52027e-12 -1.55585e-12 -1.60597e-12 -1.61634e-12 -1.65232e-12 -1.69198e-12 -1.7438e-12 -1.77951e-12 -1.8239e-12 -1.85721e-12 -1.89554e-12 -1.95747e-12 -2.02753e-12 -2.06826e-12 -2.13031e-12 -2.13196e-12 -2.15219e-12 -2.27799e-12 -2.31803e-12 -2.34879e-12 -2.40175e-12 -2.47899e-12 -2.545e-12 -2.59225e-12 -2.60742e-12 -2.62943e-12 -2.66125e-12 -2.76783e-12 -2.81567e-12 -2.90772e-12 -2.96702e-12 -3.01653e-12 -3.15335e-12 -3.23855e-12 -3.24751e-12 -3.3031e-12 -3.33305e-12 -3.40871e-12 -3.45337e-12 -3.50447e-12 -3.61073e-12 -3.64742e-12 -3.72637e-12 -3.77371e-12 -3.84323e-12 -3.90057e-12 -3.97848e-12 -4.07818e-12 -4.12454e-12 -4.15046e-12 -4.23121e-12 -4.29324e-12 -4.37574e-12 -4.44523e-12 -4.54149e-12 -4.65499e-12 -4.76751e-12 -4.80435e-12 -4.88517e-12 -4.90975e-12 -5.01624e-12 -5.16949e-12 -5.22383e-12 -5.35253e-12 -5.36621e-12 -5.60203e-12 -5.66449e-12 -5.75557e-12 -5.87071e-12 -5.91888e-12 -5.98385e-12 -6.03217e-12 -6.2059e-12 -6.35759e-12 -6.49219e-12 -6.5615e-12 -6.6576e-12 -6.72985e-12 -6.80709e-12 -6.98579e-12 -7.06008e-12 -7.25475e-12 -7.41201e-12 -7.61434e-12 -7.68743e-12 -7.79264e-12 -7.90597e-12 -7.99185e-12 -8.1357e-12 -8.2733e-12 -8.52011e-12 -8.68321e-12 -8.72805e-12 -8.78545e-12 -9.14293e-12 -9.36166e-12 -9.41481e-12 -9.55606e-12 -9.68282e-12 -1.0036e-11 -1.01177e-11 -1.02118e-11 -1.02935e-11 -1.06209e-11 -1.0927e-11 -1.13614e-11 -1.14727e-11 -1.16499e-11 -1.21045e-11 -1.23256e-11 -1.28173e-11 -1.37283e-11 -1.45226e-11 -1.54851e-11 ]

LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:318) Wrote LDA transform to pretrained-model/lre17_train_30s/transform_100.mat
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat ark:pretrained-model/lre17_train_30s/spk_xvector_mean.ark ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 14 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 2.75929 (should be close to zero), length divided by sqrt(dim) was 1.40767 (should probably be close to one)
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 492731 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 1.53327 (should be close to zero), length divided by sqrt(dim) was 1.2004 (should probably be close to one)
ivector-mean ark:pretrained-model/lre17_train_30s/xvector_lda100.ark pretrained-model/lre17_train_30s/xvector_lda100.global.vec 
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100.ark ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 14 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean.ark ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 14 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.00249, standard deviation was 1.05391
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_dev_30s/xvector.scp ark:pretrained-model/lre17_dev_30s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 933 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 1.70772 (should be close to zero), length divided by sqrt(dim) was 0.975826 (should probably be close to one)
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_dev_30s/xvector_lda100.ark ark:pretrained-model/lre17_dev_30s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 933 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_dev_30s/xvector_lda100_submean.ark ark:pretrained-model/lre17_dev_30s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 933 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 0.869818, standard deviation was 0.508233
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_train_30s/xvector_lda100.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 492731 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.12033, standard deviation was 0.431066
ivector-compute-plda ark:data/lre17_train_30s/spk2utt ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark pretrained-model/lre17_train_30s/plda 
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance ara-acm-ar-20031217-063700-0-b-40228-40378
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance ara-apc-arb-lev-20040721-231950-a-03109-03260
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance ara-apc-fla-0585-b-22127-22303
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance eng-usg-en-4902-b-50689-50953
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance spa-car-20050929-220820-154-fsp-a-65613-65764
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance spa-lac-20051213-185913-737-fsp-b-35030-35185
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance zho-cmn-TM-003970-b-52704-52901
LOG (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:109) Accumulated stats from 14 speakers (0 with no utterances), consisting of 492731 utterances (7 absent from input).
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 0 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0484
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9578
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 1 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0474
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9649
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 2 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.047
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.962
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 3 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0468
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9611
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 4 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0466
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9607
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 5 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0465
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9604
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 6 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0464
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9603
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 7 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0463
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9602
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 8 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0463
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9601
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 9 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0462
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9601
LOG (ivector-compute-plda[5.5.883~1-3eea3]:GetOutput():plda.cc:540) Norm of mean of iVector distribution is 1.36617
LOG (ivector-compute-plda[5.5.883~1-3eea3]:GetOutput():plda.cc:577) Diagonal of between-class variance in normalized space is  [ 6.80719 5.07642 4.21652 3.83125 3.16111 2.9845 1.87321 1.66839 1.41967 1.1445 1.04717 0.410991 0.353115 4.81664e-06 4.80518e-06 4.79952e-06 4.78659e-06 4.78533e-06 4.76916e-06 4.76006e-06 4.75583e-06 4.74886e-06 4.74146e-06 4.73521e-06 4.7278e-06 4.72638e-06 4.72055e-06 4.71931e-06 4.71476e-06 4.71429e-06 4.70611e-06 4.70559e-06 4.70065e-06 4.69463e-06 4.68495e-06 4.68266e-06 4.67751e-06 4.67639e-06 4.67314e-06 4.66825e-06 4.66532e-06 4.6579e-06 4.65547e-06 4.65337e-06 4.64883e-06 4.64267e-06 4.63883e-06 4.63503e-06 4.63125e-06 4.62836e-06 4.62584e-06 4.62273e-06 4.61574e-06 4.61444e-06 4.61382e-06 4.61116e-06 4.60593e-06 4.6043e-06 4.60154e-06 4.59664e-06 4.5945e-06 4.59077e-06 4.5904e-06 4.58557e-06 4.58489e-06 4.58178e-06 4.57969e-06 4.57774e-06 4.5726e-06 4.56954e-06 4.56696e-06 4.56051e-06 4.55849e-06 4.55698e-06 4.55087e-06 4.54951e-06 4.54885e-06 4.54784e-06 4.54052e-06 4.53963e-06 4.53844e-06 4.53773e-06 4.53246e-06 4.52768e-06 4.52645e-06 4.52513e-06 4.52103e-06 4.51922e-06 4.51895e-06 4.51649e-06 4.51228e-06 4.51026e-06 4.50859e-06 4.5083e-06 4.5071e-06 4.5048e-06 4.50242e-06 4.49714e-06 4.48988e-06 4.43171e-06 ]

ivector-plda-scoring --normalize-length=true --num-utts=ark:pretrained-model/lre17_train_30s/num_utts.ark 'ivector-copy-plda --smoothing=0.0 pretrained-model/lre17_train_30s/plda - |' ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean_norm.ark ark:pretrained-model/lre17_dev_30s/xvector_lda100_submean_norm.ark - pretrained-model/lre17_dev_30s/score/plda_lre17_train_30s_lre17_dev_30s_lda100_submean_norm.score 
ivector-copy-plda --smoothing=0.0 pretrained-model/lre17_train_30s/plda - 
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:96) Reading train iVectors
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:122) Read 14 training iVectors, errors on 0
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:126) Average renormalization scale on training iVectors was 0.375309
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:129) Reading test iVectors
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:147) Read 933 test iVectors.
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:150) Average renormalization scale on test iVectors was 1.33985
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:214) Mean score was -52.2905, standard deviation was 52.2928
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:217) Processed 13062 trials, 0 had errors.
compute-eer - 
LOG (compute-eer[5.5.671~1-e5a5a]:main():compute-eer.cc:136) Equal error rate is 21.6506%, at threshold -15.1299
EER% 21.65

[ lre17_dev_30s ]
21.65	pretrained-model/lre17_dev_30s/score/plda_lre17_train_30s_lre17_dev_30s_lda100_submean_norm.eer


############################
lre17_train_30s
lre17_train_30s
lre17_dev_30s
true
############################
#LOG::Steps 01 Check and generate config Done
[Auto find] Your vectortype is xvector

#LOG Steps02::Determine Your vectortype is xvector Done
[Notice] It will set the default config lre17_train_30s[lre17_train_30s lre17_train_30s lre17_dev_30s] for lda, submean and whiten, if used.
#LOG Steps03::Configuration information Done
steps4 start
Vectordir:pretrained-model
lre17_dev_30s
lre17_train_30s
#LOG Steps04::Check x-vector or I-vector generation error Done
##Step5 start
lre17_train_30s[lre17_train_30s lre17_train_30s lre17_dev_30s]
#LOG Steps05:: Done
#LOG Steps06::Build connection between score process and enrollset Done
#LOG Steps07::False to speed up and true to clear files with error   Done
#LOG Steps08::Get Trials Done
#LOG Steps09:: Done
[ plda ]
ivector-mean ark:data/lre17_train_30s/spk2utt scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/spk_xvector_mean.ark ark,t:pretrained-model/lre17_train_30s/num_utts.ark 
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance ara-acm-ar-20031217-063700-0-b-40228-40378
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance ara-apc-arb-lev-20040721-231950-a-03109-03260
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance ara-apc-fla-0585-b-22127-22303
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance eng-usg-en-4902-b-50689-50953
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance spa-car-20050929-220820-154-fsp-a-65613-65764
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance spa-lac-20051213-185913-737-fsp-b-35030-35185
WARNING (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:105) No iVector present in input for utterance zho-cmn-TM-003970-b-52704-52901
LOG (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:134) Computed mean of 14 speakers (0 with no utterances), consisting of 492731 utterances (7 absent from input).
LOG (ivector-mean[5.5.883~1-3eea3]:main():ivector-mean.cc:145) Norm of mean of speakers is 17.7422, root-mean-square speaker-iVector length divided by sqrt(dim) is 1.29274
ivector-normalize-length scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.26335, standard deviation was 0.352726
ivector-compute-lda --dim=100 --total-covariance-factor=0.1 ark:pretrained-model/lre17_train_30s/xvector_norm.ark ark:data/lre17_train_30s/utt2spk pretrained-model/lre17_train_30s/transform_100.mat 
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:288) Read 492731 utterances, 0 with errors.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:294) Computing within-class covariance.
LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:299) 2-norm of iVector mean is 15.9953
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:136) Stats have 14 speakers, 492731 utterances. 
WARNING (ivector-compute-lda[5.5.883~1-3eea3]:ComputeNormalizingTransform():ivector-compute-lda.cc:98) Floored 129 eigenvalues of covariance to 1.98841e-05
LOG (ivector-compute-lda[5.5.883~1-3eea3]:ComputeLdaTransform():ivector-compute-lda.cc:171) Singular values of between-class covariance after projecting with interpolated [total/within] covariance with a weight of 0.1 on the total covariance, are:  [ 2.90588 2.40519 2.15112 1.57986 1.19346 1.04706 1.00489 0.626699 0.492955 0.430551 0.339654 0.262772 0.222067 3.39918e-10 1.50561e-11 1.41766e-11 1.3787e-11 1.29261e-11 1.23991e-11 1.20515e-11 1.17956e-11 1.15073e-11 1.10855e-11 1.08423e-11 1.06726e-11 1.043e-11 1.01446e-11 9.95466e-12 9.83686e-12 9.57723e-12 9.48959e-12 9.26464e-12 9.12586e-12 8.87946e-12 8.80499e-12 8.68645e-12 8.51801e-12 8.23287e-12 8.02002e-12 7.93907e-12 7.75004e-12 7.65597e-12 7.50592e-12 7.38407e-12 7.29823e-12 7.26841e-12 7.11195e-12 7.03757e-12 6.84603e-12 6.69938e-12 6.68551e-12 6.52209e-12 6.43416e-12 6.36334e-12 6.18289e-12 6.10256e-12 6.02968e-12 6.02539e-12 5.94713e-12 5.87727e-12 5.75655e-12 5.65056e-12 5.59136e-12 5.47039e-12 5.42325e-12 5.29381e-12 5.1476e-12 5.13813e-12 4.97784e-12 4.89734e-12 4.82425e-12 4.79652e-12 4.68154e-12 4.63167e-12 4.53169e-12 4.4588e-12 4.41556e-12 4.32868e-12 4.29275e-12 4.19057e-12 4.11548e-12 4.02008e-12 3.97968e-12 3.93841e-12 3.84898e-12 3.72921e-12 3.66285e-12 3.59908e-12 3.56063e-12 3.48e-12 3.37638e-12 3.33863e-12 3.26507e-12 3.18899e-12 3.10062e-12 3.06316e-12 3.00453e-12 2.97976e-12 2.94594e-12 2.83621e-12 2.77813e-12 2.72591e-12 2.67029e-12 2.61699e-12 2.5666e-12 2.49457e-12 2.47714e-12 2.4372e-12 2.37685e-12 2.33161e-12 2.30659e-12 2.24263e-12 2.21768e-12 2.1415e-12 2.1314e-12 1.9936e-12 1.98225e-12 1.93959e-12 1.89675e-12 1.84232e-12 1.8375e-12 1.75251e-12 1.6588e-12 1.61799e-12 1.58681e-12 1.56003e-12 1.52131e-12 1.50476e-12 1.46223e-12 1.43422e-12 1.40598e-12 1.37978e-12 1.32589e-12 1.3049e-12 1.24837e-12 1.20679e-12 1.20302e-12 1.16225e-12 1.15229e-12 1.11321e-12 1.08741e-12 1.07572e-12 1.03118e-12 9.95066e-13 9.6071e-13 9.44611e-13 9.38644e-13 9.01184e-13 8.89295e-13 8.61739e-13 8.18741e-13 7.93326e-13 7.64134e-13 7.58101e-13 7.31556e-13 6.96536e-13 6.84815e-13 6.76163e-13 6.65161e-13 6.44676e-13 6.0626e-13 5.88007e-13 5.65442e-13 5.43494e-13 5.38411e-13 5.12856e-13 4.99757e-13 4.87929e-13 4.61777e-13 4.41923e-13 4.31617e-13 4.22456e-13 4.08345e-13 3.9768e-13 3.77143e-13 3.70577e-13 3.58922e-13 3.45225e-13 3.31515e-13 3.10234e-13 2.98156e-13 2.9378e-13 2.80455e-13 2.74832e-13 2.62717e-13 2.545e-13 2.48787e-13 2.43613e-13 2.34035e-13 2.07882e-13 2.05728e-13 1.95553e-13 1.86808e-13 1.84096e-13 1.77587e-13 1.71563e-13 1.62316e-13 1.54437e-13 1.50053e-13 1.3997e-13 1.35626e-13 1.26337e-13 1.23558e-13 1.1968e-13 1.12383e-13 1.02613e-13 9.82605e-14 9.51325e-14 8.87849e-14 8.67413e-14 8.27222e-14 7.68338e-14 7.34747e-14 7.28316e-14 6.4683e-14 6.32607e-14 5.92533e-14 5.8259e-14 5.41779e-14 5.26459e-14 5.00974e-14 4.78823e-14 4.38248e-14 4.2568e-14 3.98685e-14 3.85568e-14 3.42532e-14 3.32714e-14 3.08674e-14 2.871e-14 2.64161e-14 2.59336e-14 2.47206e-14 2.27356e-14 2.05709e-14 2.03413e-14 1.90146e-14 1.80811e-14 1.6708e-14 1.44957e-14 1.36066e-14 1.29644e-14 1.24864e-14 1.06508e-14 1.00594e-14 9.57377e-15 9.0738e-15 7.92316e-15 7.28889e-15 6.41913e-15 6.12398e-15 5.39913e-15 4.97701e-15 3.98205e-15 3.49914e-15 3.1913e-15 2.56876e-15 2.0396e-15 1.4198e-15 1.02199e-15 6.15529e-16 3.15916e-16 -1.96798e-16 -3.17994e-16 -5.90426e-16 -7.74202e-16 -1.11434e-15 -1.5514e-15 -2.05356e-15 -3.02328e-15 -3.13438e-15 -3.45145e-15 -4.22404e-15 -4.74058e-15 -5.23778e-15 -6.017e-15 -6.94501e-15 -7.65427e-15 -7.84041e-15 -8.74914e-15 -9.13164e-15 -9.87051e-15 -1.17554e-14 -1.19892e-14 -1.34698e-14 -1.51032e-14 -1.55803e-14 -1.759e-14 -1.88156e-14 -1.99412e-14 -2.13197e-14 -2.15938e-14 -2.30491e-14 -2.3581e-14 -2.49873e-14 -2.6116e-14 -2.86055e-14 -3.22324e-14 -3.3155e-14 -3.35879e-14 -3.68105e-14 -3.87799e-14 -4.09096e-14 -4.43543e-14 -4.86832e-14 -5.08205e-14 -5.21878e-14 -5.67198e-14 -5.96175e-14 -6.54364e-14 -6.73705e-14 -7.05449e-14 -7.60507e-14 -7.97494e-14 -8.15429e-14 -8.55435e-14 -8.77699e-14 -9.61526e-14 -9.99249e-14 -1.02573e-13 -1.04567e-13 -1.13184e-13 -1.16864e-13 -1.27924e-13 -1.33393e-13 -1.36094e-13 -1.42171e-13 -1.52787e-13 -1.54833e-13 -1.64929e-13 -1.699e-13 -1.79654e-13 -1.83442e-13 -1.97142e-13 -2.11987e-13 -2.2212e-13 -2.24497e-13 -2.29748e-13 -2.32274e-13 -2.42866e-13 -2.52672e-13 -2.72472e-13 -2.80419e-13 -2.89329e-13 -3.01063e-13 -3.12078e-13 -3.31314e-13 -3.44217e-13 -3.53446e-13 -3.59518e-13 -3.73339e-13 -3.91661e-13 -3.99794e-13 -4.22261e-13 -4.36173e-13 -4.581e-13 -4.6864e-13 -5.03118e-13 -5.14217e-13 -5.20797e-13 -5.56986e-13 -5.64542e-13 -5.82459e-13 -6.05782e-13 -6.25377e-13 -6.30576e-13 -6.55691e-13 -6.80004e-13 -6.83333e-13 -7.21023e-13 -7.25088e-13 -7.50613e-13 -7.70865e-13 -8.13039e-13 -8.25481e-13 -8.44359e-13 -8.4773e-13 -9.02988e-13 -9.20939e-13 -9.4504e-13 -9.96507e-13 -1.01588e-12 -1.03602e-12 -1.07917e-12 -1.09231e-12 -1.11465e-12 -1.1499e-12 -1.16233e-12 -1.19555e-12 -1.23182e-12 -1.26457e-12 -1.31607e-12 -1.33455e-12 -1.34232e-12 -1.35921e-12 -1.44e-12 -1.45164e-12 -1.48693e-12 -1.52027e-12 -1.55585e-12 -1.60597e-12 -1.61634e-12 -1.65232e-12 -1.69198e-12 -1.7438e-12 -1.77951e-12 -1.8239e-12 -1.85721e-12 -1.89554e-12 -1.95747e-12 -2.02753e-12 -2.06826e-12 -2.13031e-12 -2.13196e-12 -2.15219e-12 -2.27799e-12 -2.31803e-12 -2.34879e-12 -2.40175e-12 -2.47899e-12 -2.545e-12 -2.59225e-12 -2.60742e-12 -2.62943e-12 -2.66125e-12 -2.76783e-12 -2.81567e-12 -2.90772e-12 -2.96702e-12 -3.01653e-12 -3.15335e-12 -3.23855e-12 -3.24751e-12 -3.3031e-12 -3.33305e-12 -3.40871e-12 -3.45337e-12 -3.50447e-12 -3.61073e-12 -3.64742e-12 -3.72637e-12 -3.77371e-12 -3.84323e-12 -3.90057e-12 -3.97848e-12 -4.07818e-12 -4.12454e-12 -4.15046e-12 -4.23121e-12 -4.29324e-12 -4.37574e-12 -4.44523e-12 -4.54149e-12 -4.65499e-12 -4.76751e-12 -4.80435e-12 -4.88517e-12 -4.90975e-12 -5.01624e-12 -5.16949e-12 -5.22383e-12 -5.35253e-12 -5.36621e-12 -5.60203e-12 -5.66449e-12 -5.75557e-12 -5.87071e-12 -5.91888e-12 -5.98385e-12 -6.03217e-12 -6.2059e-12 -6.35759e-12 -6.49219e-12 -6.5615e-12 -6.6576e-12 -6.72985e-12 -6.80709e-12 -6.98579e-12 -7.06008e-12 -7.25475e-12 -7.41201e-12 -7.61434e-12 -7.68743e-12 -7.79264e-12 -7.90597e-12 -7.99185e-12 -8.1357e-12 -8.2733e-12 -8.52011e-12 -8.68321e-12 -8.72805e-12 -8.78545e-12 -9.14293e-12 -9.36166e-12 -9.41481e-12 -9.55606e-12 -9.68282e-12 -1.0036e-11 -1.01177e-11 -1.02118e-11 -1.02935e-11 -1.06209e-11 -1.0927e-11 -1.13614e-11 -1.14727e-11 -1.16499e-11 -1.21045e-11 -1.23256e-11 -1.28173e-11 -1.37283e-11 -1.45226e-11 -1.54851e-11 ]

LOG (ivector-compute-lda[5.5.883~1-3eea3]:main():ivector-compute-lda.cc:318) Wrote LDA transform to pretrained-model/lre17_train_30s/transform_100.mat
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat ark:pretrained-model/lre17_train_30s/spk_xvector_mean.ark ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 14 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 2.75929 (should be close to zero), length divided by sqrt(dim) was 1.40767 (should probably be close to one)
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_train_30s/xvector.scp ark:pretrained-model/lre17_train_30s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 492731 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 1.53327 (should be close to zero), length divided by sqrt(dim) was 1.2004 (should probably be close to one)
ivector-mean ark:pretrained-model/lre17_train_30s/xvector_lda100.ark pretrained-model/lre17_train_30s/xvector_lda100.global.vec 
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100.ark ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 14 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean.ark ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 14 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.00249, standard deviation was 1.05391
ivector-transform pretrained-model/lre17_train_30s/transform_100.mat scp:pretrained-model/lre17_dev_30s/xvector.scp ark:pretrained-model/lre17_dev_30s/xvector_lda100.ark 
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:91) Processed 933 iVectors.
LOG (ivector-transform[5.5.883~1-3eea3]:main():ivector-transform.cc:99) Norm of mean was 1.70772 (should be close to zero), length divided by sqrt(dim) was 0.975826 (should probably be close to one)
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_dev_30s/xvector_lda100.ark ark:pretrained-model/lre17_dev_30s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 933 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_dev_30s/xvector_lda100_submean.ark ark:pretrained-model/lre17_dev_30s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 933 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 0.869818, standard deviation was 0.508233
ivector-subtract-global-mean pretrained-model/lre17_train_30s/xvector_lda100.global.vec ark:pretrained-model/lre17_train_30s/xvector_lda100.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark 
LOG (ivector-subtract-global-mean[5.5.883~1-3eea3]:main():ivector-subtract-global-mean.cc:108) Wrote 492731 mean-subtracted iVectors
ivector-normalize-length ark:pretrained-model/lre17_train_30s/xvector_lda100_submean.ark ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark 
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:90) Processed 492731 iVectors.
LOG (ivector-normalize-length[5.5.883~1-3eea3]:main():ivector-normalize-length.cc:94) Average ratio of iVector to expected length was 1.12033, standard deviation was 0.431066
ivector-compute-plda ark:data/lre17_train_30s/spk2utt ark:pretrained-model/lre17_train_30s/xvector_lda100_submean_norm.ark pretrained-model/lre17_train_30s/plda 
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance ara-acm-ar-20031217-063700-0-b-40228-40378
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance ara-apc-arb-lev-20040721-231950-a-03109-03260
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance ara-apc-fla-0585-b-22127-22303
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance eng-usg-en-4902-b-50689-50953
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance spa-car-20050929-220820-154-fsp-a-65613-65764
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance spa-lac-20051213-185913-737-fsp-b-35030-35185
WARNING (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:80) No iVector present in input for utterance zho-cmn-TM-003970-b-52704-52901
LOG (ivector-compute-plda[5.5.883~1-3eea3]:main():ivector-compute-plda.cc:109) Accumulated stats from 14 speakers (0 with no utterances), consisting of 492731 utterances (7 absent from input).
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 0 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0484
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9578
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 1 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0474
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9649
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 2 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.047
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.962
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 3 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0468
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9611
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 4 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0466
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9607
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 5 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0465
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9604
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 6 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0464
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9603
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 7 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0463
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9602
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 8 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0463
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9601
LOG (ivector-compute-plda[5.5.883~1-3eea3]:Estimate():plda.cc:529) Plda estimation iteration 9 of 10
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:511) Trace of within-class variance is 80.0462
LOG (ivector-compute-plda[5.5.883~1-3eea3]:EstimateFromStats():plda.cc:512) Trace of between-class variance is 31.9601
LOG (ivector-compute-plda[5.5.883~1-3eea3]:GetOutput():plda.cc:540) Norm of mean of iVector distribution is 1.36617
LOG (ivector-compute-plda[5.5.883~1-3eea3]:GetOutput():plda.cc:577) Diagonal of between-class variance in normalized space is  [ 6.80719 5.07642 4.21652 3.83125 3.16111 2.9845 1.87321 1.66839 1.41967 1.1445 1.04717 0.410991 0.353115 4.81664e-06 4.80518e-06 4.79952e-06 4.78659e-06 4.78533e-06 4.76916e-06 4.76006e-06 4.75583e-06 4.74886e-06 4.74146e-06 4.73521e-06 4.7278e-06 4.72638e-06 4.72055e-06 4.71931e-06 4.71476e-06 4.71429e-06 4.70611e-06 4.70559e-06 4.70065e-06 4.69463e-06 4.68495e-06 4.68266e-06 4.67751e-06 4.67639e-06 4.67314e-06 4.66825e-06 4.66532e-06 4.6579e-06 4.65547e-06 4.65337e-06 4.64883e-06 4.64267e-06 4.63883e-06 4.63503e-06 4.63125e-06 4.62836e-06 4.62584e-06 4.62273e-06 4.61574e-06 4.61444e-06 4.61382e-06 4.61116e-06 4.60593e-06 4.6043e-06 4.60154e-06 4.59664e-06 4.5945e-06 4.59077e-06 4.5904e-06 4.58557e-06 4.58489e-06 4.58178e-06 4.57969e-06 4.57774e-06 4.5726e-06 4.56954e-06 4.56696e-06 4.56051e-06 4.55849e-06 4.55698e-06 4.55087e-06 4.54951e-06 4.54885e-06 4.54784e-06 4.54052e-06 4.53963e-06 4.53844e-06 4.53773e-06 4.53246e-06 4.52768e-06 4.52645e-06 4.52513e-06 4.52103e-06 4.51922e-06 4.51895e-06 4.51649e-06 4.51228e-06 4.51026e-06 4.50859e-06 4.5083e-06 4.5071e-06 4.5048e-06 4.50242e-06 4.49714e-06 4.48988e-06 4.43171e-06 ]

ivector-plda-scoring --normalize-length=true --num-utts=ark:pretrained-model/lre17_train_30s/num_utts.ark 'ivector-copy-plda --smoothing=0.0 pretrained-model/lre17_train_30s/plda - |' ark:pretrained-model/lre17_train_30s/spk_xvector_mean_lda100_submean_norm.ark ark:pretrained-model/lre17_dev_30s/xvector_lda100_submean_norm.ark - pretrained-model/lre17_dev_30s/score/plda_lre17_train_30s_lre17_dev_30s_lda100_submean_norm.score 
ivector-copy-plda --smoothing=0.0 pretrained-model/lre17_train_30s/plda - 
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:96) Reading train iVectors
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:122) Read 14 training iVectors, errors on 0
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:126) Average renormalization scale on training iVectors was 0.375309
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:129) Reading test iVectors
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:147) Read 933 test iVectors.
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:150) Average renormalization scale on test iVectors was 1.33985
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:214) Mean score was -52.2905, standard deviation was 52.2928
LOG (ivector-plda-scoring[5.5.883~1-3eea3]:main():ivector-plda-scoring.cc:217) Processed 13062 trials, 0 had errors.
Cavg: 0.2313

[ lre17_dev_30s ]
Cavg: 0.2313	pretrained-model/lre17_dev_30s/score/plda_lre17_train_30s_lre17_dev_30s_lda100_submean_norm.Cavg


#LOG:: getScore use plda Done!
*****Resuls are stored in pretrained-model *****
____________Step 2: getScore use plda and lr ended @ Mon Jan 24 14:02:14 SGT 2022____________
